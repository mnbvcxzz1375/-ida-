{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc643128-e9a7-430a-953b-2bcc8358bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import jn\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "warnings.filterwarnings('ignore')\n",
    "# %matplotlib inline\n",
    "# ## 模型预测的\n",
    "# !pip install scikit-learn\n",
    "# !pip install lightgbm\n",
    "# !pip install xgboost\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "## 数据降维处理的\n",
    "from sklearn.decomposition import PCA,FastICA,FactorAnalysis,SparsePCA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "## 参数搜索和评价的\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score,StratifiedKFold,train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# !pip install jieba\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a894c0f2-f759-4a1b-934a-edf338a8e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_insensitiveness_path = 'D:\\\\PycharmProjects\\\\shuju\\\\题目B基于文本内容的敏感信息识别\\\\data\\\\train_insensitiveness.csv'\n",
    "train_sensitiveness_path = 'D:\\\\PycharmProjects\\\\shuju\\\\题目B基于文本内容的敏感信息识别\\\\data\\\\train_sensitiveness.csv'\n",
    "\n",
    "# Read sensitiveness data\n",
    "with open(train_insensitiveness_path, 'r', encoding='ANSI',errors='ignore') as insensitiveness_file:\n",
    "    sensitiveness_data = pd.read_csv(insensitiveness_file)\n",
    "\n",
    "# Read insensitiveness data\n",
    "with open(train_sensitiveness_path, 'r', encoding='ANSI',errors='ignore') as sensitiveness_file:\n",
    "    insensitiveness_data = pd.read_csv(sensitiveness_file)\n",
    "\n",
    "# Merge the datasets\n",
    "merged_data = pd.concat([sensitiveness_data, insensitiveness_data], ignore_index=True)\n",
    "\n",
    "# Save the merged dataset\n",
    "output_path = 'merged_data.csv'\n",
    "with open(output_path, 'w', encoding='utf-8', newline='') as output_file:\n",
    "    merged_data.to_csv(output_file, index=False)\n",
    "\n",
    "output_path = 'sensitiveness_n.csv'\n",
    "with open(output_path, 'w', encoding='utf-8', newline='') as output_file:\n",
    "    sensitiveness_data.to_csv(output_file, index=False)\n",
    "    \n",
    "output_path = 'insensitiveness_n.csv'\n",
    "with open(output_path, 'w', encoding='utf-8', newline='') as output_file:\n",
    "    insensitiveness_data.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "134d043a-31df-418c-b403-eafcfdb818f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting complete. Split data written to merged_data_n.csv.\n",
      "Splitting complete. Split data written to test_new.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取原始数据\n",
    "input_path = 'merged_data.csv'\n",
    "output_path = 'merged_data_n.csv'\n",
    "data = pd.read_csv(input_path)\n",
    "\n",
    "# 对 '0' 列的每一行进行逐字拆分\n",
    "def split_text(row):\n",
    "    return ' '.join(list(row))\n",
    "\n",
    "data['0'] = data['0'].astype(str)\n",
    "data['segmented_text'] = data['0'].apply(split_text)\n",
    "\n",
    "# 保存逐字拆分后的数据到新文件\n",
    "data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Splitting complete. Split data written to {output_path}.\")\n",
    "\n",
    "\n",
    "\n",
    "# 读取原始数据\n",
    "input_path = 'test_n.csv'\n",
    "output_path = 'test_new.csv'\n",
    "data = pd.read_csv(input_path)\n",
    "\n",
    "# 对 '0' 列的每一行进行逐字拆分\n",
    "def split_text(row):\n",
    "    return ' '.join(list(row))\n",
    "\n",
    "data['0'] = data['0'].astype(str)\n",
    "data['segmented_text'] = data['0'].apply(split_text)\n",
    "\n",
    "# 保存逐字拆分后的数据到新文件\n",
    "data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Splitting complete. Split data written to {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dfbdb86-2b8e-40e9-a2fd-9ae4c889c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from gensim.models import KeyedVectors\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "# from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# # 读取数据\n",
    "# data_path = 'merged_data_n.csv'\n",
    "# data = pd.read_csv(data_path)\n",
    "\n",
    "# # 分割数据集为训练集和测试集\n",
    "# train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# # 读取预训练的 SGNS 词向量模型\n",
    "# sgns_model_path = 'D:\\\\PycharmProjects\\\\shuju\\\\pre_model\\\\sgns.baidubaike.bigram-char'  # 替换为你的 SGNS 模型文件路径\n",
    "# sgns_model = KeyedVectors.load_word2vec_format(sgns_model_path, binary=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "# # 准备文本数据\n",
    "# max_words = 5000\n",
    "# max_sequence_length = 100\n",
    "# tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "# tokenizer.fit_on_texts(train_data['segmented_text'])\n",
    "# word_index = tokenizer.word_index\n",
    "# num_words = min(max_words, len(word_index) + 1)\n",
    "\n",
    "# embedding_matrix = np.zeros((num_words, sgns_model.vector_size))\n",
    "# for word, i in word_index.items():\n",
    "#     if i < max_words and word in sgns_model:\n",
    "#         embedding_matrix[i] = sgns_model[word]\n",
    "\n",
    "# # 对序列进行填充\n",
    "# train_sequences = tokenizer.texts_to_sequences(train_data['segmented_text'])\n",
    "# test_sequences = tokenizer.texts_to_sequences(test_data['segmented_text'])\n",
    "\n",
    "# train_inputs = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "# test_inputs = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# # 准备标签\n",
    "# train_labels = train_data['label']\n",
    "# test_labels = test_data['label']\n",
    "\n",
    "# # 构建模型\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(num_words, sgns_model.vector_size, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
    "# model.add(Conv1D(filters=100, kernel_size=3, activation='relu'))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# # 编译模型\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # 训练模型\n",
    "# model.fit(train_inputs, train_labels, epochs=10, batch_size=64, validation_data=(test_inputs, test_labels))\n",
    "\n",
    "# # 评估模型\n",
    "# loss, accuracy = model.evaluate(test_inputs, test_labels)\n",
    "# print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# # 预测测试集\n",
    "# predictions = np.argmax(model.predict(test_inputs), axis=1)\n",
    "\n",
    "# # 计算混淆矩阵\n",
    "# conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "# # 计算精确率、召回率、F1值\n",
    "# precision = precision_score(test_labels, predictions)\n",
    "# recall = recall_score(test_labels, predictions)\n",
    "# f1 = f1_score(test_labels, predictions)\n",
    "\n",
    "# print(f'Precision: {precision}, Recall: {recall}, F1 Score: {f1}')\n",
    "\n",
    "# model.save('text_classification_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb5b0535-2e27-4a34-8944-e0a94b363bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from gensim.models import Word2Vec\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "# from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# # 读取数据\n",
    "# data_path = 'merged_data_n.csv'\n",
    "# data = pd.read_csv(data_path)\n",
    "\n",
    "# # 分割数据集为训练集和测试集\n",
    "# train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# # 准备文本数据\n",
    "# max_words = 5000\n",
    "# max_sequence_length = 100\n",
    "# tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "# tokenizer.fit_on_texts(train_data['segmented_text'])\n",
    "# word_index = tokenizer.word_index\n",
    "# num_words = min(max_words, len(word_index) + 1)\n",
    "\n",
    "# # 训练 CBOW 模型\n",
    "# cbow_model = Word2Vec(sentences=train_data['segmented_text'], vector_size=100, window=4, sg=0, min_count=1,seed=42,workers=4,hs=0)\n",
    "\n",
    "# embedding_matrix = np.zeros((num_words, cbow_model.vector_size))\n",
    "# for word, i in word_index.items():\n",
    "#     if i < max_words and word in cbow_model.wv:\n",
    "#         embedding_matrix[i] = cbow_model.wv[word]\n",
    "\n",
    "# # 对序列进行填充\n",
    "# train_sequences = tokenizer.texts_to_sequences(train_data['segmented_text'])\n",
    "# test_sequences = tokenizer.texts_to_sequences(test_data['segmented_text'])\n",
    "\n",
    "# train_inputs = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "# test_inputs = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# # 准备标签\n",
    "# train_labels = train_data['label']\n",
    "# test_labels = test_data['label']\n",
    "\n",
    "# # 构建模型\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(num_words, cbow_model.vector_size, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
    "# model.add(Conv1D(filters=100, kernel_size=3, activation='relu'))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# # 编译模型\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # 训练模型\n",
    "# model.fit(train_inputs, train_labels, epochs=10, batch_size=64, validation_data=(test_inputs, test_labels))\n",
    "\n",
    "# # 评估模型\n",
    "# loss, accuracy = model.evaluate(test_inputs, test_labels)\n",
    "# print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# # 预测测试集\n",
    "# predictions = np.argmax(model.predict(test_inputs), axis=1)\n",
    "\n",
    "# # 计算混淆矩阵\n",
    "# conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "# # 计算精确率、召回率、F1值\n",
    "# precision = precision_score(test_labels, predictions)\n",
    "# recall = recall_score(test_labels, predictions)\n",
    "# f1 = f1_score(test_labels, predictions)\n",
    "\n",
    "# print(f'Precision: {precision}, Recall: {recall}, F1 Score: {f1}')\n",
    "\n",
    "# model.save('text_classification_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f73e024-13e4-49c7-a02e-dfe73c5f3648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ErnieModel were not initialized from the model checkpoint at nghuyong/ernie-3.0-mini-zh and are newly initialized: ['ernie.pooler.dense.bias', 'ernie.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, ErnieModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"nghuyong/ernie-3.0-mini-zh\")\n",
    "model = ErnieModel.from_pretrained(\"nghuyong/ernie-3.0-mini-zh\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32495632-27bc-43d1-bb74-835ff1e2808e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "210/210 [==============================] - 6s 25ms/step - loss: 0.6546 - accuracy: 0.7303 - val_loss: 0.6243 - val_accuracy: 0.7325\n",
      "Epoch 2/10\n",
      "210/210 [==============================] - 5s 25ms/step - loss: 0.6089 - accuracy: 0.7303 - val_loss: 0.5953 - val_accuracy: 0.7325\n",
      "Epoch 3/10\n",
      "210/210 [==============================] - 5s 24ms/step - loss: 0.5908 - accuracy: 0.7303 - val_loss: 0.5849 - val_accuracy: 0.7325\n",
      "Epoch 4/10\n",
      "210/210 [==============================] - 5s 23ms/step - loss: 0.5849 - accuracy: 0.7303 - val_loss: 0.5817 - val_accuracy: 0.7325\n",
      "Epoch 5/10\n",
      "210/210 [==============================] - 5s 24ms/step - loss: 0.5834 - accuracy: 0.7303 - val_loss: 0.5810 - val_accuracy: 0.7325\n",
      "Epoch 6/10\n",
      "210/210 [==============================] - 5s 24ms/step - loss: 0.5830 - accuracy: 0.7303 - val_loss: 0.5808 - val_accuracy: 0.7325\n",
      "Epoch 7/10\n",
      "210/210 [==============================] - 5s 24ms/step - loss: 0.5829 - accuracy: 0.7303 - val_loss: 0.5808 - val_accuracy: 0.7325\n",
      "Epoch 8/10\n",
      "210/210 [==============================] - 5s 24ms/step - loss: 0.5830 - accuracy: 0.7303 - val_loss: 0.5808 - val_accuracy: 0.7325\n",
      "Epoch 9/10\n",
      "210/210 [==============================] - 5s 24ms/step - loss: 0.5829 - accuracy: 0.7303 - val_loss: 0.5808 - val_accuracy: 0.7325\n",
      "Epoch 10/10\n",
      "210/210 [==============================] - 5s 24ms/step - loss: 0.5829 - accuracy: 0.7303 - val_loss: 0.5808 - val_accuracy: 0.7325\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.5808 - accuracy: 0.7325\n",
      "Test Loss: 0.5807769894599915, Test Accuracy: 0.7325164079666138\n",
      "105/105 [==============================] - 1s 5ms/step\n",
      "Precision: 0.0, Recall: 0.0, F1 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, ErnieModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# 读取数据\n",
    "data_path = 'merged_data_n.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# 分割数据集为训练集和测试集\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# 读取预训练的 ERNIE 模型\n",
    "ernie_model_name = \"nghuyong/ernie-1.0-base-zh\"\n",
    "tokenizer = BertTokenizer.from_pretrained(ernie_model_name)\n",
    "ernie_model = ErnieModel.from_pretrained(ernie_model_name)\n",
    "\n",
    "# 准备文本数据\n",
    "max_words = 5000\n",
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_data['0'])\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_words, len(word_index) + 1)\n",
    "\n",
    "# 建立空的embedding_matrix\n",
    "embedding_matrix = np.zeros((num_words, ernie_model.config.hidden_size))\n",
    "\n",
    "# 对序列进行填充\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['segmented_text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['segmented_text'])\n",
    "\n",
    "train_inputs = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "test_inputs = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# 准备标签\n",
    "train_labels = train_data['label']\n",
    "test_labels = test_data['label']\n",
    "\n",
    "# 构建模型\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, ernie_model.config.hidden_size, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Conv1D(filters=100, kernel_size=3, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit(train_inputs, train_labels, epochs=10, batch_size=64, validation_data=(test_inputs, test_labels))\n",
    "\n",
    "# 评估模型\n",
    "loss, accuracy = model.evaluate(test_inputs, test_labels)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# 预测测试集\n",
    "predictions = np.argmax(model.predict(test_inputs), axis=1)\n",
    "\n",
    "# 计算混淆矩阵\n",
    "conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "# 计算精确率、召回率、F1值\n",
    "precision = precision_score(test_labels, predictions)\n",
    "recall = recall_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "\n",
    "print(f'Precision: {precision}, Recall: {recall}, F1 Score: {f1}')\n",
    "\n",
    "model.save('text_classification_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94440968-745f-4b25-91e2-8fc8770616e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tokenizer' object has no attribute 'get_vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m embedding_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((num_words, ernie_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size))\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, i \u001b[38;5;129;01min\u001b[39;00m word_index\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m max_words \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab\u001b[49m()\u001b[38;5;241m.\u001b[39mget(word):  \u001b[38;5;66;03m# 检查词是否在ERNIE的vocab中\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         embedding_matrix[i] \u001b[38;5;241m=\u001b[39m ernie_model\u001b[38;5;241m.\u001b[39mget_input_embeddings()(torch\u001b[38;5;241m.\u001b[39mtensor([tokenizer\u001b[38;5;241m.\u001b[39mencode(word)]))\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# 对序列进行填充\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tokenizer' object has no attribute 'get_vocab'"
     ]
    }
   ],
   "source": [
    "# from transformers import BertTokenizer, ErnieModel\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Bidirectional, LSTM, Dropout\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# # 读取数据\n",
    "# data_path = 'merged_data_n.csv'\n",
    "# data = pd.read_csv(data_path)\n",
    "\n",
    "# # 分割数据集为训练集和测试集\n",
    "# train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# # 读取预训练的 ERNIE 模型\n",
    "# ernie_model_name = \"nghuyong/ernie-1.0-base-zh\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(ernie_model_name)\n",
    "# ernie_model = ErnieModel.from_pretrained(ernie_model_name)\n",
    "\n",
    "# # 准备文本数据\n",
    "# max_words = 5000\n",
    "# max_sequence_length = 32  # 调整文本处理长度\n",
    "# tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "# tokenizer.fit_on_texts(train_data['0'])\n",
    "# word_index = tokenizer.word_index\n",
    "# num_words = min(max_words, len(word_index) + 1)\n",
    "\n",
    "# # 建立embedding_matrix\n",
    "# embedding_matrix = np.zeros((num_words, ernie_model.config.hidden_size))\n",
    "# for word, i in word_index.items():\n",
    "#     if i < max_words and tokenizer.get_vocab().get(word):  # 检查词是否在ERNIE的vocab中\n",
    "#         embedding_matrix[i] = ernie_model.get_input_embeddings()(torch.tensor([tokenizer.encode(word)])).numpy()\n",
    "\n",
    "# # 对序列进行填充\n",
    "# train_sequences = tokenizer.texts_to_sequences(train_data['segmented_text'])\n",
    "# test_sequences = tokenizer.texts_to_sequences(test_data['segmented_text'])\n",
    "\n",
    "# train_inputs = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "# test_inputs = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# # 准备标签\n",
    "# train_labels = train_data['label']\n",
    "# test_labels = test_data['label']\n",
    "\n",
    "# # 构建模型\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(num_words, ernie_model.config.hidden_size, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
    "# model.add(Dropout(0.1))  # 添加Dropout\n",
    "# model.add(Conv1D(filters=100, kernel_size=3, activation='relu'))\n",
    "# model.add(Bidirectional(LSTM(ernie_model.config.hidden_size)))  # 使用Bidirectional LSTM\n",
    "# model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# # 编译模型\n",
    "# optimizer = Adam(learning_rate=3e-5)  # 设置学习率\n",
    "# model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # 训练模型\n",
    "# model.fit(train_inputs, train_labels, epochs=10, batch_size=64, validation_data=(test_inputs, test_labels))\n",
    "\n",
    "# # 评估模型\n",
    "# loss, accuracy = model.evaluate(test_inputs, test_labels)\n",
    "# print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# # 预测测试集\n",
    "# predictions = np.argmax(model.predict(test_inputs), axis=1)\n",
    "\n",
    "# # 计算混淆矩阵\n",
    "# conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "# # 计算精确率、召回率、F1值\n",
    "# precision = precision_score(test_labels, predictions)\n",
    "# recall = recall_score(test_labels, predictions)\n",
    "# f1 = f1_score(test_labels, predictions)\n",
    "\n",
    "# print(f'Precision: {precision}, Recall: {recall}, F1 Score: {f1}')\n",
    "\n",
    "# model.save('text_classification_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3db07011-d22e-49a4-9711-28d09f4ac10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d362d10d6be6433ab184fc8b36698cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/62.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e07c490fe5b406aa52536cd82f57649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/91.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa830b233a24f18b14d6dbbbc3e8166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a1681bff73491aac8392867d57865c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/470 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e87e85b0ba438381e2b4affd1f1c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/402M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 97/210 [============>.................] - ETA: 4:03 - loss: 0.6231 - accuracy: 0.7328"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# 评估模型\u001b[39;00m\n\u001b[0;32m     64\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_inputs, test_labels)\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, ErnieModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Bidirectional, LSTM\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# 读取数据\n",
    "data_path = 'merged_data_n.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# 分割数据集为训练集和测试集\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# 读取预训练的 ERNIE 模型\n",
    "ernie_model_name = \"nghuyong/ernie-1.0-base-zh\"\n",
    "tokenizer = BertTokenizer.from_pretrained(ernie_model_name)\n",
    "ernie_model = ErnieModel.from_pretrained(ernie_model_name)\n",
    "\n",
    "# 准备文本数据\n",
    "max_words = 5000\n",
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_data['0'])\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_words, len(word_index) + 1)\n",
    "\n",
    "# 建立空的embedding_matrix\n",
    "embedding_matrix = np.zeros((num_words, ernie_model.config.hidden_size))\n",
    "\n",
    "# 对序列进行填充\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['segmented_text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['segmented_text'])\n",
    "\n",
    "train_inputs = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "test_inputs = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# 准备标签\n",
    "train_labels = train_data['label']\n",
    "test_labels = test_data['label']\n",
    "\n",
    "# 构建模型\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, ernie_model.config.hidden_size, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Bidirectional(LSTM(ernie_model.config.hidden_size, dropout=0.1)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# 编译模型\n",
    "optimizer = AdamW(learning_rate=3e-5)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit(train_inputs, train_labels, epochs=10, batch_size=64, validation_data=(test_inputs, test_labels))\n",
    "\n",
    "# 评估模型\n",
    "loss, accuracy = model.evaluate(test_inputs, test_labels)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# 预测测试集\n",
    "predictions = np.argmax(model.predict(test_inputs), axis=1)\n",
    "\n",
    "# 计算混淆矩阵\n",
    "conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "# 计算精确率、召回率、F1值\n",
    "precision = precision_score(test_labels, predictions)\n",
    "recall = recall_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "\n",
    "print(f'Precision: {precision}, Recall: {recall}, F1 Score: {f1}')\n",
    "\n",
    "model.save('text_classification_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e21daff-0216-4c28-9632-efb3afb23097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a04046280e413b915fa1f21b0782d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/62.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75beb2737b8942a4ac87c21d88d3bf87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/470 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1b336d789448c3bb02685257272509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/91.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a1d2c008364b55b5b5b99f2bb92dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951d0ac7214f443f89c724fb1df496ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/402M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ErnieForSequenceClassification were not initialized from the model checkpoint at nghuyong/ernie-1.0 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ErnieForSequenceClassification(\n",
      "  (ernie): ErnieModel(\n",
      "    (embeddings): ErnieEmbeddings(\n",
      "      (word_embeddings): Embedding(18000, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(513, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): ErnieEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ErnieLayer(\n",
      "          (attention): ErnieAttention(\n",
      "            (self): ErnieSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ErnieSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ErnieIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): ErnieOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): ErnieLayer(\n",
      "          (attention): ErnieAttention(\n",
      "            (self): ErnieSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ErnieSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ErnieIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): ErnieOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): ErnieLayer(\n",
      "          (attention): ErnieAttention(\n",
      "            (self): ErnieSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ErnieSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ErnieIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): ErnieOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): ErnieLayer(\n",
      "          (attention): ErnieAttention(\n",
      "            (self): ErnieSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ErnieSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ErnieIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): ErnieOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): ErnieLayer(\n",
      "          (attention): ErnieAttention(\n",
      "            (self): ErnieSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ErnieSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ErnieIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): ErnieOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): ErnieLayer(\n",
      "          (attention): ErnieAttention(\n",
      "            (self): ErnieSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ErnieSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ErnieIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): ErnieOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): ErnieLayer(\n",
      "          (attention): ErnieAttention(\n",
      "            (self): ErnieSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ErnieSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ErnieIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): ErnieOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): ErnieLayer(\n",
      "          (attention): ErnieAttention(\n",
      "            (self): ErnieSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ErnieSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ErnieIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): ErnieOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): ErnieLayer(\n",
      "          (attention): ErnieAttention(\n",
      "            (self): ErnieSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ErnieSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ErnieIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): ErnieOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): ErnieLayer(\n",
      "          (attention): ErnieAttention(\n",
      "            (self): ErnieSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ErnieSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ErnieIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): ErnieOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): ErnieLayer(\n",
      "          (attention): ErnieAttention(\n",
      "            (self): ErnieSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ErnieSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ErnieIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): ErnieOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): ErnieLayer(\n",
      "          (attention): ErnieAttention(\n",
      "            (self): ErnieSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ErnieSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ErnieIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): ErnieOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): ErniePooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# ERNIE 模型的名称\n",
    "ernie_model_name = \"nghuyong/ernie-1.0\"\n",
    "\n",
    "# 加载 ERNIE 模型和分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(ernie_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ernie_model_name)\n",
    "\n",
    "# 打印模型架构\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c061b3b-f524-49bf-9346-82f49abef687",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "D:\\PycharmProjects\\shuju\\pre_model\\ERNIE_1.0_max-len-512.tar does not appear to have a file named config.json. Checkout 'https://huggingface.co/D:\\PycharmProjects\\shuju\\pre_model\\ERNIE_1.0_max-len-512.tar/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m ernie_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mPycharmProjects\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mshuju\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mpre_model\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mERNIE_1.0_max-len-512.tar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 加载 ERNIE 模型和分词器\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mernie_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(ernie_model_name)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 打印模型架构\u001b[39;00m\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:733\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m--> 733\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    734\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    735\u001b[0m         )\n\u001b[0;32m    736\u001b[0m     config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1048\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1045\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1046\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1048\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1049\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1050\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\transformers\\configuration_utils.py:622\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    620\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    621\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 622\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m    624\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\transformers\\configuration_utils.py:677\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\transformers\\utils\\hub.py:401\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    402\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m         )\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: D:\\PycharmProjects\\shuju\\pre_model\\ERNIE_1.0_max-len-512.tar does not appear to have a file named config.json. Checkout 'https://huggingface.co/D:\\PycharmProjects\\shuju\\pre_model\\ERNIE_1.0_max-len-512.tar/None' for available files."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# ERNIE 模型的名称\n",
    "ernie_model_name = \"nghuyong/ernie-1.0\"\n",
    "\n",
    "# 加载 ERNIE 模型和分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(ernie_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ernie_model_name)\n",
    "\n",
    "# 打印模型架构\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad188f08-cce9-4fad-925e-a4a1ad110620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "210/210 [==============================] - 5s 24ms/step - loss: 0.6544 - accuracy: 0.7303 - val_loss: 0.6240 - val_accuracy: 0.7328\n",
      "Epoch 2/10\n",
      "210/210 [==============================] - 5s 23ms/step - loss: 0.6096 - accuracy: 0.7303 - val_loss: 0.5957 - val_accuracy: 0.7328\n",
      "Epoch 3/10\n",
      "210/210 [==============================] - 5s 23ms/step - loss: 0.5914 - accuracy: 0.7303 - val_loss: 0.5851 - val_accuracy: 0.7328\n",
      "Epoch 4/10\n",
      "210/210 [==============================] - 5s 23ms/step - loss: 0.5852 - accuracy: 0.7303 - val_loss: 0.5817 - val_accuracy: 0.7328\n",
      "Epoch 5/10\n",
      "210/210 [==============================] - 5s 23ms/step - loss: 0.5835 - accuracy: 0.7303 - val_loss: 0.5807 - val_accuracy: 0.7328\n",
      "Epoch 6/10\n",
      "210/210 [==============================] - 5s 23ms/step - loss: 0.5831 - accuracy: 0.7303 - val_loss: 0.5805 - val_accuracy: 0.7328\n",
      "Epoch 7/10\n",
      "210/210 [==============================] - 5s 23ms/step - loss: 0.5830 - accuracy: 0.7303 - val_loss: 0.5805 - val_accuracy: 0.7328\n",
      "Epoch 8/10\n",
      "210/210 [==============================] - 5s 23ms/step - loss: 0.5830 - accuracy: 0.7303 - val_loss: 0.5805 - val_accuracy: 0.7328\n",
      "Epoch 9/10\n",
      "210/210 [==============================] - 5s 23ms/step - loss: 0.5830 - accuracy: 0.7303 - val_loss: 0.5805 - val_accuracy: 0.7328\n",
      "Epoch 10/10\n",
      "210/210 [==============================] - 5s 23ms/step - loss: 0.5830 - accuracy: 0.7303 - val_loss: 0.5805 - val_accuracy: 0.7328\n",
      "Epoch 1/5\n",
      "837/837 [==============================] - 8s 9ms/step - loss: 0.5831 - accuracy: 0.7303 - val_loss: 0.5805 - val_accuracy: 0.7328\n",
      "Epoch 2/5\n",
      "837/837 [==============================] - 8s 9ms/step - loss: 0.5831 - accuracy: 0.7303 - val_loss: 0.5805 - val_accuracy: 0.7328\n",
      "Epoch 3/5\n",
      "837/837 [==============================] - 8s 9ms/step - loss: 0.5831 - accuracy: 0.7303 - val_loss: 0.5804 - val_accuracy: 0.7328\n",
      "Epoch 4/5\n",
      "837/837 [==============================] - 10s 12ms/step - loss: 0.5831 - accuracy: 0.7303 - val_loss: 0.5805 - val_accuracy: 0.7328\n",
      "Epoch 5/5\n",
      "837/837 [==============================] - 9s 10ms/step - loss: 0.5831 - accuracy: 0.7303 - val_loss: 0.5805 - val_accuracy: 0.7328\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.5805 - accuracy: 0.7328\n",
      "Test Loss: 0.5804939270019531, Test Accuracy: 0.7328153252601624\n",
      "105/105 [==============================] - 1s 5ms/step\n",
      "Precision: 0.0, Recall: 0.0, F1 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, ErnieModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# 读取数据\n",
    "data_path = 'merged_data_n.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "# 打乱数据\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 分割数据集为训练集和测试集\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# 读取预训练的 ERNIE 模型\n",
    "# ernie_model_name = \"nghuyong/ernie-1.0-base-zh\"\n",
    "ernie_model_name = 'D:\\\\PycharmProjects\\\\shuju\\\\pre_model\\\\ernie-3.0-mini-zh'\n",
    "tokenizer = BertTokenizer.from_pretrained(ernie_model_name)\n",
    "ernie_model = ErnieModel.from_pretrained(ernie_model_name)\n",
    "\n",
    "# 准备文本数据\n",
    "max_words = 5000\n",
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_data['segmented_text'])\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_words, len(word_index) + 1)\n",
    "\n",
    "# 建立空的 embedding_matrix\n",
    "embedding_matrix = np.zeros((num_words, ernie_model.config.hidden_size))\n",
    "\n",
    "# 对序列进行填充\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['segmented_text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['segmented_text'])\n",
    "\n",
    "train_inputs = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "test_inputs = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# 准备标签\n",
    "train_labels = train_data['label']\n",
    "test_labels = test_data['label']\n",
    "\n",
    "# 构建模型\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, ernie_model.config.hidden_size, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Conv1D(filters=100, kernel_size=3, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit(train_inputs, train_labels, epochs=10, batch_size=64, validation_data=(test_inputs, test_labels))\n",
    "\n",
    "# 冻结预训练模型的权重\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 微调模型\n",
    "model.fit(train_inputs, train_labels, epochs=5, batch_size=16, validation_data=(test_inputs, test_labels))\n",
    "\n",
    "# 评估模型\n",
    "loss, accuracy = model.evaluate(test_inputs, test_labels)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# 预测测试集\n",
    "predictions = np.argmax(model.predict(test_inputs), axis=1)\n",
    "\n",
    "# 计算混淆矩阵\n",
    "conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "# 计算精确率、召回率、F1值\n",
    "precision = precision_score(test_labels, predictions)\n",
    "recall = recall_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "\n",
    "print(f'Precision: {precision}, Recall: {recall}, F1 Score: {f1}')\n",
    "\n",
    "model.save('text_classification_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa45d4ee-0a2f-4744-b9b4-108b6c85c6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ErnieModel were not initialized from the model checkpoint at nghuyong/ernie-3.0-mini-zh and are newly initialized: ['ernie.pooler.dense.bias', 'ernie.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "210/210 [==============================] - 38s 169ms/step - loss: 0.6135 - accuracy: 0.7303 - val_loss: 0.5804 - val_accuracy: 0.7328\n",
      "Epoch 2/10\n",
      "210/210 [==============================] - 43s 206ms/step - loss: 0.5840 - accuracy: 0.7303 - val_loss: 0.5809 - val_accuracy: 0.7328\n",
      "Epoch 3/10\n",
      "210/210 [==============================] - 36s 170ms/step - loss: 0.5842 - accuracy: 0.7303 - val_loss: 0.5822 - val_accuracy: 0.7328\n",
      "Epoch 4/10\n",
      "210/210 [==============================] - 37s 176ms/step - loss: 0.5838 - accuracy: 0.7303 - val_loss: 0.5832 - val_accuracy: 0.7328\n",
      "Epoch 5/10\n",
      "210/210 [==============================] - 38s 181ms/step - loss: 0.5834 - accuracy: 0.7303 - val_loss: 0.5812 - val_accuracy: 0.7328\n",
      "Epoch 6/10\n",
      "210/210 [==============================] - 35s 167ms/step - loss: 0.5837 - accuracy: 0.7303 - val_loss: 0.5819 - val_accuracy: 0.7328\n",
      "Epoch 7/10\n",
      "210/210 [==============================] - 36s 174ms/step - loss: 0.5831 - accuracy: 0.7303 - val_loss: 0.5830 - val_accuracy: 0.7328\n",
      "Epoch 8/10\n",
      "210/210 [==============================] - 38s 179ms/step - loss: 0.5840 - accuracy: 0.7303 - val_loss: 0.5810 - val_accuracy: 0.7328\n",
      "Epoch 9/10\n",
      "210/210 [==============================] - 35s 168ms/step - loss: 0.5839 - accuracy: 0.7303 - val_loss: 0.5866 - val_accuracy: 0.7328\n",
      "Epoch 10/10\n",
      "118/210 [===============>..............] - ETA: 14s - loss: 0.5826 - accuracy: 0.7315"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# 评估模型\u001b[39;00m\n\u001b[0;32m     61\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_inputs, test_labels)\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, ErnieModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 读取数据\n",
    "data_path = 'merged_data_n.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "# 打乱数据\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 分割数据集为训练集和测试集\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# 读取预训练的 ERNIE 模型\n",
    "ernie_model_name = \"nghuyong/ernie-3.0-mini-zh\"\n",
    "tokenizer = BertTokenizer.from_pretrained(ernie_model_name)\n",
    "ernie_model = ErnieModel.from_pretrained(ernie_model_name)\n",
    "\n",
    "# 准备文本数据\n",
    "max_words = 5000\n",
    "max_sequence_length = 32  # 将文本处理长度设置为32\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_data['segmented_text'])\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_words, len(word_index) + 1)\n",
    "\n",
    "# 建立空的 embedding_matrix\n",
    "embedding_matrix = np.zeros((num_words, ernie_model.config.hidden_size))\n",
    "\n",
    "# 对序列进行填充\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['segmented_text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['segmented_text'])\n",
    "\n",
    "train_inputs = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "test_inputs = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# 准备标签\n",
    "train_labels = train_data['label']\n",
    "test_labels = test_data['label']\n",
    "\n",
    "# 构建模型\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, ernie_model.config.hidden_size, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Bidirectional(LSTM(ernie_model.config.hidden_size, dropout=0.1)))\n",
    "model.add(Dense(1, activation='sigmoid'))  # 由于是二分类任务，输出层使用sigmoid激活函数\n",
    "\n",
    "# 编译模型\n",
    "optimizer = Adam(learning_rate=3e-5)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit(train_inputs, train_labels, epochs=10, batch_size=64, validation_data=(test_inputs, test_labels))\n",
    "\n",
    "# 评估模型\n",
    "loss, accuracy = model.evaluate(test_inputs, test_labels)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e183ac2-dfa2-4628-938f-ccfbe987d443",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer embedding_9 weight shape (3856, 384) is not compatible with provided weight shape (3856, 300).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# 构建模型\u001b[39;00m\n\u001b[0;32m     49\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m---> 50\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mernie_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_sequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43membedding_matrix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Bidirectional(LSTM(ernie_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)))\n\u001b[0;32m     52\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m))  \u001b[38;5;66;03m# 由于是二分类任务，输出层使用sigmoid激活函数\u001b[39;00m\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\keras\\src\\engine\\base_layer.py:1822\u001b[0m, in \u001b[0;36mLayer.set_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   1820\u001b[0m ref_shape \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ref_shape\u001b[38;5;241m.\u001b[39mis_compatible_with(weight_shape):\n\u001b[1;32m-> 1822\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1823\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weight shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1824\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not compatible with provided weight \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1825\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1826\u001b[0m     )\n\u001b[0;32m   1827\u001b[0m weight_value_tuples\u001b[38;5;241m.\u001b[39mappend((param, weight))\n\u001b[0;32m   1828\u001b[0m weight_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Layer embedding_9 weight shape (3856, 384) is not compatible with provided weight shape (3856, 300)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# 读取数据\n",
    "data_path = 'merged_data_n.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# 分割数据集为训练集和测试集\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 读取预训练的 SGNS 词向量模型\n",
    "sgns_model_path = 'D:\\\\PycharmProjects\\\\shuju\\\\pre_model\\\\sgns.baidubaike.bigram-char'  # 替换为你的 SGNS 模型文件路径\n",
    "sgns_model = KeyedVectors.load_word2vec_format(sgns_model_path, binary=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "# 准备文本数据\n",
    "max_words = 5000\n",
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_data['segmented_text'])\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_words, len(word_index) + 1)\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, sgns_model.vector_size))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words and word in sgns_model:\n",
    "        embedding_matrix[i] = sgns_model[word]\n",
    "\n",
    "# 对序列进行填充\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['segmented_text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['segmented_text'])\n",
    "\n",
    "train_inputs = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "test_inputs = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# 准备标签\n",
    "train_labels = train_data['label']\n",
    "test_labels = test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ca1d417-5eb1-47e3-b8f7-b72b2e1d389c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "210/210 [==============================] - 103s 480ms/step - loss: 0.5391 - accuracy: 0.7367 - val_loss: 0.5072 - val_accuracy: 0.7654\n",
      "Epoch 2/10\n",
      "210/210 [==============================] - 108s 516ms/step - loss: 0.4168 - accuracy: 0.8155 - val_loss: 0.3861 - val_accuracy: 0.8347\n",
      "Epoch 3/10\n",
      "210/210 [==============================] - 110s 523ms/step - loss: 0.3705 - accuracy: 0.8426 - val_loss: 0.3650 - val_accuracy: 0.8461\n",
      "Epoch 4/10\n",
      "210/210 [==============================] - 115s 550ms/step - loss: 0.3484 - accuracy: 0.8573 - val_loss: 0.3458 - val_accuracy: 0.8527\n",
      "Epoch 5/10\n",
      "210/210 [==============================] - 111s 530ms/step - loss: 0.3355 - accuracy: 0.8605 - val_loss: 0.3390 - val_accuracy: 0.8589\n",
      "Epoch 6/10\n",
      "210/210 [==============================] - 103s 490ms/step - loss: 0.3276 - accuracy: 0.8663 - val_loss: 0.3334 - val_accuracy: 0.8586\n",
      "Epoch 7/10\n",
      "210/210 [==============================] - 114s 540ms/step - loss: 0.3185 - accuracy: 0.8711 - val_loss: 0.3350 - val_accuracy: 0.8625\n",
      "Epoch 8/10\n",
      "210/210 [==============================] - 99s 474ms/step - loss: 0.3138 - accuracy: 0.8749 - val_loss: 0.3255 - val_accuracy: 0.8643\n",
      "Epoch 9/10\n",
      "210/210 [==============================] - 99s 473ms/step - loss: 0.3023 - accuracy: 0.8793 - val_loss: 0.3440 - val_accuracy: 0.8604\n",
      "Epoch 10/10\n",
      "210/210 [==============================] - 99s 473ms/step - loss: 0.2913 - accuracy: 0.8824 - val_loss: 0.3232 - val_accuracy: 0.8658\n",
      "105/105 [==============================] - 8s 73ms/step - loss: 0.3232 - accuracy: 0.8658\n",
      "Test Loss: 0.32320135831832886, Test Accuracy: 0.8658099174499512\n"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "model = Sequential()\n",
    "# model.add(Embedding(num_words, ernie_model.config.hidden_size, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Embedding(num_words, sgns_model.vector_size, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Bidirectional(LSTM(ernie_model.config.hidden_size, dropout=0.1)))\n",
    "model.add(Dense(1, activation='sigmoid'))  # 由于是二分类任务，输出层使用sigmoid激活函数\n",
    "\n",
    "# 编译模型\n",
    "optimizer = Adam(learning_rate=3e-5)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit(train_inputs, train_labels, epochs=10, batch_size=64, validation_data=(test_inputs, test_labels))\n",
    "\n",
    "# 评估模型\n",
    "loss, accuracy = model.evaluate(test_inputs, test_labels)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71ca9a0f-dc50-40c9-950c-f1c99ef6cc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/10, Loss: 0.36330610513687134, Test Loss: 0.5539846420288086\n",
      "Epoch 2/10, Loss: 0.21545758843421936, Test Loss: 0.47787755727767944\n",
      "Epoch 3/10, Loss: 0.5565760135650635, Test Loss: 0.4845725893974304\n",
      "Epoch 4/10, Loss: 0.6278083324432373, Test Loss: 0.4349423348903656\n",
      "Epoch 5/10, Loss: 0.23739856481552124, Test Loss: 0.4065406322479248\n",
      "Epoch 6/10, Loss: 0.8047160506248474, Test Loss: 0.3873452842235565\n",
      "Epoch 7/10, Loss: 0.3495711088180542, Test Loss: 0.37704014778137207\n",
      "Epoch 8/10, Loss: 0.7974711656570435, Test Loss: 0.3583381474018097\n",
      "Epoch 9/10, Loss: 0.2557772397994995, Test Loss: 0.3660028278827667\n",
      "Epoch 10/10, Loss: 0.37347060441970825, Test Loss: 0.38243746757507324\n",
      "Test Accuracy: 0.8335325717926025\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 将数据转换为 PyTorch 张量\n",
    "train_inputs_tensor = torch.tensor(train_inputs, dtype=torch.long)\n",
    "train_labels_tensor = torch.tensor(train_labels.values, dtype=torch.float32).unsqueeze(1)  # 二分类任务需要将标签转换为列向量\n",
    "test_inputs_tensor = torch.tensor(test_inputs, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# 构建 DataLoader\n",
    "train_dataset = TensorDataset(train_inputs_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_inputs_tensor, test_labels_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 检查 GPU 是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 构建模型\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size=384):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_matrix.shape[1], hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)  # 由于是二分类任务，输出层只有一个单元\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out.mean(dim=1)  # 使用平均池化\n",
    "        output = torch.sigmoid(self.fc(lstm_out))\n",
    "        return output\n",
    "\n",
    "model = SentimentModel(embedding_matrix)\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 在测试集上进行评估\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(test_inputs_tensor.to(device))\n",
    "        test_loss = criterion(test_outputs, test_labels_tensor.to(device))\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}, Test Loss: {test_loss.item()}')\n",
    "\n",
    "# 评估模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_inputs_tensor.to(device))\n",
    "    test_outputs = (test_outputs > 0.5).float()  # 将概率转换为二分类输出\n",
    "    accuracy = (test_outputs == test_labels_tensor.to(device)).float().mean().item()\n",
    "\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dac5e5a8-e6eb-4151-8641-38af569f2049",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 728. MiB for an array with shape (635974, 300) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 读取预训练的 SGNS 词向量模型\u001b[39;00m\n\u001b[0;32m     29\u001b[0m sgns_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mPycharmProjects\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mshuju\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mpre_model\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msgns.baidubaike.bigram-char\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 30\u001b[0m sgns_model \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43msgns_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# 准备文本数据\u001b[39;00m\n\u001b[0;32m     33\u001b[0m max_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:2062\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m limit:\n\u001b[0;32m   2061\u001b[0m     vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(vocab_size, limit)\n\u001b[1;32m-> 2062\u001b[0m kv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m   2065\u001b[0m     _word2vec_read_binary(\n\u001b[0;32m   2066\u001b[0m         fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001b[0;32m   2067\u001b[0m     )\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:245\u001b[0m, in \u001b[0;36mKeyedVectors.__init__\u001b[1;34m(self, vector_size, count, dtype, mapfile_path)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# pointer to where next new entry will land\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_to_index \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors \u001b[38;5;241m=\u001b[39m \u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# formerly known as syn0\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# \"expandos\" are extra attributes stored for each key: {attribute_name} => numpy array of values of\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# this attribute, with one array value for each vector key.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# The same information used to be stored in a structure called Vocab in Gensim <4.0.0, but\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# with different indexing: {vector key} => Vocab object containing all attributes for the given vector key.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# Don't modify expandos directly; call set_vecattr()/get_vecattr() instead.\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 728. MiB for an array with shape (635974, 300) and data type float32"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Bidirectional, LSTM\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 读取数据\n",
    "data_path = 'merged_data_n.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "# 打乱数据\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 分割数据集为训练集和测试集\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# 读取预训练的 SGNS 词向量模型\n",
    "sgns_model_path = 'D:\\\\PycharmProjects\\\\shuju\\\\pre_model\\\\sgns.baidubaike.bigram-char'\n",
    "sgns_model = KeyedVectors.load_word2vec_format(sgns_model_path, binary=False, encoding='utf-8')\n",
    "\n",
    "# 准备文本数据\n",
    "max_words = 5000\n",
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_data['segmented_text'])\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_words, len(word_index) + 1)\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, sgns_model.vector_size))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words and word in sgns_model:\n",
    "        embedding_matrix[i] = sgns_model[word]\n",
    "\n",
    "# 对序列进行填充\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['segmented_text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['segmented_text'])\n",
    "\n",
    "train_inputs = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "test_inputs = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# 准备标签\n",
    "train_labels = train_data['label']\n",
    "test_labels = test_data['label']\n",
    "\n",
    "# 构建 DataLoader\n",
    "train_inputs_tensor = torch.tensor(train_inputs, dtype=torch.long)\n",
    "train_labels_tensor = torch.tensor(train_labels.values, dtype=torch.float32).unsqueeze(1)\n",
    "test_inputs_tensor = torch.tensor(test_inputs, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(train_inputs_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_inputs_tensor, test_labels_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 定义微调模型\n",
    "class FineTunedModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size=384):\n",
    "        super(FineTunedModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_matrix.shape[1], hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out.mean(dim=1)\n",
    "        output = torch.sigmoid(self.fc(lstm_out))\n",
    "        return output\n",
    "\n",
    "# 创建微调模型并移动到 GPU\n",
    "finetuned_model = FineTunedModel(embedding_matrix)\n",
    "finetuned_model = finetuned_model.to(device)\n",
    "\n",
    "# 定义微调优化器和学习率\n",
    "finetune_optimizer = optim.Adam([\n",
    "    {'params': finetuned_model.embedding.parameters(), 'lr': 3e-5},\n",
    "    {'params': finetuned_model.lstm.parameters()},\n",
    "    {'params': finetuned_model.fc.parameters()}\n",
    "], lr=3e-4)\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 微调模型\n",
    "num_finetune_epochs = 50\n",
    "for epoch in range(num_finetune_epochs):\n",
    "    finetuned_model.train()\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        finetune_optimizer.zero_grad()\n",
    "        outputs = finetuned_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        finetune_optimizer.step()\n",
    "\n",
    "    # 在测试集上进行评估\n",
    "    finetuned_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = finetuned_model(test_inputs_tensor.to(device))\n",
    "        test_loss = criterion(test_outputs, test_labels_tensor.to(device))\n",
    "\n",
    "        # 计算准确率\n",
    "        test_outputs = (test_outputs > 0.5).float()\n",
    "        accuracy = (test_outputs == test_labels_tensor.to(device)).float().mean().item()\n",
    "\n",
    "    print(f'Fine-tune Epoch {epoch + 1}/{num_finetune_epochs}, Loss: {loss.item()}, Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# 训练模型\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size=384):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_matrix.shape[1], hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out.mean(dim=1)\n",
    "        output = torch.sigmoid(self.fc(lstm_out))\n",
    "        return output\n",
    "\n",
    "model = SentimentModel(embedding_matrix)\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 150\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 在测试集上进行评估\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(test_inputs_tensor.to(device))\n",
    "        test_loss = criterion(test_outputs, test_labels_tensor.to(device))\n",
    "\n",
    "        # 计算准确率\n",
    "        test_outputs = (test_outputs > 0.5).float()\n",
    "        accuracy = (test_outputs == test_labels_tensor.to(device)).float().mean().item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}, Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# 最终评估微调后的模型\n",
    "finetuned_model.eval()\n",
    "with torch.no_grad():\n",
    "    finetuned_outputs = finetuned_model(test_inputs_tensor.to(device))\n",
    "    finetuned_outputs = (finetuned_outputs > 0.5).float()\n",
    "    finetuned_accuracy = (finetuned_outputs == test_labels_tensor.to(device)).float().mean().item()\n",
    "\n",
    "print(f'Fine-tuned Model Test Accuracy: {finetuned_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "291e86ab-aed1-4db4-849c-c0be4aa0ec0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'frombuffer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(bert_model_name)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 加载 BERT 模型\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m \u001b[43mBertModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 示例文本\u001b[39;00m\n\u001b[0;32m     13\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m你好，这是一个例子。\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:3170\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_pt:\n\u001b[0;32m   3168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded \u001b[38;5;129;01mand\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3169\u001b[0m         \u001b[38;5;66;03m# Time to load the checkpoint\u001b[39;00m\n\u001b[1;32m-> 3170\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3172\u001b[0m     \u001b[38;5;66;03m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[0;32m   3173\u001b[0m     \u001b[38;5;66;03m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[0;32m   3174\u001b[0m     \u001b[38;5;66;03m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[0;32m   3175\u001b[0m     \u001b[38;5;66;03m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[0;32m   3176\u001b[0m     \u001b[38;5;66;03m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[0;32m   3177\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:474\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    470\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    471\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe safetensors archive passed at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not contain the valid metadata. Make sure \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    472\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou save your model with the `save_pretrained` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    473\u001b[0m         )\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msafe_load_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    477\u001b[0m         (is_deepspeed_zero3_enabled() \u001b[38;5;129;01mor\u001b[39;00m is_fsdp_enabled())\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_initialized()\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mget_rank() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    480\u001b[0m     ):\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\safetensors\\torch.py:310\u001b[0m, in \u001b[0;36mload_file\u001b[1;34m(filename, device)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m safe_open(filename, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m--> 310\u001b[0m         result[k] \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'frombuffer'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 指定预训练的 BERT 模型名称\n",
    "bert_model_name = 'bert-base-chinese'\n",
    "\n",
    "# 加载 BERT 标记器\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# 加载 BERT 模型\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "# 示例文本\n",
    "text = \"你好，这是一个例子。\"\n",
    "\n",
    "# 将文本转换为 BERT 输入格式\n",
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# 获取 BERT 模型的输出\n",
    "outputs = bert_model(**tokens)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# 打印输出的形状\n",
    "print(last_hidden_states.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8525e1ea-1cac-4903-8e80-a04c606ccbc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
