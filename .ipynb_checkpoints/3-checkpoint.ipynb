{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc739a9-8150-4ffd-90fe-d1be88da7076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cafc1d69-4daa-4b0c-9570-c8da42bf8131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pre_model/bert/bert-base-chinese/ were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel\n",
    "vocab_file = 'pre_model/bert/vocab.txt' \n",
    "tokenizer = BertTokenizer(vocab_file)\n",
    "bert = BertModel.from_pretrained(f\"pre_model/bert/bert-base-chinese/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dd56b36-c5af-47d0-88b2-132312f3095c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8930e-01,  8.6803e-02,  5.9561e-01, -6.3697e-02,  4.4173e-01,\n",
      "         -3.8647e-01, -7.5449e-02, -2.0317e-01, -2.4175e-01,  7.6031e-01,\n",
      "         -9.2879e-01, -5.7380e-01,  5.5491e-01,  6.9278e-01,  7.2702e-01,\n",
      "         -6.9877e-02,  1.4392e-01,  9.6638e-02,  3.2259e-01, -2.4516e-02,\n",
      "         -5.1038e-01, -4.2958e-01,  5.3902e-02, -3.9051e-01,  3.1485e-01,\n",
      "         -1.5559e-02, -3.3520e-01, -2.0730e-01,  7.0059e-01,  2.5755e-02,\n",
      "          2.5953e-03, -3.1733e-01, -6.4245e-01, -2.2900e-01,  3.8249e-01,\n",
      "         -5.8165e-02, -1.1045e+00,  3.6437e-01, -6.2176e-01, -1.0923e-01,\n",
      "          9.8058e-02, -1.4315e-01, -3.3677e-01,  3.6641e-01, -2.8149e-02,\n",
      "         -2.1520e-01,  6.9553e-01, -1.4031e-01,  2.3408e-01, -4.9361e-02,\n",
      "          3.6833e-01,  8.9785e+00,  7.1871e-01,  6.8660e-02,  1.2949e-02,\n",
      "          6.6136e-01,  3.1707e-01, -1.8415e-01, -2.0841e-01, -5.1593e-01,\n",
      "         -5.0333e-01, -5.5805e-02,  5.2764e-01,  3.5372e-01, -2.9743e-01,\n",
      "         -3.6874e-01, -2.3805e-01,  2.0433e-01, -3.7476e-02,  4.1759e-01,\n",
      "          4.0999e-01, -3.8283e-01, -1.3688e-01,  9.3738e-01, -3.4330e-01,\n",
      "         -5.1846e-01, -8.7744e-02, -6.9608e-01, -2.3880e-01, -2.7870e-02,\n",
      "         -7.2718e-01, -1.0385e-01, -1.8170e-01, -2.1938e-01, -7.8849e-01,\n",
      "          7.3732e-03, -6.1770e-01, -6.5531e-01,  3.1644e-01, -5.2943e-01,\n",
      "         -5.4009e-01, -8.4513e-02, -6.1839e-01, -1.2428e-01,  5.3068e-01,\n",
      "          2.3768e-01, -2.0996e-01,  3.7863e-01, -1.9047e-01,  9.2508e-04,\n",
      "         -8.7194e-01,  3.4665e-01,  5.1922e-01,  4.1833e-01, -8.5257e-01,\n",
      "          2.3359e-01, -1.9440e-01,  8.1078e-01, -4.5272e-01, -4.2397e-01,\n",
      "         -8.1707e-01, -1.5913e-01, -8.1372e-01, -4.3366e-01,  2.0865e-01,\n",
      "         -9.9103e-02, -1.9107e-01,  1.0156e+00, -1.8716e-01,  1.4863e-01,\n",
      "          9.4075e-02,  5.7701e-01,  5.5287e-01, -3.8627e-01,  4.8590e-01,\n",
      "         -1.0329e+00,  4.7336e-02, -2.7082e-01, -1.2277e-01,  2.2606e-01,\n",
      "         -4.7217e-01,  2.6869e-01,  7.0965e-01, -2.3541e-01,  3.7688e-01,\n",
      "         -8.7647e-02, -3.2500e-01, -4.0468e-02, -9.3382e-01,  3.0322e-01,\n",
      "          4.7839e-01, -6.1843e-01,  1.0583e-01,  3.6654e-01,  2.5631e-01,\n",
      "          5.0839e-01, -2.7257e-01, -1.3628e-01,  5.1375e-01,  3.2790e-01,\n",
      "         -7.4568e-01, -5.0199e-01,  2.8307e-01, -6.8440e-01, -5.3643e-01,\n",
      "         -7.7809e-01, -3.4718e-01, -7.3596e-01, -6.5878e-01, -9.4552e-01,\n",
      "          1.6128e-01,  3.7932e-01, -2.2873e-01,  1.8098e-01, -3.2484e-01,\n",
      "         -1.0995e-01, -4.7549e-01, -1.5771e-01, -5.8794e-01, -2.6388e-01,\n",
      "          4.3090e-01,  8.2562e-01, -1.0416e-01,  7.3994e-01, -1.0298e-01,\n",
      "          6.3119e-01, -8.5295e-01, -6.1079e-01,  1.2647e-01, -3.5019e-01,\n",
      "         -4.1812e-01,  3.2755e-01,  3.7450e-01, -5.9598e-01,  1.1172e-01,\n",
      "          7.0334e-01, -2.0608e-01,  6.8172e-01, -5.0855e-01, -1.6127e-01,\n",
      "         -2.3645e-01, -2.7044e-01, -7.7964e-01, -2.2255e-01,  1.0566e-01,\n",
      "          7.7660e-03, -3.2917e-02, -3.3293e-01,  7.9758e-01,  6.8490e-01,\n",
      "         -2.6931e-01, -5.3116e-01, -2.6654e-01, -1.5781e-01, -3.9488e-02,\n",
      "          2.3041e-02,  5.4270e-01,  2.0730e-01, -6.3224e-01, -8.9008e-02,\n",
      "         -7.6804e-01,  1.6909e-01,  4.1698e-01, -7.1860e-01,  5.4306e-01,\n",
      "          3.1425e-01,  3.2075e-01, -1.2153e-01, -1.9029e-01, -7.3017e-01,\n",
      "          3.1036e-02,  6.0022e-02, -2.9746e-01, -2.1979e-01,  5.6601e-02,\n",
      "         -4.8308e-02, -4.0062e-01,  3.4259e-01,  1.9215e-01, -5.6110e-01,\n",
      "         -7.9598e-02,  2.4941e-01, -1.5434e-01, -2.0188e-01, -5.1609e-02,\n",
      "         -3.8699e-02,  1.3604e-02, -4.3885e-01, -2.2576e-01,  7.3560e-01,\n",
      "          3.1453e-01,  1.1374e-01, -5.2802e-01, -2.0374e-01, -5.7048e-01,\n",
      "          1.5681e-01, -6.4508e-01, -4.7401e-02,  1.7369e-01,  4.4272e-01,\n",
      "         -2.2380e-01,  2.8875e-01, -2.5812e-01, -6.1568e-01, -1.4455e-01,\n",
      "          1.7885e-01,  2.0966e-01, -8.1725e-02, -5.3674e-01,  7.4680e-01,\n",
      "         -6.2675e-01,  3.2016e-01,  6.7660e-01, -1.0664e-01,  1.6175e+00,\n",
      "         -6.1683e-01,  2.6157e-01, -2.8362e-01, -1.4771e-01,  2.5878e-01,\n",
      "          1.3302e-01, -3.8943e-01,  2.4402e-01, -9.3384e-01, -5.3503e-01,\n",
      "         -3.7984e-01,  1.8870e-01, -3.6388e-01,  3.8598e-01, -5.9712e-01,\n",
      "         -1.2044e-01, -1.4003e-01, -5.3914e-02, -8.3504e-01, -5.4961e-01,\n",
      "         -4.0505e-01,  7.2428e-02,  9.5633e-02,  3.5252e-01, -5.1301e-01,\n",
      "          2.1983e-02, -2.3408e-01, -1.7625e-01,  8.9932e-02,  1.2288e+00,\n",
      "          2.0667e-01,  7.2033e-02, -2.1906e-01,  3.8463e-01,  1.5218e-01,\n",
      "         -2.8710e-01,  4.2798e-02, -2.1571e-01,  1.0241e+00, -6.2843e-01,\n",
      "         -1.1395e-01,  2.0176e-01, -2.1490e-02,  8.7234e-01, -2.4048e-01,\n",
      "          8.6153e-02, -5.9267e-02, -6.3969e-01, -4.3666e-01, -1.0178e+00,\n",
      "         -7.5526e-01,  1.2573e-01, -4.2211e-01, -5.3156e-01,  1.5086e-01,\n",
      "         -8.3582e-02,  7.5553e-02, -2.9900e-02,  3.9881e-01,  1.0936e-01,\n",
      "          1.2986e-01,  3.1105e-01,  1.3824e-01,  8.3075e-02,  2.9124e-01,\n",
      "          1.4680e-01, -3.2341e-01, -1.1289e-01,  3.3968e-01,  7.8429e-02,\n",
      "          3.2103e-01, -2.8976e-01, -1.4483e-01,  2.6322e-02, -8.1518e-01,\n",
      "          3.6470e-02, -4.0646e-01,  7.9263e-01,  1.3958e-02,  1.8100e-01,\n",
      "         -2.0258e-01, -1.3485e-01, -7.0151e-01,  2.1725e-01,  2.2501e-01,\n",
      "         -8.3271e-01,  1.0134e+00, -4.1113e-01, -5.9641e-02, -7.1667e-01,\n",
      "          6.3803e-01, -2.0686e-01, -1.0746e-01,  6.8767e-02,  4.9708e-02,\n",
      "          7.6045e-01,  2.4181e-02,  3.3677e-01, -4.7637e-01,  1.5633e-01,\n",
      "          3.8999e-01,  3.1157e-01, -1.8224e-01,  3.7011e-02, -1.1038e-01,\n",
      "          1.9565e-01, -1.0002e-01,  6.9038e-01, -6.1333e-01,  1.4920e-01,\n",
      "          4.4423e-02,  1.7827e-01, -2.2274e-01, -2.1894e-01, -4.8706e-01,\n",
      "         -8.6112e-01,  8.6077e-01,  4.3229e-01, -6.0843e-01, -4.3880e-01,\n",
      "          2.3072e-01,  2.4195e-02,  4.0501e-02,  6.7833e-01, -5.7069e-02,\n",
      "         -4.5820e-01,  3.8516e-01,  3.4924e-01, -4.1177e-01,  4.0970e-01,\n",
      "          8.1121e-01, -4.2645e-01,  1.4706e-01,  7.8328e-02, -5.5255e-01,\n",
      "          1.2885e-01, -2.5304e-01,  8.5920e-02,  1.5034e-02,  1.6126e-01,\n",
      "         -1.8112e+00,  1.7436e-01,  1.6848e-02,  1.0968e+00, -3.3621e-01,\n",
      "         -8.6743e-02,  5.2398e-01,  9.3017e-01, -4.2317e-02,  2.1063e-02,\n",
      "         -5.6740e-02,  5.1011e-01, -1.2855e-02,  4.7514e-01,  1.2747e-01,\n",
      "          2.4491e-01, -8.1687e-02, -8.3761e-01, -3.8379e-01,  3.9146e-01,\n",
      "         -6.9187e-01, -1.6176e-02, -1.9126e-01,  1.2144e+00,  2.5798e-03,\n",
      "         -2.8416e-02,  4.2304e-01, -7.8382e-01,  8.1518e-02,  5.2022e-02,\n",
      "          8.6583e-01, -5.9944e-01, -4.5496e-02, -3.7514e-01, -1.7973e-01,\n",
      "          3.2452e-01, -5.9238e-01,  9.4761e-02,  5.1017e-01,  9.5086e-03,\n",
      "          8.7416e-03,  1.0042e+00, -7.2214e-02,  2.3461e-03, -1.8561e-03,\n",
      "          4.8329e-01,  1.1625e-01,  8.4029e-02, -5.9816e-01, -2.8728e-01,\n",
      "         -2.0211e-01,  2.7909e-01,  6.0627e-01, -3.6217e-01,  5.8809e-01,\n",
      "          2.2377e-01,  2.1310e-01,  1.8292e-01,  4.2667e-01,  6.3095e-03,\n",
      "          5.5827e-03, -1.7389e-01, -1.9026e-01,  2.2686e-01, -5.3131e-01,\n",
      "          2.5406e-03, -1.6074e-02,  1.1104e-01,  2.7483e-01, -1.9332e-01,\n",
      "         -4.8281e-01,  7.2812e-03, -2.0029e-01, -8.4751e-01, -4.5377e-01,\n",
      "          2.1780e-01, -1.2243e-01,  9.3322e-01, -2.1501e-01,  7.0830e-01,\n",
      "          1.5008e-01,  2.4933e-01, -3.9691e-01,  6.6357e-01,  4.8613e-01,\n",
      "         -7.6551e-01, -4.6698e-01,  1.2563e+00, -7.1255e-01,  8.1782e-02,\n",
      "         -2.7021e-01,  3.3119e-01,  3.5687e-01,  2.9893e-01,  4.2952e-01,\n",
      "          1.3757e-01, -6.1965e-01,  2.8994e-01, -8.3232e-01, -3.8155e-01,\n",
      "          7.3597e-02,  5.4942e-01,  2.5378e-01,  1.4182e-03, -2.3826e-01,\n",
      "         -7.6024e-01, -3.6971e-01, -8.2765e-01,  9.3305e-02, -2.3102e-01,\n",
      "         -1.3443e-01,  8.2679e-02, -1.4799e-01,  5.1481e-02,  7.3663e-01,\n",
      "         -3.8794e-01, -4.9536e-01, -4.5831e-02,  1.3408e-01,  6.9789e-01,\n",
      "         -1.9021e-01, -1.4077e-01, -8.8303e-02, -2.3353e-01,  2.6085e-01,\n",
      "         -2.9454e-01,  4.6374e-01, -7.9966e-01, -3.7588e-01,  1.7863e-01,\n",
      "          5.9260e-01,  2.4938e-01, -1.0123e+00,  1.0915e-01,  3.9768e-01,\n",
      "         -5.6997e-02, -9.7657e-01, -1.0942e-01, -7.2729e-01, -1.3853e-01,\n",
      "         -2.5915e-01, -9.2440e-01,  2.5079e-01, -4.1477e-01,  1.5646e-02,\n",
      "         -1.7579e-01, -6.0639e-01, -1.0116e+00,  4.0838e-01, -1.9016e-01,\n",
      "         -1.0652e+00, -1.8928e-01,  5.3225e-01,  4.3492e-01,  1.8747e-01,\n",
      "          5.3847e-01, -3.6537e-01, -7.5460e-01,  4.7398e-01, -8.2004e-01,\n",
      "          2.4412e-01,  4.0334e-03, -4.2692e-01, -1.1276e+00, -5.3143e-01,\n",
      "         -1.0134e+00, -1.7502e-01,  4.7529e-01, -1.7643e-01,  5.3649e-01,\n",
      "         -2.5370e-01,  1.0474e-01,  5.8352e-01,  6.9682e-01, -2.4904e-01,\n",
      "          4.1253e-01,  4.9731e-01, -1.7626e-01, -3.3424e-01, -1.1293e+00,\n",
      "         -5.8791e-01,  2.4087e-01,  2.6727e-01,  2.4195e-02, -2.8712e-01,\n",
      "          3.6506e-01,  2.0437e-01, -2.9860e-01, -4.7480e-01, -3.3477e-02,\n",
      "         -1.1479e-01, -1.5036e-01,  3.5493e-01, -2.9227e-01,  3.9676e-02,\n",
      "          5.6973e-01, -8.3218e-01,  5.5961e-01,  4.6441e-01,  5.9103e-01,\n",
      "         -6.0183e-01,  6.5891e-01, -3.1562e-01, -2.3639e-01, -5.0188e-01,\n",
      "          1.1305e-01, -7.6865e-02,  1.5273e-01,  3.9816e-01, -8.3187e-01,\n",
      "          2.4475e-01,  2.3496e-01, -3.8624e-02,  2.2307e-01, -7.9256e-01,\n",
      "         -2.3592e-01,  1.9866e-01,  4.0667e-01,  4.3921e-01,  4.3212e-01,\n",
      "          1.0219e-01,  1.4339e+00, -1.3652e-02,  1.0679e-01, -7.3190e-01,\n",
      "         -3.5213e-01,  5.4959e-02,  1.0481e-01,  1.4104e-01,  4.8297e-01,\n",
      "         -3.7271e-01,  9.9579e-02,  4.2429e-01,  3.5931e-01, -2.2633e-02,\n",
      "          3.0289e-01, -4.3232e-01,  6.2544e-01,  2.2510e-02,  1.5381e-01,\n",
      "         -1.9057e-01,  6.0349e-01, -3.5622e-02, -2.5454e-01, -8.5510e-01,\n",
      "         -2.0913e-01, -6.3755e-02, -5.8108e-02, -3.7827e-01,  8.1696e-01,\n",
      "          1.9696e-01,  1.1342e-01, -1.7813e-01, -4.4117e-01,  2.1732e-01,\n",
      "          4.7580e-01,  1.9542e-02,  5.1670e-01,  6.3820e-01, -4.2669e-01,\n",
      "          5.6782e-01, -5.9079e-01, -7.0635e-01, -8.5598e-02,  7.1951e-01,\n",
      "          4.3731e-01, -1.3088e-01,  6.0311e-01, -5.0466e-01,  5.2438e-01,\n",
      "         -3.0072e-02,  1.1248e-01, -4.5981e-02,  3.6103e-02,  6.7669e-02,\n",
      "          1.4556e-01, -8.2263e-01, -1.3372e-02, -1.1560e-01,  3.6133e-01,\n",
      "         -2.1811e-01,  6.2019e-01, -3.3704e-01, -5.7044e-01, -1.5221e-01,\n",
      "          1.0786e+00,  3.7992e-01,  6.7876e-02, -4.3989e-01, -9.3368e-01,\n",
      "          8.3392e+00,  1.4420e-01,  2.9432e-01, -1.7240e-01, -4.4244e-01,\n",
      "         -3.0069e-03,  5.7223e-01, -4.1432e-01,  5.4200e-01, -1.5525e+00,\n",
      "         -3.0839e-01,  1.7653e-01,  5.5674e-01, -3.5025e-01, -3.3261e-01,\n",
      "         -3.2830e-01,  9.0948e-01,  3.3682e-01, -3.6920e-01, -1.7299e-03,\n",
      "          7.9257e-01,  2.5178e-02, -6.4369e-01,  4.1414e-01,  1.0813e+00,\n",
      "          1.8228e-01, -1.1332e+00,  2.1582e-01,  1.4649e-01,  7.1612e-01,\n",
      "         -7.3041e-01, -6.7848e-01,  6.4271e-01,  6.0398e-01,  1.0758e-01,\n",
      "          3.7995e-01, -7.2764e-01, -6.1100e-01,  4.1988e-01, -3.0838e-01,\n",
      "          8.8703e-01, -2.2365e-01, -2.2358e-01,  6.2809e-02,  9.9640e-01,\n",
      "          2.5230e-01, -1.4101e-01,  1.7806e-01, -2.2513e-01,  2.3867e-01,\n",
      "         -2.7054e-01,  1.4169e-01, -8.7460e-01,  1.6359e-01,  1.4822e+00,\n",
      "         -3.8059e-01, -9.5911e-01,  1.4724e-01, -2.9638e-01,  2.3158e-01,\n",
      "         -6.7434e-01,  2.5796e+00, -2.8078e-01,  2.4621e-01,  7.3273e-02,\n",
      "          8.3360e-01, -5.5293e-01, -5.1733e-01, -3.5317e-01, -5.1058e-02,\n",
      "          3.8752e-01,  4.7311e-01, -1.7359e-01]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel\n",
    "vocab_file = 'pre_model/bert/vocab.txt' \n",
    "tokenizer = BertTokenizer(vocab_file)\n",
    "bert = BertModel.from_pretrained(f\"pre_model/bert/bert-base-chinese/\")\n",
    "import torch\n",
    "text = \"这是一个文本示例。\"\n",
    "tokens = tokenizer.encode_plus(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "outputs = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "sentence_embedding = torch.mean(last_hidden_state, dim=1)\n",
    "print(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c22f703c-782b-4094-a731-de1b35fb00b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pre_model/bert/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.9198e-01,  2.0186e-01,  9.5694e-02,  1.0659e-01,  2.1093e-01,\n",
      "         -3.1002e-02,  1.1452e-01,  1.3456e-01, -1.9913e-01, -7.6997e-02,\n",
      "         -9.8602e-02, -5.7157e-01, -2.5186e-01,  1.2472e-01,  2.0938e-01,\n",
      "          1.1655e-01,  4.7091e-01, -5.1315e-02, -1.9652e-02,  2.6263e-01,\n",
      "         -1.7385e-01,  1.3070e-01,  1.9928e-01, -8.6780e-02,  2.7760e-01,\n",
      "         -4.7904e-01, -3.1096e-01, -4.7532e-01,  5.1795e-01, -5.7753e-01,\n",
      "         -2.9178e-01, -1.4486e-01, -2.8555e-01, -3.9246e-01, -8.6205e-02,\n",
      "         -4.0207e-01, -3.7120e-01,  1.2881e-01, -6.9989e-01,  8.8310e-02,\n",
      "          1.6152e-01, -4.8347e-02, -2.9126e-01,  8.3552e-02, -6.4441e-02,\n",
      "          8.6794e-02, -2.7367e-02, -1.3616e-01,  1.3668e-01,  1.0636e-01,\n",
      "          1.0426e-01,  9.7893e+00, -5.7068e-01, -6.9036e-03, -6.0817e-01,\n",
      "          3.3789e-01,  2.8414e-01, -9.7115e-03,  1.0558e-01, -3.8856e-02,\n",
      "         -1.8983e-01,  2.1047e-02, -4.3851e-01, -1.8087e-01,  2.5820e-03,\n",
      "          2.5146e-01,  3.1444e-02,  7.9317e-03,  3.7868e-02, -6.6997e-01,\n",
      "          8.0426e-02, -6.5573e-01, -1.6562e-01, -2.7846e-03,  2.7112e-01,\n",
      "          5.0002e-01,  3.5796e-01, -5.3182e-01, -7.7234e-03, -1.8468e-01,\n",
      "         -4.1917e-01,  7.7330e-02, -1.6165e-01,  1.2157e-01, -3.4383e-01,\n",
      "         -7.8481e-02, -1.1724e-01, -2.1555e-01,  3.8857e-01, -6.2194e-01,\n",
      "          1.9596e-02, -2.3894e-01,  4.5018e-01,  5.3171e-03,  4.1184e-01,\n",
      "         -2.9503e-01, -4.7770e-01,  6.7379e-02, -2.5095e-01, -4.0313e-01,\n",
      "         -5.8493e-01, -1.8267e-01, -8.9226e-02, -4.9053e-01, -5.5486e-01,\n",
      "          3.0432e-01, -2.5585e-01, -6.3194e-02, -2.9374e-01, -3.3903e-02,\n",
      "         -1.7800e-01, -5.7379e-01, -2.1225e-01,  6.5672e-02, -2.8283e-01,\n",
      "         -3.6530e-01, -7.2273e-02,  4.7643e-01, -4.3651e-02, -5.6175e-01,\n",
      "          4.0329e-01,  2.2482e-01, -2.1964e-01, -8.7658e-02, -9.8286e-02,\n",
      "         -8.8612e-01, -2.0974e-02, -3.7199e-01, -4.4768e-02, -5.7087e-02,\n",
      "         -3.2372e-03, -2.7496e-01,  5.5652e-01,  9.9114e-02,  5.3725e-02,\n",
      "         -5.8747e-01,  7.7844e-02, -1.9089e-01, -3.9344e-01,  4.3417e-01,\n",
      "          5.3958e-02, -1.0404e-01,  1.7610e-01,  3.3580e-01, -5.2573e-02,\n",
      "          3.1296e-01,  2.3486e-01, -2.0411e-01,  3.5541e-01, -1.3390e-01,\n",
      "         -2.7021e-01, -7.7194e-01,  3.5704e-02,  2.6270e-01, -2.5426e-01,\n",
      "         -8.8984e-02,  2.2250e-01, -4.6074e-01,  1.8175e-01, -4.6510e-01,\n",
      "         -3.0284e-01,  2.6521e-01,  2.6009e-01,  1.0230e+00,  3.9969e-01,\n",
      "         -4.3819e-01,  8.3440e-02, -4.6120e-02,  2.1455e-01,  9.8974e-02,\n",
      "         -5.9472e-02,  6.6188e-01, -6.3358e-02,  5.1785e-01, -1.2068e-01,\n",
      "          2.6794e-01, -3.2963e-01, -4.7919e-01,  9.0049e-03, -2.2792e-01,\n",
      "          1.3234e-02, -2.7772e-01,  7.6559e-02, -3.4619e-01,  6.3359e-01,\n",
      "          5.5682e-02,  3.2347e-01, -4.6128e-01, -2.8361e-01, -1.7834e-01,\n",
      "         -5.0372e-01,  2.2493e-01, -2.0694e-01, -4.6588e-02, -8.1708e-02,\n",
      "         -1.0569e-01, -4.0493e-01, -1.3725e-01,  1.7675e-01, -3.1021e-02,\n",
      "         -2.3936e-01, -6.0581e-02, -1.9685e-01,  9.6585e-02, -7.9352e-02,\n",
      "          3.1034e-01,  3.1514e-01,  5.6775e-02, -3.2250e-02, -4.8194e-01,\n",
      "         -6.1331e-01,  2.1134e-01, -5.3932e-02,  1.8385e-01,  3.7290e-01,\n",
      "         -1.5367e-01, -7.7027e-02,  3.7431e-01,  1.7486e-01,  1.8281e-01,\n",
      "          5.2051e-01, -2.5020e-01, -4.4148e-02,  6.9147e-02,  2.0207e-01,\n",
      "         -1.7904e-01, -2.6893e-01, -1.1622e-01,  2.8779e-01, -3.5993e-02,\n",
      "         -1.2965e-01,  2.0256e-01, -1.0310e-01,  7.1129e-02, -3.0596e-01,\n",
      "         -2.9036e-01, -8.9313e-03,  9.2010e-04, -5.8939e-01,  3.3783e-01,\n",
      "         -3.7497e-02,  2.0954e-01, -9.0972e-02, -8.1261e-02, -2.6420e-01,\n",
      "         -1.1295e-01, -5.0942e-01, -6.0978e-02, -3.0579e-01,  1.6559e-01,\n",
      "          2.1348e-02, -4.7754e-01, -1.6929e-01, -3.2141e-01, -1.4112e-01,\n",
      "         -1.6033e-01, -3.8346e-01,  2.7191e-01, -7.2883e-02,  4.4799e-01,\n",
      "          2.0813e-01,  4.0245e-01,  6.6792e-01, -6.7426e-01,  1.7404e+00,\n",
      "         -1.5300e-01, -2.3616e-01, -3.3257e-02,  3.7063e-01, -2.2258e-01,\n",
      "         -4.3218e-02,  2.6935e-01,  1.0300e-01, -7.6816e-01, -2.5968e-01,\n",
      "         -1.0144e-01, -6.3235e-03,  1.0508e-01,  1.2107e-01, -2.2291e-01,\n",
      "         -1.4436e-01,  1.2453e-01,  5.7000e-02, -1.8216e-01, -4.7112e-01,\n",
      "         -2.8006e-01, -2.8804e-01,  2.2579e-01, -1.9962e-01, -3.4991e-01,\n",
      "          8.9717e-02, -8.1502e-01,  5.8960e-02,  7.6034e-01,  5.9775e-02,\n",
      "          2.9111e-01,  4.5090e-01, -2.8819e-01,  8.8309e-02, -3.2662e-01,\n",
      "         -2.0243e-01, -3.9818e-01, -3.5310e-01, -2.0636e-01, -4.9213e-02,\n",
      "          8.9063e-03, -3.2354e-01,  1.1605e-01,  2.5656e-01,  9.1325e-02,\n",
      "          1.3832e-02, -2.8840e-01, -2.7527e-01,  5.3320e-01, -1.6672e+00,\n",
      "         -1.9172e-01,  3.8184e-01,  2.3766e-02, -8.4329e-02,  2.6842e-01,\n",
      "         -2.5912e-01, -4.2430e-02,  9.6433e-02,  4.2198e-01, -1.0891e-02,\n",
      "         -9.4749e-03,  3.5723e-01, -1.4099e-01,  2.1234e-01, -3.1093e-01,\n",
      "         -1.2003e-01,  1.3317e-01,  2.3483e-01, -2.2973e-01, -2.1371e-01,\n",
      "          2.1734e-01, -2.0519e-02, -4.2508e-02, -4.7680e-01, -6.0010e-01,\n",
      "          1.1237e-01, -9.7667e-02,  1.1815e+00,  2.5358e-01, -4.2281e-01,\n",
      "         -9.5544e-02,  2.8470e-01, -1.2003e-01,  8.6759e-02,  4.6169e-01,\n",
      "         -8.9165e-02,  3.4522e-01, -9.2452e-02,  8.9664e-02,  2.1466e-01,\n",
      "          1.1603e-01, -2.7478e-01, -1.7202e-01,  2.2828e-01, -1.2833e-01,\n",
      "         -2.4993e-01, -6.4416e-02,  4.3375e-01, -4.3385e-01, -1.2937e-01,\n",
      "          1.0953e-01, -1.1333e-01, -5.4712e-02, -2.4218e-02, -3.3824e-01,\n",
      "          8.1181e-02, -1.9449e-01,  3.7630e-01, -2.1264e-02, -5.2111e-02,\n",
      "          4.2380e-02, -5.3987e-01,  3.0595e-01, -8.1894e-02, -7.9444e-02,\n",
      "         -4.7489e-01,  2.8602e-01,  3.2571e-01, -2.8165e-01,  5.1244e-01,\n",
      "         -2.6194e-01,  2.1904e-02, -4.4476e-02,  2.6877e-01, -1.4431e-01,\n",
      "         -7.2422e-01, -1.1626e-01, -5.4797e-02, -3.0636e-01,  5.5141e-01,\n",
      "          5.8890e-01,  1.9670e-01,  8.9432e-02,  2.6078e-01, -4.2781e-01,\n",
      "         -4.0849e-01, -2.2946e-01,  1.6833e-01,  3.9348e-01,  2.8871e-01,\n",
      "          1.1099e-01,  1.1474e-02, -8.8052e-02,  7.8891e-03,  5.0952e-01,\n",
      "         -3.5622e-01, -3.1670e-01, -3.9730e-01, -1.7613e-01, -4.2220e-01,\n",
      "          3.3345e-02,  9.6751e-01, -2.1773e-01, -5.4908e-02,  6.1264e-02,\n",
      "          7.1699e-03, -5.3382e-01, -1.1976e-01, -1.6274e-01,  3.3619e-01,\n",
      "          1.7300e-01,  6.6433e-02,  3.6207e-01,  6.0496e-01, -5.0467e-02,\n",
      "         -1.3162e-01,  2.1801e-02, -4.8190e-01,  2.6627e-01,  5.7762e-01,\n",
      "         -7.9572e-02, -7.2328e-02, -5.0142e-01,  1.4012e-01,  2.8643e-02,\n",
      "         -9.1131e-02,  1.0420e-01, -3.4963e-02,  6.3149e-01, -1.2680e-01,\n",
      "         -2.8995e-01,  4.1199e-01, -5.6684e-01,  4.9629e-01, -1.1473e-01,\n",
      "         -6.2229e-01, -1.8228e-01, -8.3027e-02, -3.1894e-01,  2.1555e-01,\n",
      "         -5.3219e-01, -8.5684e-02,  9.0667e-01,  7.3848e-02,  2.5752e-01,\n",
      "         -4.1342e-02, -2.7983e-02, -3.9398e-01,  3.9310e-01,  3.0592e-01,\n",
      "         -2.8109e-01, -3.0897e-01,  1.6569e-01, -3.9276e-01, -6.0238e-01,\n",
      "         -5.6254e-01,  2.1732e-01,  4.0376e-01, -1.8088e-03, -3.5925e-01,\n",
      "          5.0841e-01,  1.8367e-01, -1.1368e-01, -8.6301e-01, -5.3330e-01,\n",
      "          6.2881e-02, -2.3127e-02,  3.0569e-01, -4.3782e-01, -3.7116e-01,\n",
      "          1.5526e-01,  5.0917e-01, -4.7231e-01,  2.2956e-01,  3.1762e-01,\n",
      "         -5.0973e-01, -4.2968e-01,  8.7861e-02, -6.3300e-02,  1.7702e-02,\n",
      "         -2.8498e-01,  1.6924e-01, -1.9825e-01, -2.2153e-01,  6.3423e-02,\n",
      "          3.2258e-01, -6.7483e-01, -1.5340e-02,  8.7432e-02,  2.8742e-02,\n",
      "          1.0169e-01,  4.6025e-01, -5.1534e-01, -1.8488e-01,  4.7430e-01,\n",
      "         -3.0224e-01, -8.8560e-01,  6.4800e-01, -6.0620e-01, -1.8822e-01,\n",
      "          9.5495e-02, -3.5035e-01, -8.4032e-02, -1.0261e-01, -1.2031e-01,\n",
      "          1.2893e-01,  4.9970e-02,  2.3974e-01, -9.5933e-03,  5.5962e-02,\n",
      "         -2.4656e-01, -2.3311e-01, -6.0388e-01,  5.5915e-01,  5.9073e-01,\n",
      "          5.5587e-02,  5.2287e-01,  1.2833e-01, -3.2446e-01,  3.4146e-01,\n",
      "          2.3702e-03,  2.7660e-01, -3.3915e-01,  5.0672e-02, -4.8512e-01,\n",
      "          2.1232e-02, -5.1525e-01,  2.9305e-01, -6.0101e-01, -2.4107e-01,\n",
      "          1.1265e-01, -3.8324e-01,  4.0355e-01,  4.8286e-02,  1.3126e-01,\n",
      "         -7.1440e-02,  2.3723e-01, -3.3955e-01,  3.5132e-01,  4.4466e-02,\n",
      "         -5.1810e-01,  1.7693e-01, -4.5009e-01, -1.7032e-01,  3.6519e-01,\n",
      "          4.0015e-01,  5.7751e-01, -3.9867e-01, -3.5129e-01, -2.9218e-01,\n",
      "          3.9971e-01,  1.0679e-01, -2.8485e-02, -7.8020e-01, -4.1346e-02,\n",
      "         -3.0339e-01,  1.2180e-01, -4.8836e-02, -3.6267e-01, -1.7892e-01,\n",
      "          3.7995e-01, -5.9443e-02, -1.2717e-01,  5.2329e-01, -4.4440e-03,\n",
      "          5.5220e-01,  2.5216e-01,  1.7240e-01, -2.7118e-01, -6.0869e-01,\n",
      "         -6.4866e-01,  5.1004e-01,  1.4937e-01, -3.6031e-01, -2.4883e-01,\n",
      "         -1.1684e-02,  1.1412e-01,  5.1500e-02,  7.9993e-02,  8.9745e-02,\n",
      "         -1.4657e-01,  1.6701e-01, -9.8740e-02,  1.1791e-01,  3.5204e-01,\n",
      "          7.9657e-02, -4.0722e-02, -1.5178e-01,  2.1756e-01, -2.9770e-01,\n",
      "         -5.3812e-01,  2.3208e-01,  7.5083e-01,  4.8473e-01, -2.5062e-01,\n",
      "          2.0984e-01, -2.1297e-01, -5.3755e-02,  2.1523e-01, -4.7144e-01,\n",
      "          4.4234e-02,  1.1491e-01,  1.0338e+00,  6.1301e-01,  4.2525e-02,\n",
      "         -2.5182e-01,  1.6784e-01, -7.3591e-01, -1.3816e-01,  1.2296e-01,\n",
      "         -1.1872e-01,  5.9972e-01,  2.0502e-03,  3.7108e-01,  2.1850e-01,\n",
      "         -2.7059e-01,  7.7818e-02, -3.3210e-02,  3.1434e-01,  1.5615e-01,\n",
      "         -1.1165e-01, -2.2783e-01,  1.6899e-01, -4.2549e-01,  3.5665e-01,\n",
      "          8.5545e-02, -2.3042e-01, -9.5225e-02, -2.2870e-01, -3.6244e-01,\n",
      "         -2.8012e-02,  1.1731e-01,  1.2339e-01, -5.9871e-01, -4.1515e-01,\n",
      "         -2.0861e-01,  2.0967e-01, -1.9771e-01, -7.7777e-02,  2.0056e-01,\n",
      "         -3.5596e-01,  3.3516e-01, -2.0227e-01, -8.1340e-02, -2.6702e-01,\n",
      "         -4.9230e-02, -5.3975e-01, -4.1987e-02, -1.3949e-02, -1.8062e-01,\n",
      "          2.8054e-01, -3.8296e-01, -7.0341e-01, -2.1767e-01,  4.0061e-01,\n",
      "          7.5975e-01, -5.1438e-01,  6.0195e-01, -3.8422e-01,  1.4980e-01,\n",
      "         -5.7259e-01, -2.6106e-01,  2.1427e-01, -1.8548e-01, -9.2511e-02,\n",
      "          5.7565e-01, -3.4179e-01, -3.8637e-02, -2.3761e-01, -5.4843e-02,\n",
      "          4.5873e-01, -1.0555e-01,  1.7755e-01,  8.2295e-02,  3.0428e-02,\n",
      "          3.4907e-01, -5.4867e-02,  3.4111e-01, -3.3950e-01, -3.5694e-01,\n",
      "          7.6797e+00,  3.0709e-01, -1.2305e-01,  1.6727e-01,  2.2764e-01,\n",
      "         -4.5587e-01,  7.4543e-02,  1.8188e-01,  7.4825e-02, -3.0399e-01,\n",
      "         -4.2330e-01,  2.9934e-01,  2.7675e-01,  2.0923e-01,  8.3122e-02,\n",
      "          1.1125e-01, -1.6084e-01,  1.8254e-01, -2.2333e-01, -6.6200e-02,\n",
      "          3.0652e-01, -5.7952e-01, -4.0248e-01, -2.0274e-01,  7.7962e-01,\n",
      "         -1.4302e-01, -4.2492e-01, -4.8691e-01, -6.2415e-02, -5.4010e-01,\n",
      "          2.1929e-03, -2.7935e-01,  1.7317e-01,  1.0865e-01, -9.1416e-02,\n",
      "          3.1046e-01,  2.5283e-02,  2.4190e-01, -1.9032e-01,  8.2367e-02,\n",
      "          7.7902e-01, -4.0560e-01, -1.1311e-01,  3.0356e-01,  3.0974e-01,\n",
      "          2.5159e-01, -1.5315e-01,  8.0775e-01,  1.0126e-01, -7.6874e-01,\n",
      "         -4.1818e-01,  2.3497e-01,  4.8801e-01, -5.5475e-01,  1.2836e+00,\n",
      "          8.7511e-02, -2.1334e-02, -6.6262e-01,  6.1212e-02, -4.1632e-01,\n",
      "          1.1412e-02,  2.7525e+00,  2.3402e-02,  4.6930e-01, -7.4741e-02,\n",
      "          4.3754e-01,  1.0082e-01, -4.0982e-02,  1.4724e-01, -2.1899e-01,\n",
      "          5.0981e-01,  2.8588e-01, -6.1505e-02]], grad_fn=<MeanBackward1>)\n",
      "tensor([[ 5.5063e-01, -5.0652e-02, -1.0942e-01,  2.5369e-01,  2.5313e-02,\n",
      "          2.2405e-01,  9.7295e-02,  4.8974e-01, -1.4251e-01,  1.7704e-01,\n",
      "         -1.8438e-01, -7.4302e-01, -2.0765e-01,  3.1069e-01,  1.1718e-01,\n",
      "          1.1895e-01,  4.9245e-01,  7.2460e-02,  8.9646e-02,  3.9496e-02,\n",
      "         -3.0829e-01,  4.9370e-01, -8.8182e-02, -2.4032e-01, -3.6414e-02,\n",
      "         -5.4316e-01, -4.5367e-03, -4.6508e-01,  8.4365e-02, -5.1092e-01,\n",
      "         -7.9528e-02, -5.5058e-01, -5.1225e-01, -5.4617e-01, -5.9231e-02,\n",
      "         -3.2621e-01, -3.0245e-01, -3.2771e-01, -4.1591e-01,  2.9573e-02,\n",
      "          3.9482e-01,  1.0706e-01,  6.3700e-03, -1.3643e-01, -1.3200e-01,\n",
      "         -6.9604e-02, -3.1063e-01,  3.7450e-01, -2.1108e-02,  4.7964e-01,\n",
      "         -2.4216e-01,  9.8994e+00, -2.7635e-01, -3.0188e-01, -5.8677e-01,\n",
      "          3.4234e-01,  7.3533e-02, -1.6456e-01,  6.1941e-02, -3.0779e-01,\n",
      "         -2.8596e-01,  1.3331e-01, -2.7817e-01, -1.0125e-01, -7.2472e-02,\n",
      "          3.8441e-01,  2.2259e-01, -2.2131e-01, -1.7391e-01, -2.8968e-01,\n",
      "         -1.6130e-01,  4.8123e-02, -2.1081e-01, -2.0784e-01,  1.3010e-01,\n",
      "          3.6557e-01,  3.0128e-02, -2.5633e-01,  1.9031e-01, -5.0737e-01,\n",
      "         -6.0723e-01, -1.8306e-01, -2.5064e-01,  1.4504e-01, -1.3846e-01,\n",
      "         -2.4659e-01, -3.5984e-01, -1.9200e-01,  4.3989e-01, -5.7064e-01,\n",
      "         -5.4474e-02,  1.4371e-01,  3.3103e-01, -3.5467e-01,  2.1438e-01,\n",
      "         -1.7887e-01, -2.1708e-01, -5.2924e-01, -2.7212e-02, -1.3847e-01,\n",
      "         -5.6091e-01, -9.7949e-02,  5.6840e-02, -5.9180e-01, -3.1189e-01,\n",
      "          1.4733e-01, -1.0383e-01,  1.0396e-01, -5.1110e-01,  2.7363e-02,\n",
      "         -1.3499e-01, -5.5443e-01, -3.1046e-01,  1.7263e-02, -1.3308e-01,\n",
      "         -3.9045e-02, -2.2530e-01,  3.8756e-01,  5.9262e-02, -2.2580e-01,\n",
      "          2.5242e-01,  2.4806e-01, -2.3369e-01, -3.1506e-03,  5.5320e-02,\n",
      "         -4.0585e-01,  1.9136e-02, -3.9645e-01, -2.4914e-01,  2.4425e-01,\n",
      "          2.9717e-02, -1.9015e-01,  2.5318e-01,  1.8394e-01, -4.7971e-02,\n",
      "         -5.1713e-01,  1.2362e-03, -9.0059e-02, -3.0284e-01, -9.8248e-02,\n",
      "          7.3395e-02,  6.6858e-02, -1.7889e-01,  1.6821e-01, -4.7747e-02,\n",
      "          7.6117e-01,  1.2568e-01,  3.4475e-01,  2.1632e-01,  4.8829e-01,\n",
      "         -1.5458e-01, -6.9597e-01, -2.5459e-01, -5.1430e-01, -4.7511e-01,\n",
      "         -2.2954e-01,  6.7034e-01, -1.0426e-01,  4.4091e-01, -3.1289e-01,\n",
      "         -4.3420e-02,  1.5214e-01,  3.8334e-01,  3.7670e-01, -8.6696e-04,\n",
      "         -4.0743e-01, -1.9101e-01,  4.6708e-02, -2.7664e-02,  6.4820e-02,\n",
      "          1.8016e-01,  6.8199e-01,  1.2250e-01,  4.2781e-01,  1.4352e-01,\n",
      "         -9.9272e-03, -6.3115e-01, -6.4921e-02, -7.1536e-02, -5.7909e-01,\n",
      "         -4.0790e-02, -1.9972e-01,  5.2045e-02, -3.3699e-01,  7.0581e-01,\n",
      "          3.4623e-01,  4.2617e-01,  2.4928e-01,  1.8609e-01, -3.6664e-02,\n",
      "         -2.5257e-01,  1.0755e-01, -2.6499e-01, -1.8609e-01,  7.9871e-03,\n",
      "         -2.3770e-01, -1.6492e-01,  3.5866e-02,  2.5138e-01, -1.0985e-04,\n",
      "         -2.1166e-01, -2.9911e-01, -1.4202e-01,  2.1448e-01,  2.4447e-01,\n",
      "         -8.7891e-02, -6.6794e-02, -1.9702e-01, -2.4714e-01, -3.3560e-01,\n",
      "         -3.0288e-01, -9.0198e-02, -2.9408e-01, -1.4665e-01, -9.4891e-02,\n",
      "         -2.0372e-01,  7.2375e-03, -2.4479e-02,  7.0960e-02,  3.6178e-01,\n",
      "         -6.7293e-02,  1.3552e-01,  1.9662e-01, -4.3465e-01, -1.4992e-01,\n",
      "         -9.9980e-03, -1.1037e-01,  4.2421e-02,  5.1881e-01, -2.2333e-01,\n",
      "          2.1296e-01,  1.1450e-01,  3.0985e-01,  3.2685e-02, -2.6828e-01,\n",
      "          6.7285e-02,  3.1433e-01,  1.1783e-02, -5.8354e-01,  1.6735e-01,\n",
      "          1.2832e-01,  2.5896e-01, -7.2091e-02,  1.2757e-02,  6.4522e-02,\n",
      "          3.0737e-01, -2.1868e-01, -5.6263e-02, -4.4550e-01,  6.5198e-02,\n",
      "          2.7454e-01, -6.9383e-01,  3.1238e-01, -5.9283e-02,  9.7222e-02,\n",
      "          1.7414e-01, -8.5126e-02,  9.8148e-02,  2.3212e-01,  1.8503e-01,\n",
      "         -2.6751e-01,  6.5378e-01,  9.6902e-01, -4.2896e-01,  1.4786e+00,\n",
      "         -1.9499e-01,  1.1975e-02, -3.8265e-01,  3.4491e-01, -8.4455e-02,\n",
      "          3.2204e-02,  1.3405e-01,  4.1137e-01, -2.6796e-01,  9.6225e-03,\n",
      "          4.8449e-01,  4.0846e-01, -6.9776e-02,  5.1027e-02, -2.8626e-01,\n",
      "          3.3560e-01,  1.3735e-01, -4.0540e-01, -4.1990e-01, -5.6870e-01,\n",
      "         -5.6057e-01,  1.1293e-01, -2.5647e-01, -2.4471e-01, -5.6532e-01,\n",
      "          9.5050e-02, -1.1134e+00,  4.7165e-02,  3.1361e-01, -1.9172e-02,\n",
      "          3.2808e-01,  3.0650e-03, -3.6408e-01, -2.1341e-01, -5.3129e-01,\n",
      "          5.0565e-01, -4.1125e-01,  1.9825e-01, -1.9901e-01, -2.9457e-02,\n",
      "         -2.3615e-02, -5.5298e-02, -2.7967e-01,  3.0321e-01, -2.1692e-01,\n",
      "         -1.0217e-01, -2.2431e-01, -4.7003e-01,  6.0248e-02, -1.4408e+00,\n",
      "          2.8743e-02,  1.4312e-01,  5.3617e-01,  1.5901e-01, -4.0716e-01,\n",
      "         -1.7791e-01,  3.6102e-01,  7.0152e-01,  5.5688e-01, -6.0763e-01,\n",
      "          1.5949e-02,  3.1250e-01,  4.5591e-02,  9.9259e-02, -2.2834e-01,\n",
      "          1.2162e-01,  1.2726e-01,  9.4560e-03, -7.3945e-01, -8.0161e-02,\n",
      "          2.8060e-01,  2.4285e-01, -2.6596e-01, -2.0050e-01, -4.3315e-01,\n",
      "          8.9948e-02,  1.1606e-01,  7.1760e-01,  1.6927e-01, -2.8251e-01,\n",
      "          1.7469e-01,  1.4037e-01, -4.1985e-01, -1.9607e-01,  3.1195e-01,\n",
      "         -1.8884e-02,  3.0157e-02, -2.4866e-01,  1.2352e-01,  2.2872e-01,\n",
      "          1.4972e-01, -4.6825e-01, -1.3735e-01, -1.2941e-01,  9.6248e-02,\n",
      "         -1.8107e-01,  2.1804e-01,  1.5394e-01, -3.1858e-01,  1.3138e-01,\n",
      "          4.0779e-01,  5.5546e-03,  1.2085e-01,  1.1858e-01, -1.6680e-01,\n",
      "          1.8883e-01, -3.7446e-01, -2.2723e-02,  6.7306e-02, -2.1500e-01,\n",
      "          1.2963e-01, -3.6314e-01,  1.8208e-01, -3.4901e-01, -1.4377e-01,\n",
      "         -5.0385e-01,  5.1588e-02,  1.9777e-01,  9.7469e-02,  3.5449e-01,\n",
      "         -3.3777e-02, -2.4418e-01, -2.0413e-01,  1.9433e-01, -2.2916e-01,\n",
      "         -3.5776e-01, -1.7981e-01,  3.2507e-01, -2.5103e-01,  2.7900e-01,\n",
      "          7.2872e-01,  4.2556e-02, -2.7985e-01, -9.7357e-02,  3.6077e-01,\n",
      "         -3.1938e-02, -3.4652e-01,  3.3736e-01,  2.8031e-01,  1.3662e-01,\n",
      "         -2.5505e-01, -7.2263e-02,  3.9670e-01,  7.2606e-02,  8.2543e-02,\n",
      "         -2.3244e-01,  1.5221e-01, -2.9255e-01,  5.3622e-02, -2.1205e-01,\n",
      "          6.7606e-02,  6.7513e-01, -2.0429e-01, -4.9586e-02, -2.8254e-01,\n",
      "         -2.4013e-01,  6.4269e-03,  1.8374e-01, -2.0404e-01,  2.7066e-01,\n",
      "         -2.0077e-01,  9.7518e-02,  2.1491e-01,  3.3057e-01, -4.7765e-01,\n",
      "         -2.3624e-01, -3.0763e-01, -2.6409e-02, -4.9973e-02,  1.8070e-01,\n",
      "          2.8445e-01, -1.7426e-01, -2.1122e-01,  3.5615e-01,  2.1911e-01,\n",
      "          6.3145e-02, -1.8400e-02,  1.1617e-02,  2.9638e-01, -2.3698e-01,\n",
      "         -4.4229e-01,  3.0217e-01,  3.3307e-02,  1.9783e-01, -2.5311e-01,\n",
      "         -9.7082e-02, -1.9924e-01, -1.3034e-01, -2.6191e-01,  1.7348e-01,\n",
      "         -6.9418e-01, -1.2837e-01,  8.4120e-01,  1.8148e-01,  2.5013e-01,\n",
      "         -2.2335e-02,  2.4312e-01,  9.9051e-02,  7.1658e-02, -9.9424e-02,\n",
      "          1.4882e-01, -1.8934e-01,  2.6403e-02, -1.3880e-01, -2.4939e-01,\n",
      "         -7.5073e-01,  3.0094e-01, -1.7858e-01,  1.8784e-01, -2.4557e-01,\n",
      "          2.7637e-01,  7.7270e-02,  3.5969e-02, -1.2929e+00, -3.5080e-01,\n",
      "         -8.7150e-02,  4.3824e-02,  3.5729e-01, -1.4179e-01, -8.4556e-03,\n",
      "          9.0216e-02,  2.7957e-01, -8.1817e-01,  1.7556e-01, -4.6026e-02,\n",
      "         -4.0549e-01, -3.5393e-01,  2.6699e-02, -1.8529e-01, -1.1809e-01,\n",
      "         -8.1365e-01,  3.8168e-01, -2.5827e-01, -1.9154e-02, -2.0845e-03,\n",
      "         -6.4374e-02, -4.8926e-01,  3.6723e-01, -6.3565e-02, -4.8969e-01,\n",
      "          2.9597e-01,  7.2034e-01,  5.7033e-02, -1.5277e-01, -1.7241e-01,\n",
      "         -3.7377e-01, -1.4730e+00,  4.6293e-01, -1.0447e-01,  1.8174e-01,\n",
      "          3.9379e-01, -1.4587e-01,  3.9697e-02,  1.1542e-01, -1.0646e-01,\n",
      "          3.7473e-01, -4.2873e-03,  5.3292e-01,  2.8524e-01,  1.6452e-01,\n",
      "         -2.1217e-01, -1.4388e-01, -2.7999e-01,  4.8439e-01,  4.9683e-01,\n",
      "          1.8718e-02,  5.5284e-01,  1.8352e-01, -1.5450e-01, -3.0835e-02,\n",
      "          5.9308e-01,  3.7168e-01, -7.2045e-01,  5.0686e-02,  1.6429e-01,\n",
      "          2.2830e-01, -6.6336e-01,  3.6247e-01, -6.5015e-01, -1.2376e-02,\n",
      "         -4.6181e-01, -4.9329e-01,  1.3148e-02, -1.4816e-01, -1.3329e-02,\n",
      "         -2.9476e-01,  1.8765e-01, -1.8616e-02,  1.7053e-03,  5.6284e-02,\n",
      "         -5.0665e-01,  2.2598e-01, -3.9405e-01, -2.2847e-02,  2.3083e-01,\n",
      "          1.3586e-01,  2.3592e-01, -1.3965e-01, -7.1696e-02,  1.2083e-01,\n",
      "          4.5956e-01, -2.3365e-01,  1.3297e-01, -3.3776e-01,  4.9686e-01,\n",
      "         -2.3343e-04,  1.4205e-01, -2.1490e-01,  1.4180e-02, -3.3182e-01,\n",
      "          2.6748e-01, -5.7132e-02,  3.9376e-02,  2.7675e-01,  1.7420e-01,\n",
      "          7.0856e-01,  2.3611e-01,  6.3680e-02,  1.2193e-01, -4.5915e-02,\n",
      "         -4.9337e-01, -1.3169e-01, -9.0710e-03, -3.3040e-01,  1.2193e-01,\n",
      "         -2.4516e-01, -5.9111e-02, -5.9769e-02, -1.0823e-01, -2.4766e-02,\n",
      "          1.5450e-01,  1.3024e-01,  2.9333e-02,  1.2263e-01,  7.9651e-02,\n",
      "          3.6949e-01, -2.0435e-01, -3.8155e-01,  2.5855e-01, -4.7911e-02,\n",
      "         -2.6866e-01, -1.5272e-01,  4.7637e-01, -2.0073e-01,  1.5601e-01,\n",
      "          1.0693e-01, -4.8329e-02, -1.8992e-01,  3.6821e-01, -2.9205e-01,\n",
      "         -2.3701e-01, -2.0445e-01,  1.0657e+00,  2.1197e-01, -3.7812e-01,\n",
      "         -3.3841e-02,  2.7672e-01, -1.1468e-01, -3.7172e-01,  2.1316e-01,\n",
      "          5.1548e-02,  7.3218e-01,  7.0812e-02,  3.9792e-01,  3.5674e-01,\n",
      "          5.8130e-03,  1.9178e-02, -1.6597e-01,  2.5067e-01, -8.4910e-02,\n",
      "         -1.7462e-01, -1.9001e-01,  4.5862e-01, -1.2428e-01, -9.9172e-02,\n",
      "         -2.7050e-01, -1.9365e-01,  2.3877e-01, -2.4886e-01, -4.6948e-01,\n",
      "         -9.9272e-02, -2.3655e-02,  3.2519e-01, -4.8946e-01, -2.3741e-01,\n",
      "         -1.0664e-01, -8.5234e-03, -2.0645e-01, -1.6090e-01, -3.6073e-02,\n",
      "         -2.6900e-01,  5.1386e-01, -6.5974e-02, -7.0602e-02, -6.5457e-02,\n",
      "          2.8834e-01, -4.5976e-01,  1.5625e-01, -8.0979e-02, -1.5614e-01,\n",
      "          2.1150e-01, -2.8113e-01, -6.6135e-01, -1.8959e-01,  5.0625e-01,\n",
      "          7.7981e-01, -2.6754e-01,  1.5704e-01, -2.9521e-01,  2.6422e-01,\n",
      "         -8.4072e-01, -2.7286e-01, -2.4881e-02, -9.2446e-02,  1.3586e-02,\n",
      "          3.7773e-01, -5.4367e-01, -2.2259e-01, -4.3320e-01,  4.0428e-02,\n",
      "          6.0966e-01, -4.2338e-02,  9.1767e-02, -9.8637e-02, -9.2549e-02,\n",
      "          4.8570e-01, -5.9905e-02,  3.2777e-01, -2.9302e-01,  1.0773e-01,\n",
      "          7.3143e+00,  1.9201e-01, -1.5799e-01, -3.3729e-01, -3.3916e-01,\n",
      "         -3.6559e-01, -1.1678e-01,  1.1434e-01, -8.7240e-03, -8.5406e-02,\n",
      "         -1.3716e-01,  3.0520e-01, -1.6171e-01, -4.4573e-02, -2.8929e-01,\n",
      "         -7.0776e-02, -1.8283e-01,  4.6829e-02, -2.6232e-01, -3.0052e-01,\n",
      "          1.1999e-01, -5.9710e-01, -2.8465e-01, -9.3357e-01,  9.7585e-01,\n",
      "         -4.3425e-01, -3.3210e-01,  3.8310e-02,  2.1127e-01, -2.0559e-01,\n",
      "         -1.5016e-01,  7.1635e-02, -1.5190e-01, -1.1621e-01, -1.9579e-01,\n",
      "          3.3951e-01,  1.5875e-01,  7.0373e-02,  1.0900e-01, -1.4516e-01,\n",
      "          4.8552e-01, -1.7933e-01, -1.7879e-01,  6.4570e-01,  8.2787e-02,\n",
      "          8.9110e-01, -2.4418e-01,  5.9427e-01,  2.8143e-02,  5.4555e-02,\n",
      "         -5.8350e-01, -2.6169e-01, -6.1442e-02, -3.6951e-01,  1.2149e+00,\n",
      "         -1.3188e-01,  2.7338e-01, -4.6594e-01, -6.0431e-01, -1.2236e-01,\n",
      "          3.4216e-02,  2.5483e+00, -3.1588e-01,  5.6549e-01, -2.8916e-01,\n",
      "         -7.0141e-02, -3.2455e-01,  1.6358e-01, -1.8577e-01, -1.7113e-01,\n",
      "          3.9528e-01,  1.7308e-01, -1.6505e-01]], grad_fn=<MeanBackward1>)\n",
      "tensor([[ 1.5927e-01,  2.9425e-01,  1.0872e-01,  9.0241e-02,  6.2248e-01,\n",
      "         -2.0321e-01, -6.7020e-01,  1.3077e-01, -7.1581e-01,  5.2457e-01,\n",
      "         -4.3974e-01, -7.3792e-01,  3.6265e-01,  3.1931e-01, -1.6905e-02,\n",
      "         -6.6281e-01,  4.0966e-01,  5.5123e-01,  1.5566e-01,  4.1528e-01,\n",
      "          3.3994e-01,  5.2533e-01,  3.5830e-01, -2.8214e-01, -4.6200e-02,\n",
      "         -5.3144e-02, -3.3025e-01, -4.1357e-02, -3.3109e-01,  8.6114e-03,\n",
      "         -2.7546e-03, -2.2970e-01, -1.1978e-01,  4.2075e-01,  1.4604e-01,\n",
      "          1.6479e-01,  4.4808e-01, -3.8339e-01, -6.7812e-01, -6.4243e-01,\n",
      "          2.7375e-01, -7.9364e-01, -7.6227e-01,  1.2054e-01, -1.2804e-01,\n",
      "          1.4942e-01,  3.9802e-01, -5.4019e-03,  1.5217e-01,  1.6275e-01,\n",
      "          1.7928e-01,  9.2596e+00,  9.2902e-01,  1.0863e-01, -8.0728e-01,\n",
      "          1.3383e-01,  8.5326e-01,  2.9566e-01, -9.0191e-02, -3.7359e-01,\n",
      "         -4.9319e-02, -1.4269e-01,  2.1424e-01, -2.5665e-01, -3.6733e-01,\n",
      "         -1.3234e-01, -4.1092e-01,  1.3753e-01, -9.7925e-01, -1.2414e-01,\n",
      "          4.2625e-01, -5.1423e-01, -1.9945e-01,  6.9944e-02,  5.3475e-01,\n",
      "          3.2963e-01,  1.6903e-01, -2.7516e-01, -5.5616e-01, -7.5679e-01,\n",
      "         -5.0967e-01, -2.5242e-02, -7.2165e-01, -6.1103e-01, -7.3213e-01,\n",
      "         -1.0886e-01,  3.0441e-02, -7.5408e-01,  3.9165e-01, -4.6768e-01,\n",
      "         -2.0985e-01,  1.4394e-01,  1.5269e-01,  5.6499e-02,  2.2522e-01,\n",
      "          6.0718e-02, -5.3208e-02, -2.3501e-01,  1.7140e-01,  1.9307e-02,\n",
      "         -7.2922e-01,  8.3458e-01,  1.3853e-01,  3.1822e-01, -6.0812e-01,\n",
      "          2.7917e-01, -2.6471e-01,  3.6776e-01,  3.0336e-03, -4.3473e-01,\n",
      "         -3.6546e-01, -4.6680e-01, -5.0109e-01,  2.5553e-02, -3.4842e-01,\n",
      "         -2.6412e-02, -7.6837e-01,  8.9975e-01, -3.4533e-01,  2.6641e-02,\n",
      "          2.8916e-01,  7.3160e-01,  2.3397e-01,  4.7073e-01,  2.1864e-01,\n",
      "         -9.9105e-01, -4.0404e-02,  1.5772e-01,  4.2076e-01, -2.6101e-01,\n",
      "         -1.4895e-01, -2.7832e-01,  5.5151e-01,  8.8429e-03,  1.2276e+00,\n",
      "         -1.0383e+00,  5.2135e-01, -3.1318e-01, -1.1129e+00, -2.6258e-01,\n",
      "         -3.4342e-01, -5.1454e-01,  8.6634e-02, -8.7721e-02,  5.8063e-01,\n",
      "          7.5905e-01, -5.8563e-02,  3.1598e-01,  1.1744e+00,  5.6745e-01,\n",
      "         -1.6793e-01, -6.0322e-01, -7.2233e-01, -7.0290e-02, -7.0269e-02,\n",
      "          8.4832e-02,  1.3552e-01,  1.3330e-02,  4.6213e-01, -1.9091e-01,\n",
      "         -1.9257e-01,  2.5274e-01,  3.4225e-01,  8.1143e-01,  7.5734e-02,\n",
      "         -8.8944e-01,  3.0936e-01, -9.7661e-02, -4.7127e-01, -5.0662e-01,\n",
      "         -1.4347e-01,  6.0627e-01,  3.8769e-01, -3.7985e-01, -8.4405e-01,\n",
      "          4.3511e-01, -7.4108e-01,  6.7100e-02,  4.6981e-02,  2.3369e-01,\n",
      "         -2.0381e-01, -3.7794e-02, -7.7472e-02, -2.5826e-01,  3.6656e-02,\n",
      "          1.0456e-01,  3.3079e-01, -5.3185e-01,  2.6217e-01, -3.0923e-01,\n",
      "          5.5184e-02,  3.9756e-01, -1.0403e+00,  6.1768e-02,  2.6110e-01,\n",
      "          1.8618e-01, -1.6419e-01, -1.4493e-01,  2.4202e-01,  9.0086e-01,\n",
      "         -5.3780e-01, -2.1701e-02, -6.3295e-02,  1.6923e-01,  6.5059e-02,\n",
      "         -3.4415e-01, -3.6367e-01, -5.3173e-01, -6.0451e-01,  4.5434e-01,\n",
      "         -4.3114e-01, -2.4113e-01,  4.3329e-01, -5.4590e-01,  5.8075e-01,\n",
      "         -5.7410e-01,  1.2608e-01,  7.6870e-02, -9.3537e-02, -4.8309e-01,\n",
      "         -2.9286e-02,  2.8045e-01,  2.6058e-01, -3.1484e-01, -1.1940e-02,\n",
      "         -1.7808e-01, -4.0950e-01,  8.5339e-01,  4.7427e-02,  1.7517e-01,\n",
      "         -3.2674e-01,  2.5241e-01, -5.3287e-01,  1.8607e-04,  5.4800e-01,\n",
      "         -1.5439e-01,  3.3511e-01, -1.0582e-01, -9.3540e-01,  5.5014e-01,\n",
      "          1.6344e-01, -1.8158e-01, -6.0840e-01, -9.4299e-02, -6.8373e-01,\n",
      "          4.5810e-01, -2.4942e-01,  6.2302e-01, -8.8126e-03, -3.1285e-01,\n",
      "          2.7173e-01, -3.9444e-01, -2.5144e-01, -7.1060e-01, -3.6633e-01,\n",
      "          7.4089e-01,  2.2663e-01, -2.3164e-01, -4.0780e-01, -5.8208e-02,\n",
      "         -3.9209e-01,  3.3050e-02,  8.2428e-01,  5.1185e-01,  2.1585e+00,\n",
      "         -5.7625e-01,  1.1125e+00, -7.3584e-01, -3.2785e-02, -1.8808e-02,\n",
      "         -3.4090e-01,  4.2218e-01, -1.8571e-01, -5.9059e-01, -1.7569e-01,\n",
      "         -2.1790e-02,  2.0014e-01,  2.8241e-01,  4.6125e-01, -3.6599e-01,\n",
      "          3.5042e-01, -2.7826e-01,  2.5914e-01, -2.4215e-02, -8.9564e-01,\n",
      "          1.5443e-01,  2.6570e-01, -1.3915e-01,  3.8602e-01, -3.3483e-02,\n",
      "          5.0023e-03, -8.7422e-01, -5.6883e-01,  7.7309e-01,  1.5978e+00,\n",
      "          5.6102e-01,  2.2832e-01, -6.3272e-01,  2.9700e-02, -1.7328e-01,\n",
      "         -7.1600e-01, -4.9107e-01, -8.6990e-02,  4.2954e-01,  2.6004e-01,\n",
      "         -7.3574e-03,  4.2238e-01,  2.8072e-01,  3.1462e-01, -7.9869e-01,\n",
      "         -1.9916e-01, -3.5838e-02, -3.5655e-01, -2.3553e-01, -9.8659e-01,\n",
      "         -5.7816e-02,  6.8918e-01, -3.2009e-01,  3.5526e-01, -1.5481e-01,\n",
      "         -1.3867e-01,  9.9161e-01,  5.0922e-01,  4.4127e-01,  3.4923e-01,\n",
      "          5.8957e-01,  8.0843e-01,  4.0821e-01, -3.8742e-01, -2.9195e-01,\n",
      "          1.3353e-01,  1.6413e-01, -3.7247e-01,  1.2143e-01, -5.0046e-01,\n",
      "         -4.8224e-02, -3.2149e-01, -2.4144e-01, -2.1360e-01, -5.9599e-01,\n",
      "          3.6355e-01, -2.6277e-01,  5.8887e-01, -3.1458e-01, -1.8071e-02,\n",
      "          7.7000e-01,  1.8239e-01, -2.9300e-01,  6.3080e-02,  3.1779e-01,\n",
      "         -1.7827e-01,  6.3102e-01, -7.0679e-01, -1.6413e-01,  4.1259e-01,\n",
      "          5.5189e-01,  3.1958e-01, -1.3911e-01,  1.6020e-01,  8.3834e-01,\n",
      "         -6.0810e-02,  6.1423e-01,  2.4061e-01, -6.4489e-01,  2.2147e-01,\n",
      "          2.8620e-01, -4.8071e-01,  5.1663e-01,  1.8031e-01, -2.4911e-01,\n",
      "         -2.0697e-02,  7.5264e-02,  9.4097e-01,  2.0830e-01, -4.9028e-02,\n",
      "          2.8657e-01,  1.1484e-01,  4.2264e-01,  4.5610e-01,  1.9280e-01,\n",
      "         -7.0507e-01, -4.0578e-01,  1.1476e-01,  1.2416e-01,  2.9148e-01,\n",
      "         -2.7348e-01,  2.7119e-01, -3.6680e-01,  5.0857e-01, -3.7881e-01,\n",
      "          6.2863e-03,  5.2324e-01, -1.2688e-01,  6.6752e-02,  6.0057e-01,\n",
      "          1.6464e+00,  7.3494e-01, -7.6457e-02, -2.1737e-01, -1.4711e-01,\n",
      "         -3.3640e-01, -2.3621e-01, -1.0011e-02,  4.3864e-01,  3.1364e-01,\n",
      "         -6.8652e-01, -1.2102e-01,  3.3952e-01,  3.3830e-01, -1.0919e-01,\n",
      "         -3.2268e-01,  5.5717e-01,  2.6312e-01,  7.3608e-01, -6.5787e-01,\n",
      "          6.4325e-01,  1.9403e-01, -1.7396e-01,  2.3919e-01, -2.2498e-01,\n",
      "         -3.2431e-01,  2.8195e-01,  4.0996e-01, -2.7576e-01,  4.2853e-01,\n",
      "          1.6306e-01, -2.6716e-01, -1.8792e-01, -6.7187e-02, -2.1382e-01,\n",
      "         -1.8128e-01,  6.1473e-01,  8.1047e-02, -1.8276e-01,  1.9996e-01,\n",
      "         -4.8312e-02, -6.3445e-01,  1.3708e-02, -2.9234e-01, -5.2429e-02,\n",
      "          1.3101e-03, -6.4851e-01, -3.5699e-02,  3.0420e-01,  1.9397e-01,\n",
      "          1.2361e-01,  5.1794e-01,  5.1794e-01,  5.4046e-02,  1.1001e-01,\n",
      "          8.3991e-01, -5.7548e-01, -3.3093e-01, -7.8830e-01,  1.9982e-02,\n",
      "         -3.6918e-01,  7.7036e-02,  5.8242e-01,  3.2838e-01,  2.0916e-01,\n",
      "         -7.6982e-03,  1.6356e-01,  2.8374e-01, -1.7406e-01,  3.5234e-01,\n",
      "          4.3263e-01,  1.5015e-01, -3.5133e-01, -1.0555e-01,  5.6339e-01,\n",
      "         -7.4888e-01,  1.3751e-01,  1.8868e-01,  6.5813e-01, -6.0068e-03,\n",
      "         -4.0326e-02,  1.4889e-01, -3.7301e-01, -1.7181e+00, -6.6710e-01,\n",
      "         -1.5996e-01, -5.1982e-01,  5.5577e-01,  1.1755e-01,  1.5954e-02,\n",
      "         -1.9313e-01, -1.3434e-01,  7.6464e-02, -3.0814e-01, -2.1044e-01,\n",
      "         -6.8317e-01, -3.6444e-01,  1.3164e-02, -9.6237e-01, -4.9526e-01,\n",
      "         -1.1825e+00,  1.7608e-01,  3.5661e-01,  7.5748e-01, -6.3445e-02,\n",
      "          4.2688e-01, -6.4584e-01, -1.9055e-01, -3.4513e-01, -8.1888e-01,\n",
      "         -3.3330e-01, -8.6899e-02, -3.1011e-01, -1.9695e-01, -1.8385e-01,\n",
      "          3.3668e-01, -9.5748e-01, -4.7764e-01, -2.8134e-01,  2.8576e-01,\n",
      "         -9.3677e-03, -2.0276e-01, -2.7594e-01, -6.6485e-01, -5.1841e-01,\n",
      "          1.7801e-01, -2.2904e-01, -5.9783e-01,  3.9031e-02, -3.0346e-01,\n",
      "          2.2130e-01, -4.0567e-01, -2.7645e-02,  4.1924e-01,  5.6283e-02,\n",
      "         -6.1633e-01,  2.7566e-01,  4.3790e-01, -9.9272e-02,  1.1505e-01,\n",
      "          2.2531e-01,  5.4375e-01, -1.4423e+00,  6.7097e-03,  4.4786e-02,\n",
      "          2.5107e-01, -8.6989e-01,  8.9242e-02, -5.9207e-02,  9.5470e-02,\n",
      "         -9.0513e-01,  8.3242e-02,  4.4612e-01, -7.7240e-01, -3.9592e-02,\n",
      "          1.6795e-01,  5.2364e-01, -1.0666e+00,  3.7620e-01, -4.0544e-01,\n",
      "          8.8258e-02, -5.2532e-02, -3.3994e-01,  4.6252e-02,  2.7446e-01,\n",
      "          8.1246e-03,  2.9743e-01, -7.0272e-02,  1.1082e-01, -2.2411e-01,\n",
      "          2.9618e-01, -7.3876e-01, -5.8287e-02, -7.0978e-02, -3.2865e-01,\n",
      "         -2.7065e-01,  3.9502e-01,  1.0973e-01, -9.4006e-03, -8.1894e-01,\n",
      "         -5.0338e-02, -5.7467e-02,  3.7312e-01,  5.5719e-01, -4.1319e-01,\n",
      "          1.1638e-01,  3.1519e-01, -8.5385e-02, -1.9772e-03, -3.5124e-01,\n",
      "         -7.3294e-01, -1.1701e-01,  4.4756e-01, -2.7334e-01,  2.6733e-01,\n",
      "          5.4698e-02,  3.1460e-01,  4.2349e-02, -6.8678e-02, -1.7646e-01,\n",
      "          3.0123e-01,  2.2058e-01, -7.1329e-02, -2.9261e-01,  2.7881e-01,\n",
      "          6.0376e-01, -6.8271e-03,  2.4414e-01,  1.0827e-01,  4.9343e-01,\n",
      "          2.9810e-01,  4.1794e-01,  3.5471e-01,  3.2802e-01,  3.6160e-01,\n",
      "         -6.7387e-01,  3.0954e-01,  3.1318e-01, -2.8645e-01, -5.3300e-01,\n",
      "         -2.0391e-01, -4.5002e-01,  1.7779e-01,  4.2528e-01,  3.9713e-01,\n",
      "          2.0560e-01, -6.2851e-01, -6.6900e-02,  1.8531e-01,  1.0269e-01,\n",
      "         -5.2439e-01,  1.2121e+00,  2.2250e-01, -1.0319e-01, -3.7995e-02,\n",
      "         -3.8841e-01, -4.6608e-01,  6.7928e-02, -1.7468e-01, -1.4702e-01,\n",
      "          3.3050e-02,  3.4929e-01, -4.3561e-02, -2.9483e-01, -1.2634e+00,\n",
      "          2.6725e-01,  2.4581e-01, -7.0948e-01,  1.0005e-01,  1.8963e-01,\n",
      "         -4.5142e-02,  5.5372e-01, -1.9887e-01, -4.8703e-01, -7.1943e-01,\n",
      "          4.2914e-01,  4.0257e-01, -2.6686e-01, -3.1936e-01,  1.7668e-01,\n",
      "         -3.1858e-01,  7.2901e-01, -4.2103e-01, -3.6171e-02, -4.2882e-02,\n",
      "         -1.0361e-01,  4.1674e-01,  1.0218e+00, -2.0068e-01, -1.9370e-01,\n",
      "         -3.8740e-01, -2.5900e-01, -2.0621e-01, -2.8401e-01,  2.3333e-01,\n",
      "          6.8367e-01, -5.1584e-01,  8.1982e-01, -7.9148e-01, -5.7031e-01,\n",
      "         -6.7657e-01, -1.8659e-01, -1.8739e-01,  7.2916e-02, -5.1539e-01,\n",
      "          2.2286e-01,  1.5731e-01, -4.7579e-01, -6.7417e-01,  1.8925e-01,\n",
      "          1.0375e+00, -1.5013e-01, -6.8843e-02, -2.6970e-01, -1.6022e-01,\n",
      "          6.6822e-02, -3.6178e-02, -5.5674e-01, -6.7184e-01, -1.2367e-01,\n",
      "          5.2622e+00,  3.3333e-03, -4.8923e-01,  3.2618e-01, -2.3333e-01,\n",
      "          6.9641e-01,  6.5850e-01,  1.8911e-01, -6.7175e-02,  9.4417e-02,\n",
      "         -5.0636e-01, -2.8568e-01, -4.7026e-01, -4.3476e-01,  3.3929e-01,\n",
      "         -4.7784e-01, -2.6816e-01,  3.1347e-01, -1.4180e-01,  1.8787e-01,\n",
      "          3.9517e-01, -8.8215e-01, -4.0855e-01, -1.6108e-01,  8.2958e-01,\n",
      "         -4.8854e-01, -4.8991e-01,  1.9307e-01,  2.5308e-01, -3.8534e-01,\n",
      "         -1.5333e-01, -2.4488e-01,  1.5446e-01, -2.2590e-01,  2.3183e-01,\n",
      "          4.8348e-01, -1.7960e-02, -9.2997e-02,  3.5490e-01, -2.0886e-01,\n",
      "          2.3756e-01,  5.8706e-01,  5.6405e-02, -1.5296e-01, -3.8153e-01,\n",
      "          2.1041e-01, -5.3608e-02, -1.3351e-01,  2.5591e-01,  2.8186e-02,\n",
      "         -2.6559e-01,  1.3667e-01, -2.2915e-01, -5.5086e-01,  1.1988e+00,\n",
      "         -1.8088e-01, -4.3322e-01,  5.2939e-01, -6.7323e-01,  3.8881e-01,\n",
      "         -6.4196e-01,  2.4995e+00,  7.8567e-02, -2.9378e-01, -3.4712e-01,\n",
      "         -6.9832e-02, -6.8260e-01, -3.5438e-01,  4.1226e-03, -4.7253e-01,\n",
      "          3.2728e-02,  1.0861e-01, -2.7695e-01]], grad_fn=<MeanBackward1>)\n",
      "tensor([[ 4.3008e-01,  7.7679e-02,  2.5674e-01,  5.6300e-01,  4.9044e-01,\n",
      "         -4.8749e-02, -5.1036e-02,  1.8914e-01, -4.9902e-01, -2.9995e-01,\n",
      "         -5.5541e-02, -8.9408e-01, -1.6891e-01,  1.2907e-01,  1.0221e-01,\n",
      "         -2.3130e-01,  6.3670e-01, -9.0605e-02,  4.7220e-01,  3.7754e-01,\n",
      "         -4.4230e-01,  4.0345e-01,  4.8201e-02, -6.5375e-02,  1.1767e-01,\n",
      "         -3.3595e-01, -1.9819e-01, -3.7413e-01,  3.1778e-01, -3.7956e-01,\n",
      "         -1.8747e-01, -2.3270e-01, -5.4831e-01, -8.4997e-02,  1.0667e-01,\n",
      "         -4.8997e-01,  2.8826e-01, -2.5485e-01, -1.6573e-01, -2.6482e-01,\n",
      "          8.2702e-02, -2.9250e-01, -3.6835e-01, -1.8467e-02, -6.5304e-02,\n",
      "          1.4511e-01, -2.3762e-01,  2.1459e-01,  1.1638e-01,  4.4855e-01,\n",
      "          1.5162e-01,  9.9175e+00, -2.2693e-01, -2.0648e-01, -5.1716e-01,\n",
      "          2.4850e-01,  6.2802e-01, -5.5794e-01, -2.7905e-01, -2.2240e-01,\n",
      "         -3.7438e-01,  3.2401e-01, -3.2920e-01, -3.2367e-01, -1.6140e-01,\n",
      "          3.1972e-01,  4.9555e-01,  2.3839e-01,  3.6990e-03, -5.1016e-02,\n",
      "          3.1062e-01, -2.6051e-01, -1.5326e-01, -7.5095e-02,  6.6761e-02,\n",
      "          2.3877e-01,  2.1955e-01,  2.2975e-02, -8.9634e-02, -3.8576e-01,\n",
      "         -3.6330e-01, -1.0986e-02, -1.3227e-01, -4.8935e-02, -5.1605e-01,\n",
      "          3.4246e-01, -1.8758e-01, -1.6542e-01,  1.1195e-01, -2.4205e-01,\n",
      "         -9.8128e-02,  4.3296e-01,  4.8692e-01, -2.1341e-01,  4.9159e-01,\n",
      "          3.9419e-01, -4.5428e-02, -6.4943e-03, -2.0875e-01,  1.5252e-01,\n",
      "         -5.8126e-01,  3.9467e-01, -1.4692e-01, -1.9556e-01, -5.3512e-01,\n",
      "          5.1753e-02,  1.0278e-01,  2.2770e-01, -3.6194e-01,  2.4392e-01,\n",
      "         -2.8144e-01, -5.5454e-01, -9.7427e-02,  1.5518e-01, -2.5985e-01,\n",
      "          1.8569e-02, -2.2928e-01,  4.9728e-01,  7.3178e-02, -4.2280e-01,\n",
      "          2.1954e-02,  6.9997e-01,  1.3367e-01,  2.2384e-01,  2.7800e-01,\n",
      "         -1.5249e-01, -1.1688e-01, -2.9193e-01, -1.3342e-01, -2.4069e-01,\n",
      "         -2.1262e-01, -1.5155e-01,  4.0197e-01, -1.5232e-01,  2.1315e-01,\n",
      "         -3.9864e-01,  1.5467e-01, -1.6172e-01, -4.7311e-01,  3.2362e-01,\n",
      "         -1.9235e-01, -1.2338e-01, -1.4399e-01,  2.7862e-01,  1.4312e-02,\n",
      "          6.3558e-01,  2.3135e-01, -1.9163e-01,  3.5026e-01,  4.1354e-01,\n",
      "          7.4840e-02, -1.1927e+00, -1.4160e-01, -7.5381e-02, -3.2072e-01,\n",
      "          4.1799e-01,  1.7395e-01, -5.0114e-01,  2.4135e-01, -2.3262e-01,\n",
      "         -1.5997e-01,  5.6750e-01,  4.3025e-01,  8.7940e-01, -1.2149e-01,\n",
      "         -3.0843e-01,  1.9219e-01,  2.5008e-01,  6.3455e-02,  2.5111e-01,\n",
      "         -5.0507e-02,  1.0594e+00,  2.4474e-01,  1.3311e-01, -5.4080e-01,\n",
      "          4.3221e-01, -5.9259e-01, -3.5335e-01,  4.2012e-02, -3.9487e-01,\n",
      "          1.2135e-01,  4.2587e-03,  1.7074e-01, -2.8541e-01,  9.1341e-01,\n",
      "          2.9542e-01,  4.0167e-01, -3.2774e-01,  4.0883e-01, -3.3495e-01,\n",
      "         -3.7979e-02,  9.9271e-02, -1.5361e-01,  1.7923e-01,  4.6583e-03,\n",
      "         -1.5265e-01, -3.5039e-01,  2.3503e-01,  3.8312e-01, -1.3807e-02,\n",
      "         -2.4474e-01, -2.6696e-01, -3.1945e-01,  1.6809e-01,  2.8187e-01,\n",
      "         -5.3722e-02, -9.0932e-02, -6.8548e-01, -5.0145e-01, -1.0163e-01,\n",
      "         -4.9852e-01, -1.6448e-01, -7.8384e-02, -8.1389e-02, -1.0891e-01,\n",
      "         -1.8739e-01,  3.3368e-01,  3.2567e-01, -1.7821e-01, -6.4820e-02,\n",
      "          1.5215e-01,  1.5466e-01,  3.0487e-01, -3.6235e-01, -1.9566e-01,\n",
      "         -4.0878e-03,  3.3202e-01,  4.0394e-01,  5.4862e-01,  3.7471e-02,\n",
      "         -4.7800e-01, -3.7338e-01,  1.1276e-01,  3.2906e-01,  3.1208e-02,\n",
      "         -1.8869e-02, -9.1743e-02, -4.0575e-01, -4.2731e-01, -6.2542e-02,\n",
      "         -1.4663e-01, -2.7783e-01, -2.6644e-01, -2.7254e-01, -2.4324e-01,\n",
      "          6.3105e-02, -2.1532e-01,  2.7953e-02, -6.8763e-01,  1.6568e-01,\n",
      "          3.1359e-01, -5.8367e-01, -1.3701e-02, -3.7712e-01,  4.2577e-02,\n",
      "          5.6400e-01, -2.5877e-01,  3.8855e-01,  1.3309e-01, -9.4506e-02,\n",
      "         -4.0975e-01,  6.5382e-01,  6.8668e-01, -5.1426e-01,  1.3031e+00,\n",
      "         -1.4064e-01,  3.3654e-02, -2.8798e-01,  1.4487e-01, -2.6786e-01,\n",
      "         -5.6472e-01,  2.2837e-01, -1.5605e-01, -6.4160e-01,  9.9792e-02,\n",
      "          1.9272e-01,  4.7652e-01,  2.4551e-01,  3.1553e-01, -3.8992e-01,\n",
      "          1.7513e-01, -3.4111e-02, -2.5750e-01, -2.4779e-01, -7.4581e-01,\n",
      "         -6.8775e-02,  2.1117e-01,  3.1133e-01, -1.9445e-01, -4.5228e-01,\n",
      "         -2.1088e-01, -8.3172e-01, -1.7821e-01, -4.8371e-02,  2.7650e-01,\n",
      "          4.8937e-01, -3.3740e-01, -4.4884e-01, -2.1771e-01, -7.6428e-01,\n",
      "         -2.7977e-01, -4.5843e-01, -7.0504e-03,  9.3992e-02,  5.4912e-02,\n",
      "         -2.5462e-01,  1.4753e-02, -1.2315e-01, -6.4332e-02, -3.5489e-01,\n",
      "         -1.0018e-01, -1.9941e-01, -5.1883e-01, -2.6247e-01, -1.2902e+00,\n",
      "         -2.0850e-02,  4.5512e-01, -4.0560e-02, -2.5919e-02, -5.8213e-02,\n",
      "         -3.1479e-01,  3.5051e-01,  3.9528e-01,  5.7213e-01, -2.9358e-01,\n",
      "          2.9143e-01,  2.7083e-01, -9.4975e-02, -1.0962e-01, -4.1861e-01,\n",
      "         -3.1519e-02,  1.2736e-01, -2.2213e-01, -3.8927e-01, -2.8741e-01,\n",
      "         -1.1883e-01, -4.0485e-01, -3.0008e-02, -8.5593e-01, -3.0098e-01,\n",
      "          5.7329e-02,  2.0359e-01,  9.0890e-01,  5.4515e-01, -7.8063e-02,\n",
      "          2.0351e-01,  1.1678e-02, -2.2461e-01, -8.0181e-02, -3.5846e-02,\n",
      "          1.2198e-01,  1.7813e-01, -1.7955e-01, -4.2743e-02,  3.0500e-01,\n",
      "          4.2593e-01,  5.1404e-02, -4.2828e-01,  6.4336e-01,  3.8191e-01,\n",
      "         -9.9742e-02, -3.2118e-02,  2.1542e-01, -6.1284e-01,  3.9318e-01,\n",
      "          3.0074e-01, -2.5692e-01, -1.0523e-02,  5.2305e-02, -1.2576e-01,\n",
      "          5.1853e-02, -4.2536e-01,  6.0214e-01, -1.2782e-01, -4.7115e-01,\n",
      "          9.9230e-02, -2.2911e-01,  5.5325e-01, -7.4199e-02, -5.1099e-01,\n",
      "         -8.8728e-01,  2.3694e-01,  2.2888e-01, -2.7035e-01,  2.3156e-01,\n",
      "          2.5600e-01, -1.1285e-01, -3.6177e-01,  3.8297e-01, -2.7941e-01,\n",
      "         -2.7619e-01, -2.2937e-01, -1.2285e-01,  2.9599e-02,  8.0633e-01,\n",
      "          4.9590e-01, -4.8629e-01, -1.5190e-01,  2.9643e-01,  1.2456e-01,\n",
      "          2.8873e-01, -4.1250e-01,  1.5291e-01,  5.6961e-02,  2.1235e-01,\n",
      "         -1.5173e-01, -1.4200e-01, -5.0221e-02, -6.2383e-02,  1.8465e-01,\n",
      "         -2.7408e-01,  5.9414e-01, -1.5785e-01,  4.8099e-02, -1.5560e-01,\n",
      "         -5.9309e-02,  7.3397e-01,  1.5008e-01, -8.6132e-03,  9.0595e-02,\n",
      "          6.2162e-02, -2.9582e-01, -3.5080e-01, -2.4142e-01,  2.8607e-01,\n",
      "          1.1601e-03,  5.0201e-02,  4.6201e-01,  4.3106e-01, -9.8616e-02,\n",
      "         -6.4288e-01,  1.4014e-01, -2.2420e-01,  4.4731e-01, -3.2116e-01,\n",
      "          1.9004e-01,  1.0765e-02, -2.9441e-01,  4.2497e-01, -1.6744e-01,\n",
      "         -1.8586e-01, -2.6825e-01, -9.5371e-02,  3.3036e-01, -2.2265e-01,\n",
      "         -3.9151e-01,  4.8502e-01, -2.7690e-01,  1.9741e-01, -1.0455e-01,\n",
      "         -1.9671e-01, -1.1705e-01, -2.4835e-01, -4.6531e-01,  4.1970e-01,\n",
      "         -4.9393e-01,  1.4358e-01,  7.1597e-01,  5.2808e-02,  2.2248e-02,\n",
      "          2.2454e-01,  4.7973e-01,  2.7318e-01,  3.3233e-01,  7.7523e-02,\n",
      "         -4.2657e-01, -1.6550e-01,  1.8227e-01,  8.5383e-02,  1.6031e-01,\n",
      "         -2.4338e-01, -1.9057e-04,  1.6941e-01,  3.8818e-01, -1.7524e-01,\n",
      "          1.8545e-01,  7.6662e-02, -2.9745e-01, -1.2476e+00, -6.1391e-01,\n",
      "          2.2144e-01,  3.4015e-02,  2.1986e-01, -4.6957e-01,  5.6572e-02,\n",
      "         -4.1876e-02,  5.3924e-01, -6.3026e-01,  2.7343e-01,  3.0567e-01,\n",
      "         -6.4665e-01, -3.7123e-01,  1.1540e-02, -1.6257e-01,  3.7577e-01,\n",
      "         -5.4629e-01,  2.7424e-01,  2.1018e-01,  2.3573e-03,  5.8592e-02,\n",
      "          4.8017e-01, -5.8760e-01, -6.4012e-02,  1.9434e-01, -5.3881e-01,\n",
      "          2.0163e-01,  7.0247e-01, -2.3563e-01,  9.3385e-02,  3.0439e-01,\n",
      "          6.1475e-02, -1.0893e+00,  4.9014e-01, -6.6281e-01, -6.5559e-01,\n",
      "          5.5507e-01, -3.6814e-01, -3.5381e-01, -2.5868e-01, -4.6043e-01,\n",
      "         -2.9847e-01,  1.1117e-01, -4.7215e-02,  1.0611e-01,  2.8930e-02,\n",
      "         -1.2089e-01, -2.7319e-01, -5.2250e-01,  5.3802e-01,  2.4385e-01,\n",
      "         -1.0694e-01,  2.0001e-01,  2.1793e-01, -2.6016e-01,  3.3672e-01,\n",
      "          2.1066e-01, -3.9978e-02, -9.5168e-01,  2.3457e-02, -2.7786e-01,\n",
      "          4.0200e-01, -6.1569e-01,  1.1292e-01, -3.2735e-01, -7.0435e-02,\n",
      "         -5.2498e-01, -2.3553e-01,  7.1890e-02, -2.7301e-01, -2.4124e-02,\n",
      "         -9.2939e-02,  4.1697e-01, -3.5408e-01,  2.6557e-01,  2.1341e-02,\n",
      "         -5.0738e-01,  4.6428e-01, -1.0334e-01,  1.9740e-01, -5.8083e-02,\n",
      "          2.4543e-01,  1.5923e-01, -2.3462e-01, -1.4397e-01, -2.5720e-01,\n",
      "         -3.2044e-03, -2.0543e-01,  2.2402e-01, -6.4008e-01,  8.9585e-02,\n",
      "         -1.9059e-01,  1.5243e-01,  8.3722e-03, -1.9974e-01, -4.7684e-01,\n",
      "          2.5776e-01, -2.3194e-01,  2.0996e-01,  6.2705e-01, -2.6211e-01,\n",
      "          5.3076e-01,  4.7339e-01, -1.6855e-02, -1.0862e-01, -1.4018e-01,\n",
      "         -6.4110e-01,  1.7950e-01,  4.0869e-01, -1.9213e-02, -2.0236e-01,\n",
      "          6.5565e-03,  2.0468e-01,  7.8219e-03, -5.5244e-02, -1.1046e-01,\n",
      "          5.0228e-01,  3.3743e-01, -2.7822e-03,  2.7203e-02,  1.6154e-01,\n",
      "          4.5541e-01,  8.9581e-02, -1.8685e-01, -1.0681e-01,  2.8764e-01,\n",
      "         -3.2936e-01,  4.6438e-04,  5.2022e-01, -2.3183e-01, -9.1636e-02,\n",
      "         -1.0614e-01, -2.4226e-01,  1.0299e-01,  5.8520e-02, -5.2734e-01,\n",
      "         -4.9169e-01,  3.4798e-02,  6.7981e-01,  2.6205e-01, -5.5102e-02,\n",
      "          1.5534e-01,  2.2006e-01, -5.8550e-01, -3.1484e-01,  1.3539e-01,\n",
      "         -2.0717e-01,  1.1714e+00,  3.0950e-01,  1.5737e-02,  3.3489e-01,\n",
      "         -3.5562e-01, -9.3242e-02,  2.2407e-02,  1.2903e-01, -2.9655e-01,\n",
      "         -2.1799e-01, -1.4468e-01,  2.5756e-01, -1.5337e-01, -2.5903e-01,\n",
      "          2.9649e-01, -4.9301e-01, -1.1597e-01, -1.4828e-01, -3.8371e-01,\n",
      "          1.5230e-01,  2.8886e-01,  5.1370e-02, -3.9481e-01, -8.5020e-01,\n",
      "         -9.3769e-03,  2.2222e-01, -3.7617e-01, -2.2615e-01,  2.0201e-01,\n",
      "         -2.3318e-01,  4.4718e-01,  6.1245e-02, -4.3432e-01, -1.4893e-01,\n",
      "         -5.1298e-02,  1.9209e-01,  2.2959e-01, -2.0827e-01,  1.3990e-01,\n",
      "          2.0062e-01, -3.9528e-01, -3.4394e-01,  5.3658e-02,  1.0371e-01,\n",
      "          6.3129e-01, -6.1245e-01,  2.4583e-01, -3.3939e-01, -1.4108e-01,\n",
      "         -8.3463e-01, -2.8100e-01, -7.3511e-02,  3.5985e-02, -1.1998e-01,\n",
      "          2.5551e-01, -6.2860e-02, -1.2489e-01, -1.2437e-01,  3.5015e-01,\n",
      "          8.4449e-01, -2.2810e-01,  2.2430e-01,  5.4226e-02, -1.9406e-01,\n",
      "         -2.9753e-02,  4.4338e-02, -2.9486e-02, -5.4136e-01, -5.6112e-01,\n",
      "          7.0701e+00, -8.9728e-02, -2.6980e-01, -2.1640e-02, -1.1760e-01,\n",
      "         -3.4828e-01,  4.4014e-01,  2.4307e-01,  6.3663e-02,  1.1580e-01,\n",
      "         -1.4702e-01, -8.0346e-02, -8.5546e-02, -2.8921e-01,  2.8521e-01,\n",
      "         -3.1764e-01, -4.5743e-02,  1.4616e-01, -1.0881e-01,  6.4766e-02,\n",
      "          1.8333e-01, -8.4562e-01, -4.7556e-01, -8.0452e-01,  8.2178e-01,\n",
      "         -4.9883e-01, -5.7040e-01, -5.1504e-01,  2.1401e-01, -2.1899e-01,\n",
      "         -2.6427e-01, -1.1146e-01, -2.0570e-01, -1.4150e-01, -1.5034e-01,\n",
      "          9.3098e-02,  3.2510e-01,  1.5771e-01,  1.8984e-01, -9.1931e-02,\n",
      "          4.0933e-01,  2.0624e-01,  1.8156e-01,  5.7724e-01,  2.7090e-01,\n",
      "          4.7362e-01, -2.8250e-01,  3.8533e-01, -1.1237e-01, -1.9009e-01,\n",
      "         -6.2574e-01, -3.2024e-03,  2.0414e-01, -1.8217e-01,  1.1343e+00,\n",
      "          8.4473e-02, -1.1064e-02, -5.0512e-02, -3.3089e-01, -4.1621e-01,\n",
      "         -3.7123e-02,  3.0044e+00,  5.1839e-01,  2.4349e-01, -5.2138e-01,\n",
      "          9.5585e-02, -8.4830e-02, -4.0007e-01,  2.9005e-01, -7.1092e-01,\n",
      "          1.3797e-01,  2.8602e-01, -2.0486e-01]], grad_fn=<MeanBackward1>)\n",
      "tensor([[ 3.2193e-01,  6.7185e-02,  2.1524e-01,  1.2046e-01,  2.7544e-01,\n",
      "         -7.1844e-03, -2.8346e-01,  1.1066e-01, -3.2986e-01,  2.4691e-01,\n",
      "         -5.6837e-01, -7.3714e-01,  1.1942e-01,  1.5840e-01,  1.5004e-01,\n",
      "         -6.1985e-01,  6.1880e-01, -1.8923e-01,  2.2839e-01,  1.5934e-01,\n",
      "         -1.0319e-01,  1.6766e-01,  1.1145e-01,  5.3095e-02,  1.0046e-01,\n",
      "         -1.0917e-01, -5.8803e-02, -3.3936e-01,  2.5813e-01, -3.2926e-01,\n",
      "         -2.8990e-01,  8.2206e-02, -3.9266e-02, -2.2378e-01,  8.7309e-02,\n",
      "         -4.5781e-01, -2.0501e-02, -5.4711e-01, -8.6422e-01, -5.0971e-02,\n",
      "         -3.2175e-01, -9.1920e-02, -9.7586e-02,  2.5818e-01,  4.2483e-02,\n",
      "         -3.4214e-02, -8.6280e-02, -3.0946e-01, -1.3511e-03,  1.2904e-01,\n",
      "         -3.4128e-01,  9.4438e+00, -2.1505e-01, -2.0430e-01, -3.5605e-01,\n",
      "          6.0677e-01,  6.2734e-01, -7.2378e-01,  1.2673e-01, -3.4855e-01,\n",
      "          2.7497e-01,  2.5425e-01,  3.6037e-01,  1.1677e-02, -1.0598e-01,\n",
      "          5.5856e-01,  2.4699e-01, -1.5315e-01, -6.3868e-02, -3.9344e-01,\n",
      "          2.6296e-01, -4.2513e-01, -9.9570e-02,  6.1378e-02,  2.3522e-01,\n",
      "          3.9589e-01,  2.2506e-01, -2.5858e-01, -2.1156e-01, -2.4316e-01,\n",
      "         -6.8950e-01, -6.1679e-02, -8.8437e-02,  4.7885e-01, -5.9119e-01,\n",
      "         -1.1715e-01, -2.9747e-01, -2.5985e-01,  1.4626e-01, -3.5788e-01,\n",
      "         -2.6696e-01,  3.3374e-01,  2.3464e-01,  3.2689e-02,  7.0236e-01,\n",
      "         -2.8164e-01, -3.3297e-01,  6.2407e-02, -2.9804e-01, -1.0407e-01,\n",
      "         -5.5491e-01,  4.3945e-01, -1.6007e-01, -8.3062e-02, -4.5072e-01,\n",
      "          6.1091e-01, -3.4183e-01, -6.6077e-02, -1.9276e-01,  2.8778e-01,\n",
      "          2.5784e-01, -1.4137e-01,  1.3770e-01,  9.0897e-02, -2.1279e-01,\n",
      "          1.4242e-01,  1.7759e-01,  2.7813e-01, -1.8367e-01, -7.0332e-01,\n",
      "          6.1799e-01,  4.0566e-01, -6.9436e-03,  4.7445e-01,  2.0061e-01,\n",
      "         -6.8073e-02,  1.0072e-01, -2.1866e-01, -1.0149e-01, -9.1593e-02,\n",
      "         -1.2162e-01,  1.3970e-01,  5.5270e-01,  4.3328e-01,  1.2400e-01,\n",
      "         -1.9967e-01,  1.8367e-02, -2.1877e-01, -6.6628e-01,  3.6710e-01,\n",
      "          2.5259e-02, -4.2154e-02,  1.7056e-01, -1.8033e-02,  1.1272e-02,\n",
      "          4.6295e-01,  1.0769e-01, -4.0648e-01, -5.0934e-02,  2.2334e-01,\n",
      "         -3.8620e-01, -8.4545e-01, -1.0827e-01, -3.2385e-01, -1.0930e-01,\n",
      "          1.3251e-01,  1.0411e-01, -2.4502e-01,  5.2938e-01, -6.2622e-01,\n",
      "         -1.1401e-01,  1.7872e-01,  1.7823e-01,  7.7152e-01, -8.0634e-02,\n",
      "         -3.1659e-01, -6.0000e-02,  2.9837e-01,  3.6118e-01,  1.6693e-01,\n",
      "         -1.6433e-01,  2.8717e-01,  1.0690e-01,  7.4181e-01, -8.0111e-01,\n",
      "          4.6295e-01, -1.1157e-01, -3.0709e-01,  9.7190e-03, -2.7088e-01,\n",
      "          1.7976e-01, -1.6879e-01, -5.7931e-04, -1.2577e-01,  5.7916e-01,\n",
      "          4.0851e-01,  4.2834e-01, -1.2449e-01,  2.7901e-01,  5.3119e-02,\n",
      "          2.6007e-02,  1.2630e-01, -2.2513e-01,  3.7869e-02, -8.9245e-02,\n",
      "          1.2496e-01, -3.4470e-01, -1.4273e-01,  3.5395e-01,  2.1407e-01,\n",
      "         -4.5645e-01,  1.1225e-01, -2.6768e-01,  3.2699e-01,  7.0096e-02,\n",
      "         -1.2301e-01, -1.9065e-01, -4.6448e-01, -3.9995e-01, -9.1017e-02,\n",
      "         -3.8246e-01, -4.6277e-02, -1.8014e-01, -1.8142e-01,  5.8569e-02,\n",
      "         -1.9455e-01,  4.2598e-01,  1.8558e-01,  2.7947e-02,  1.2051e-01,\n",
      "          4.2303e-01, -2.4826e-01,  1.5568e-01, -5.5105e-02,  8.7650e-02,\n",
      "         -1.7499e-01,  2.3443e-01, -1.9711e-01,  2.4755e-01,  3.4361e-02,\n",
      "         -1.3761e-01, -8.6814e-02,  4.6824e-02,  7.9557e-02, -1.4788e-01,\n",
      "          9.4988e-02, -1.5309e-01, -4.2740e-01, -6.6383e-01,  2.0063e-01,\n",
      "         -2.1316e-01,  1.4344e-01, -2.7107e-01,  1.0774e-01, -1.6306e-01,\n",
      "         -5.0296e-02,  6.9719e-02, -4.7833e-01, -4.6881e-01,  5.4471e-01,\n",
      "          4.7013e-01, -4.1354e-01, -1.1837e-01, -4.1436e-01,  4.0996e-01,\n",
      "         -1.9252e-01,  4.2794e-01,  3.3198e-01, -4.2660e-02,  7.6780e-02,\n",
      "         -1.6588e-01,  2.0080e-01,  3.7246e-01, -3.3396e-01,  1.4882e+00,\n",
      "         -1.7035e-01, -7.7493e-02, -3.2024e-01,  4.1307e-01, -2.9630e-01,\n",
      "          1.1476e-01, -2.6887e-01, -2.9078e-01, -1.0988e+00, -5.4029e-01,\n",
      "          8.6818e-02,  3.0834e-01,  3.2991e-01,  8.2555e-02, -4.4030e-01,\n",
      "          3.1178e-01, -4.7157e-01, -8.6121e-02, -2.3948e-01, -6.5079e-01,\n",
      "         -1.8316e-01,  3.0282e-01,  8.5823e-02,  2.3735e-01, -2.7064e-01,\n",
      "         -5.9188e-01, -4.7648e-01, -3.3444e-02,  3.4524e-01,  1.1388e-01,\n",
      "          8.3974e-01, -4.4769e-01, -2.5634e-01, -3.3773e-01, -3.8707e-01,\n",
      "         -2.9303e-01, -5.6263e-01, -3.1058e-01, -2.1625e-01, -4.8634e-01,\n",
      "         -1.9095e-01,  2.4420e-01,  2.1250e-02, -1.3847e-01, -5.1503e-01,\n",
      "         -5.3771e-01, -3.1945e-01, -4.5347e-01,  5.2906e-02, -1.5131e+00,\n",
      "         -4.2765e-01,  1.0620e-01, -5.0506e-02, -2.3444e-01,  2.5109e-01,\n",
      "         -3.8488e-01, -8.0963e-02,  1.1645e-01,  9.9133e-02, -1.7967e-01,\n",
      "          3.5154e-01,  5.6475e-01, -7.1949e-02, -5.0225e-02, -2.7351e-01,\n",
      "          2.6165e-01,  2.3576e-01, -1.2700e-01, -2.5670e-01, -2.4589e-01,\n",
      "         -4.0188e-02, -3.3946e-01,  1.3766e-02, -2.4077e-01, -4.6037e-01,\n",
      "          5.7860e-01, -2.8296e-01,  1.1384e+00,  2.4074e-01, -1.6915e-01,\n",
      "          3.1456e-01,  1.1950e-01, -7.7960e-02, -1.8947e-01,  7.3387e-01,\n",
      "         -9.8318e-02, -1.5067e-01, -2.9812e-01,  2.6520e-03, -5.1554e-02,\n",
      "          2.0218e-01,  1.4718e-01, -7.1670e-02,  6.1401e-01, -5.0837e-01,\n",
      "         -1.3552e-01,  1.6109e-01,  1.7887e-01, -3.0070e-01,  1.7851e-01,\n",
      "          4.9792e-01, -2.3762e-01,  2.9858e-01,  7.3907e-02, -2.6994e-01,\n",
      "         -3.5765e-02, -2.7690e-01,  5.1562e-01,  2.8343e-01, -1.6359e-02,\n",
      "         -7.9626e-02,  1.5491e-01,  4.9547e-01, -6.4238e-02, -2.1203e-01,\n",
      "         -1.0564e+00,  9.1826e-02,  1.8571e-01, -3.2018e-01,  4.1818e-01,\n",
      "         -2.8171e-02, -3.1985e-01, -3.4010e-02,  5.7287e-01, -1.3257e-01,\n",
      "         -2.7719e-01,  2.1997e-01, -2.4302e-01, -3.7256e-01,  9.0145e-01,\n",
      "          4.5689e-01, -3.9360e-01, -3.8110e-02,  3.3917e-01,  1.1477e-01,\n",
      "         -2.2446e-01, -2.1298e-01,  2.7354e-01,  1.6333e-01,  3.0413e-01,\n",
      "         -4.3622e-01, -2.1937e-01, -2.8204e-01,  4.9203e-01,  7.0621e-02,\n",
      "         -2.4966e-01,  2.9699e-01,  2.2108e-01, -1.1725e-01,  1.8567e-01,\n",
      "          8.7552e-02,  8.7682e-01, -2.1849e-01,  2.0300e-02, -4.7530e-02,\n",
      "         -4.2561e-01, -4.9324e-01, -7.0075e-02, -4.4353e-01,  3.5209e-02,\n",
      "          8.7856e-02,  3.3295e-01,  4.8524e-01,  7.1610e-01, -3.0440e-01,\n",
      "         -2.3625e-01, -1.9633e-01, -5.0600e-01,  9.3097e-01, -2.1494e-02,\n",
      "          3.4556e-01, -2.8899e-01, -1.5436e-01,  1.4311e-01,  7.5109e-02,\n",
      "         -1.6440e-01, -2.4931e-01, -3.4902e-02,  4.9528e-01, -1.0751e-02,\n",
      "         -3.0724e-01,  1.3498e-01, -1.4189e-01,  5.7089e-01, -2.7116e-01,\n",
      "         -2.9700e-01, -2.0008e-01, -1.4551e-01, -5.0334e-01,  1.7504e-01,\n",
      "         -1.0461e-01,  2.0256e-01,  5.4272e-01, -2.9258e-02,  2.4513e-01,\n",
      "          3.6171e-01,  1.8467e-02, -2.6359e-01,  2.5592e-01,  5.3625e-02,\n",
      "         -5.9501e-02, -4.2599e-01,  2.8835e-01, -9.5752e-02, -5.9866e-01,\n",
      "         -3.5269e-01, -1.5727e-01,  7.3269e-01,  2.3089e-01, -2.4047e-01,\n",
      "          3.3915e-01,  1.0159e-01, -2.6145e-01, -1.6875e+00, -5.6602e-01,\n",
      "          6.1286e-02,  6.6074e-02,  6.2059e-02, -1.2961e-01, -7.0955e-02,\n",
      "          1.7966e-01,  4.8564e-01, -5.3528e-03,  2.3751e-01,  1.2096e-01,\n",
      "         -5.3898e-01, -5.8567e-01,  3.1169e-01, -3.6092e-01,  8.5592e-02,\n",
      "         -1.7818e-01,  3.9082e-01,  4.4898e-01, -1.8021e-01,  1.7759e-01,\n",
      "          2.6643e-01, -6.4273e-01,  1.8501e-01, -4.1305e-02, -2.7291e-01,\n",
      "          2.0618e-01,  2.8938e-01, -3.2513e-01,  3.3989e-02,  6.3985e-02,\n",
      "          1.7769e-01, -1.3842e+00,  5.1385e-01, -6.3287e-01,  4.9798e-02,\n",
      "          6.0384e-01,  1.2295e-01, -1.9667e-01, -2.9388e-02, -4.0155e-01,\n",
      "         -3.4209e-01,  5.9446e-02, -3.3323e-02,  4.4818e-02,  3.0804e-01,\n",
      "         -5.3116e-01,  2.9297e-02, -3.0907e-01,  5.1967e-01,  2.3167e-01,\n",
      "         -2.5224e-01,  1.2112e-01, -3.7518e-04, -4.4532e-01,  7.3041e-01,\n",
      "          3.2378e-01,  3.3325e-01, -7.0762e-01,  4.3988e-02, -1.3050e-01,\n",
      "          3.5482e-01, -5.4167e-01,  1.0649e-01, -4.9868e-01,  1.5450e-02,\n",
      "         -4.9114e-01, -2.9805e-01,  5.6818e-01, -1.3258e-01, -7.5081e-02,\n",
      "         -6.5219e-01,  9.8852e-02, -8.1340e-01,  1.8103e-01, -4.4608e-01,\n",
      "         -4.9887e-01,  1.3031e-01, -4.8674e-02,  3.8080e-02,  1.9360e-01,\n",
      "          1.3606e-01,  3.5177e-01, -6.0134e-01, -2.6583e-01,  1.0812e-01,\n",
      "          4.8789e-02,  5.6229e-03,  2.2537e-01, -8.7628e-01,  2.9964e-02,\n",
      "         -5.7273e-01,  1.2971e-01, -1.3987e-01, -2.7091e-01, -1.7369e-01,\n",
      "          3.0619e-01,  1.7623e-01,  5.3584e-02,  4.2744e-01, -2.5780e-01,\n",
      "          5.5172e-01,  4.6565e-01,  3.1819e-01, -3.2220e-01, -5.1970e-01,\n",
      "         -1.6906e-01,  2.4374e-01,  2.8670e-01,  5.7065e-02, -4.3624e-01,\n",
      "         -1.6868e-02,  1.9008e-01, -1.0384e-01, -1.2019e-02, -1.1375e-01,\n",
      "          1.8784e-01,  6.9553e-01, -1.4613e-01, -9.7740e-02,  1.8030e-02,\n",
      "          3.8289e-01, -1.0232e-01, -5.9612e-02,  3.4338e-01, -2.7743e-01,\n",
      "         -6.5095e-01,  5.6867e-01,  7.2794e-02, -4.1906e-01, -2.4378e-01,\n",
      "          7.9001e-02,  1.5024e-01,  9.8455e-02, -1.4038e-01, -6.0831e-01,\n",
      "         -3.6212e-01, -4.3488e-02,  2.7424e-01,  1.7192e-02, -2.4797e-02,\n",
      "          6.8439e-02,  1.2794e-01, -6.5740e-01, -9.0474e-02,  2.1282e-01,\n",
      "          3.6274e-01,  1.3408e+00,  2.1897e-01,  2.2397e-02, -3.5540e-02,\n",
      "         -4.7275e-01,  5.0351e-02, -7.4644e-02,  4.5696e-01,  3.4270e-01,\n",
      "         -7.7425e-02,  1.1777e-01, -8.5204e-02,  5.3658e-02, -1.5050e-01,\n",
      "          2.7686e-01, -1.1838e-01, -1.9676e-01,  1.2855e-01, -3.8712e-01,\n",
      "          7.2726e-02,  1.5640e-02, -1.0823e-01, -1.8454e-01, -3.9471e-01,\n",
      "         -4.5158e-01,  6.9986e-02, -1.1667e-01, -5.0352e-01, -2.6658e-02,\n",
      "         -8.3563e-02,  3.2908e-01,  2.6452e-02,  2.9310e-02, -3.2066e-01,\n",
      "         -6.8524e-02, -2.7182e-01,  5.0116e-02,  7.0741e-02, -2.2021e-01,\n",
      "          3.7623e-01, -3.5376e-01, -6.3654e-01,  6.2047e-02,  5.0618e-01,\n",
      "          9.4242e-01, -2.1324e-01,  1.8682e-01, -3.0529e-01, -1.3781e-01,\n",
      "         -6.3253e-01,  7.9449e-02,  6.6895e-02, -1.7294e-01, -2.7962e-02,\n",
      "          1.7438e-01, -4.1076e-01, -4.0402e-01, -8.9822e-02, -1.3359e-01,\n",
      "          8.4773e-01,  7.1109e-02,  1.3514e-01,  9.9512e-02, -2.8790e-01,\n",
      "          8.5249e-02, -1.1326e-01, -1.1030e-01, -5.7276e-01, -6.4193e-01,\n",
      "          8.2249e+00, -1.2191e-01, -2.7365e-01,  2.0544e-01,  6.5383e-03,\n",
      "         -5.2324e-01,  9.0010e-02,  8.1890e-02,  1.2232e-01, -4.2269e-01,\n",
      "         -4.6045e-01, -2.8855e-01,  7.6182e-02,  1.2623e-01,  7.5467e-01,\n",
      "         -2.5560e-01,  4.9788e-01, -2.6749e-01, -1.6190e-01, -1.6594e-01,\n",
      "          3.9699e-01, -6.9726e-01, -4.3455e-01, -4.9133e-01,  8.3395e-01,\n",
      "         -5.4713e-02, -6.9031e-01, -6.5187e-01,  3.5729e-01, -2.3216e-01,\n",
      "         -1.9121e-01, -3.5248e-01, -2.3603e-01, -5.6126e-02,  1.5550e-03,\n",
      "          5.1054e-01,  2.3115e-01, -3.2949e-02,  1.0580e-01, -1.3243e-01,\n",
      "          6.8827e-01, -4.8317e-01, -1.4947e-01,  2.8624e-01, -5.9865e-02,\n",
      "         -1.9792e-01,  2.6099e-02,  6.2024e-01,  2.2977e-01, -1.3604e-01,\n",
      "         -6.6611e-01,  1.7867e-01, -3.8156e-01, -7.1164e-01,  1.5615e+00,\n",
      "          5.5282e-02, -5.5074e-01, -9.7113e-02, -2.5416e-01, -2.0484e-01,\n",
      "         -3.3966e-01,  3.2433e+00,  1.9461e-01,  2.3576e-01, -9.3789e-02,\n",
      "         -2.4017e-02,  2.9077e-01,  8.6201e-02, -2.7754e-01, -3.8367e-02,\n",
      "         -1.3652e-01,  5.9799e-01, -2.7048e-01]], grad_fn=<MeanBackward1>)\n",
      "tensor([[ 2.2178e-01,  8.6987e-02, -1.3152e-01, -3.4704e-01,  2.0920e-01,\n",
      "          1.3081e-01, -4.4405e-01,  9.2684e-02, -3.8919e-01,  3.5393e-01,\n",
      "          3.1455e-02, -6.6182e-01, -3.6134e-02,  3.7641e-01,  3.1383e-01,\n",
      "          3.5445e-02,  2.4374e-01,  1.0835e-01,  2.3069e-01,  1.0242e-01,\n",
      "         -2.3295e-01,  5.8619e-01, -1.4347e-01, -4.4520e-01,  1.0601e-01,\n",
      "         -6.2718e-01,  3.8597e-01, -1.1460e+00,  7.9212e-02, -1.0550e+00,\n",
      "         -6.2702e-01,  1.1461e-01, -5.2008e-01,  1.6676e-01,  1.1806e-01,\n",
      "          2.6173e-01, -4.9652e-01, -7.8440e-02, -6.7778e-01, -6.4063e-02,\n",
      "          4.7661e-01, -2.8100e-01, -7.1701e-02, -4.0007e-01,  1.1900e-01,\n",
      "          2.7641e-01, -1.2426e-01, -5.0110e-01,  4.2520e-02,  3.1402e-01,\n",
      "         -2.3829e-01,  8.2747e+00, -3.4882e-01, -2.0266e-01,  4.3119e-02,\n",
      "          6.7117e-01,  6.3720e-01,  4.8353e-02,  1.2621e-01, -1.5472e-01,\n",
      "         -2.5401e-01,  2.8280e-01, -7.6968e-01, -1.8546e-01, -2.1744e-01,\n",
      "         -2.2994e-01,  5.8521e-01,  1.6933e-01,  3.4121e-01, -6.7431e-02,\n",
      "          4.0531e-01,  1.3224e-01,  1.3933e-01,  2.9337e-02,  1.9171e-01,\n",
      "          5.9934e-01,  2.8690e-01, -2.9679e-01, -2.2105e-01, -1.0851e-01,\n",
      "         -5.1943e-01, -2.5075e-01, -3.5087e-01, -1.7117e-01, -7.8114e-01,\n",
      "          1.3200e-01, -1.4385e-01, -3.7004e-01,  4.3636e-01, -6.3937e-01,\n",
      "         -1.7073e-01, -4.1245e-01, -1.5745e-01, -4.1117e-01,  4.1681e-01,\n",
      "          7.8019e-02, -5.7457e-01,  1.7896e-01, -4.1211e-01, -4.2487e-01,\n",
      "         -9.4072e-01,  4.5621e-01, -8.5102e-02,  1.8506e-01, -2.0545e-01,\n",
      "         -4.6322e-02, -3.5687e-02, -3.8013e-02, -4.5884e-01,  4.3218e-01,\n",
      "         -7.0335e-02, -6.0580e-01,  7.5937e-02,  1.4715e-01, -2.3036e-01,\n",
      "         -3.0288e-01, -1.4926e-01,  6.3680e-01,  4.1552e-01, -4.5359e-01,\n",
      "          7.2083e-01,  6.8888e-01,  4.4867e-01,  2.9563e-03,  2.3961e-01,\n",
      "         -5.1627e-01,  4.5769e-01, -3.6072e-01, -9.3527e-02, -1.6262e-01,\n",
      "          1.7606e-01, -3.9545e-01,  7.1196e-01,  6.1802e-03,  1.3528e-01,\n",
      "         -1.0483e-01,  1.4432e-01, -6.4319e-01, -6.9381e-01,  2.4225e-01,\n",
      "         -1.0985e-01,  1.8452e-01, -3.3605e-01,  1.9704e-01,  3.1373e-01,\n",
      "          5.6570e-01,  5.5720e-01,  3.8515e-01,  2.2051e-01,  9.3357e-01,\n",
      "          6.5330e-01, -5.6000e-01,  3.4942e-01,  5.0997e-02, -3.2644e-01,\n",
      "          1.1602e-01,  2.1403e-01, -5.1585e-01,  6.5191e-01, -6.0337e-01,\n",
      "         -2.1724e-01, -8.3538e-02,  1.1729e-01,  8.6961e-01,  8.6658e-02,\n",
      "         -7.2477e-01, -4.9716e-01, -1.1572e-01,  3.3890e-01,  2.8987e-01,\n",
      "          5.4476e-02,  3.2734e-01, -2.7964e-01,  4.8603e-01, -5.0489e-01,\n",
      "          1.0292e-01, -3.2433e-01, -7.1367e-01, -2.9237e-01, -3.8248e-01,\n",
      "         -1.2952e-02, -2.5557e-02, -2.1471e-01,  1.8506e-01,  5.8482e-01,\n",
      "          5.1334e-01,  3.2276e-01, -2.1819e-01, -7.9889e-02, -1.3961e-01,\n",
      "          3.8476e-01,  4.5406e-01,  1.2746e-02,  9.4336e-02,  3.4535e-03,\n",
      "          6.2793e-02, -3.7602e-01,  4.9532e-02,  7.5449e-01,  2.6841e-01,\n",
      "         -5.4373e-01, -4.6501e-01, -4.0967e-01,  2.2455e-02, -2.8925e-01,\n",
      "         -2.5443e-01,  3.0533e-01, -1.2292e-01, -5.8889e-01, -4.8789e-01,\n",
      "         -5.7905e-02, -2.0167e-01,  1.2105e-01, -2.9866e-01,  3.6222e-01,\n",
      "         -6.0424e-01,  2.9344e-01,  9.7507e-02,  4.2551e-01,  3.9767e-02,\n",
      "          2.1652e-03, -2.9624e-01,  3.3632e-01,  1.6066e-01,  2.1778e-01,\n",
      "         -2.4874e-01, -5.6195e-01, -3.7776e-03,  2.6760e-01, -1.4532e-01,\n",
      "         -1.5555e-01,  3.1654e-01,  1.9075e-01, -1.8541e-01, -1.9245e-01,\n",
      "         -5.7722e-01,  2.4769e-01,  3.0926e-01, -3.9202e-01,  8.5211e-02,\n",
      "          8.5591e-03, -2.2286e-01, -3.5288e-01, -1.1695e-01,  2.8849e-03,\n",
      "         -1.0996e-01,  3.1376e-01,  3.1349e-01, -3.6011e-01,  3.9179e-01,\n",
      "         -1.5606e-01, -3.6296e-01, -7.9870e-02,  6.0074e-02, -3.1481e-01,\n",
      "          4.1561e-02,  1.2567e-01,  1.2098e-01, -1.3833e-01,  3.1568e-01,\n",
      "          1.8306e-01,  1.6132e-01,  9.2928e-01, -4.1784e-01,  1.5962e+00,\n",
      "         -1.6730e-01,  3.2727e-01, -1.4417e-03,  3.2261e-01, -3.9990e-01,\n",
      "         -1.1512e-01,  4.4663e-01,  1.5948e-01, -8.6845e-01, -6.4276e-01,\n",
      "          3.0591e-01,  2.7305e-01, -5.3570e-02,  3.4032e-01, -2.4295e-01,\n",
      "          4.9630e-01,  1.8797e-02, -5.1304e-01, -1.0448e-01, -1.1279e+00,\n",
      "         -3.9591e-01, -9.3693e-02, -4.2240e-01, -3.5103e-01, -6.9324e-01,\n",
      "         -1.4591e-01, -8.1169e-01, -1.7312e-01,  3.7352e-01,  2.2759e-01,\n",
      "          1.0124e+00,  8.1555e-02, -4.9475e-01, -9.3700e-02, -8.4607e-01,\n",
      "         -3.2753e-01,  6.0287e-03, -3.3820e-01,  2.3226e-01,  4.1346e-01,\n",
      "         -1.8700e-02, -2.5625e-01, -1.9847e-01,  4.2204e-01,  2.2069e-01,\n",
      "         -2.4025e-01,  4.2313e-02, -4.6856e-01,  1.8039e-02, -1.0994e+00,\n",
      "          7.9616e-03,  1.1987e-01, -4.8412e-01, -1.2440e-01,  5.8714e-02,\n",
      "         -1.9177e-01,  1.0068e-01,  1.7872e-01,  5.0732e-01,  1.6110e-01,\n",
      "          6.1655e-02,  3.2920e-01, -1.7893e-01,  1.9522e-01, -5.5257e-01,\n",
      "          3.5459e-01,  1.7151e-01, -2.3630e-01, -2.0697e-01, -6.4303e-02,\n",
      "         -1.3984e-01,  2.3284e-01,  3.6758e-02, -3.1052e-01, -6.7815e-01,\n",
      "          5.3773e-01, -2.9577e-02,  1.0961e+00,  1.6984e-01, -6.8713e-01,\n",
      "          3.4455e-01,  1.0988e-01, -3.1182e-01,  1.7987e-01,  3.5250e-01,\n",
      "          9.6196e-02,  2.3131e-01, -2.4558e-01,  3.0768e-01,  3.4630e-01,\n",
      "          2.1139e-01, -7.4810e-01, -1.4754e-02, -4.5983e-01, -7.9498e-02,\n",
      "         -1.3185e-01,  3.6859e-01,  5.3224e-01, -4.1349e-01,  1.2679e-01,\n",
      "          5.2490e-01, -1.2627e-01, -6.2076e-02,  3.3767e-01, -5.2157e-01,\n",
      "          1.9199e-01, -1.0717e-02,  3.3443e-01, -5.1978e-01, -4.7103e-01,\n",
      "          1.0350e-01, -2.8161e-01, -1.1824e-02, -3.8860e-03, -7.5834e-02,\n",
      "         -6.4140e-01,  4.0169e-01,  2.7898e-01,  2.1404e-01,  7.3371e-01,\n",
      "         -2.3676e-01, -3.5278e-02,  7.7152e-03,  3.2725e-01,  1.1586e-01,\n",
      "         -6.3319e-01, -1.1747e-01, -3.6859e-01,  2.3059e-01,  4.4910e-01,\n",
      "          1.1307e+00,  2.5586e-01, -1.9934e-01, -2.2834e-01, -4.7772e-01,\n",
      "         -2.7793e-01,  1.6089e-01, -9.2034e-02,  1.1640e-01, -1.5641e-02,\n",
      "         -7.3588e-01, -6.2849e-02,  1.5232e-01,  2.9042e-01, -2.6848e-01,\n",
      "         -2.9752e-01,  2.2942e-01, -2.6278e-02, -2.3962e-01, -1.5785e-01,\n",
      "          3.6095e-01,  6.3020e-01, -3.8526e-01, -2.7251e-01, -1.6210e-01,\n",
      "          6.1471e-02, -5.9288e-01,  6.0123e-02,  2.6024e-01,  1.5065e-01,\n",
      "         -1.6207e-01, -1.5039e-02,  3.6895e-01,  7.4113e-01, -6.9355e-02,\n",
      "          6.7055e-02,  4.5755e-02, -4.0021e-02,  7.8654e-01,  8.7001e-02,\n",
      "          3.8120e-01, -3.1058e-01, -2.1040e-01,  2.2062e-01,  1.1839e-01,\n",
      "          1.1828e-01, -5.5895e-01, -1.0230e-01,  8.3365e-01, -2.1176e-01,\n",
      "         -1.2818e-01,  4.6714e-01, -4.9429e-01,  2.4465e-01, -1.3264e-01,\n",
      "          2.4603e-01,  1.2057e-01,  2.5216e-01, -3.0202e-02,  2.3048e-02,\n",
      "         -7.8718e-01,  4.3458e-01,  5.7286e-01, -3.1249e-02, -2.3594e-01,\n",
      "         -2.5327e-01,  3.9646e-01, -4.6600e-01,  1.7413e-01,  1.6699e-01,\n",
      "         -1.6717e-01, -3.4393e-01,  6.0804e-01, -8.3970e-01, -4.1661e-01,\n",
      "         -7.1245e-01,  9.2746e-02,  7.5656e-01,  1.1850e-01, -3.4187e-01,\n",
      "          5.6136e-01,  2.2793e-02, -1.8315e-01, -1.0644e+00, -5.2318e-01,\n",
      "          3.4736e-02, -4.4758e-01,  5.9095e-02, -2.2100e-01, -2.7489e-01,\n",
      "         -2.4568e-01,  3.4660e-01, -1.8980e-01,  2.1442e-01, -7.3464e-02,\n",
      "         -5.9303e-01, -3.9412e-01,  8.1673e-02,  1.1310e-01,  1.0858e-02,\n",
      "         -3.2107e-01,  6.5007e-02,  7.9256e-02, -3.4797e-02,  1.9464e-01,\n",
      "          3.7586e-01, -5.4094e-01, -1.3347e-01, -1.0526e-01, -3.0561e-01,\n",
      "          4.7998e-01,  3.6474e-01,  1.3657e-01,  5.3777e-02, -1.3462e-01,\n",
      "          7.7204e-03, -1.0329e+00,  9.0437e-01, -8.1626e-01, -6.4025e-02,\n",
      "          1.9964e-01, -2.2461e-02, -5.0575e-01,  1.9970e-01, -3.5806e-01,\n",
      "         -3.4153e-01,  4.6389e-02,  5.6035e-01,  2.1082e-01,  4.3826e-01,\n",
      "          7.3020e-02,  1.1764e-01, -3.6077e-01,  5.7938e-01,  1.0873e+00,\n",
      "         -3.9671e-01,  3.2590e-01, -4.5181e-01, -1.4844e-01,  2.5040e-01,\n",
      "          3.3958e-01,  4.1942e-01, -8.7417e-01, -5.0692e-01,  1.8004e-01,\n",
      "          2.3077e-02, -8.0477e-01,  4.4516e-01, -5.5588e-01, -1.2771e-01,\n",
      "         -1.8286e-01, -5.9401e-01,  2.8761e-01, -4.1684e-01, -1.8908e-01,\n",
      "         -9.6220e-02,  3.2036e-01, -4.4828e-01,  2.4921e-01, -9.0322e-02,\n",
      "         -2.9866e-01,  8.9463e-01, -1.0419e-01, -1.6320e-01,  2.4619e-01,\n",
      "          2.8013e-01,  7.7624e-01, -1.2455e-01, -1.8426e-01, -3.6486e-01,\n",
      "          7.8799e-02,  1.5810e-01, -2.5564e-01, -6.6514e-01,  2.3118e-01,\n",
      "         -4.7227e-01, -2.0860e-01,  2.6199e-01, -4.2019e-01,  4.8870e-02,\n",
      "          1.4100e-01, -8.6083e-03, -1.8139e-01,  4.5958e-01,  1.2670e-01,\n",
      "          2.1269e-01,  4.2397e-01,  3.1679e-01, -1.7375e-01,  2.1677e-01,\n",
      "         -2.3319e-01,  4.2997e-01,  3.2094e-01, -2.4937e-01, -2.2080e-01,\n",
      "         -1.9770e-01, -1.8087e-01,  7.7403e-02,  7.0070e-02,  3.3390e-02,\n",
      "         -1.8925e-01,  3.4839e-01,  1.7688e-01,  1.9659e-01, -2.2402e-01,\n",
      "          3.8765e-01, -4.9163e-01, -7.4785e-01,  1.9271e-02, -1.4156e-01,\n",
      "         -8.7793e-02,  3.0294e-01,  4.4609e-01,  1.3383e-02, -9.8421e-03,\n",
      "          3.0455e-01, -3.6653e-01,  3.3195e-03,  2.4133e-01, -5.5505e-01,\n",
      "         -9.5064e-02, -1.1771e-01,  5.2470e-01,  6.0261e-01, -1.8194e-01,\n",
      "         -3.9996e-01, -8.5161e-03, -5.3094e-01, -2.2997e-01,  3.1225e-01,\n",
      "         -6.8802e-02,  2.4361e-01,  6.4420e-01,  1.3507e-02, -6.5389e-01,\n",
      "         -5.3366e-01,  1.1526e-01, -4.0199e-01,  4.8323e-01, -2.7817e-01,\n",
      "         -5.0910e-01, -2.5345e-01,  2.8607e-01, -2.2640e-02, -1.3756e-01,\n",
      "          1.5684e-01, -7.8765e-01,  2.5080e-01, -3.0122e-01, -4.6713e-01,\n",
      "         -2.7882e-01,  2.3708e-01, -1.9907e-03, -4.7044e-02, -6.1901e-01,\n",
      "         -2.5480e-01,  3.2167e-01, -8.6773e-01, -3.3249e-01,  7.2707e-01,\n",
      "         -4.2285e-01,  4.6768e-01, -4.9359e-01, -4.2135e-01, -3.9831e-01,\n",
      "          1.9405e-01, -5.5586e-01, -2.3159e-01,  2.1439e-01, -5.7477e-01,\n",
      "          2.4192e-02, -4.8212e-01, -5.7451e-01, -6.2902e-01,  8.7813e-01,\n",
      "          1.1630e+00, -4.3354e-01,  3.9717e-01, -3.9470e-01, -1.5563e-01,\n",
      "         -5.2572e-01, -4.9839e-01,  9.5791e-02, -6.1256e-02, -5.8120e-02,\n",
      "          5.6495e-01, -4.6580e-01, -6.3468e-01, -2.7903e-01, -9.2146e-02,\n",
      "          3.9158e-01,  1.0200e-01, -2.3363e-01, -1.0417e-01, -1.4722e-02,\n",
      "          6.7328e-01,  3.6724e-01,  1.9475e-01, -8.7347e-01, -5.1914e-01,\n",
      "          1.0808e+01,  2.7495e-02, -3.4783e-01,  1.0210e-01, -6.4554e-02,\n",
      "         -1.8433e-01, -8.7004e-03,  9.1704e-02,  4.7103e-02, -3.2240e-01,\n",
      "         -5.4505e-01,  1.0319e-01,  1.1404e-01, -6.7741e-02,  2.7077e-01,\n",
      "         -1.2034e-01,  5.1788e-02, -2.2863e-01, -3.8920e-01,  1.0382e-01,\n",
      "          8.1195e-01, -7.9599e-01, -7.9385e-01, -2.6032e-01,  2.7922e-01,\n",
      "         -7.4551e-01, -4.4625e-01, -1.0770e-01, -2.6804e-01, -1.2866e-01,\n",
      "          3.7736e-02, -1.9006e-01,  1.2467e-01, -4.4026e-02, -5.5268e-01,\n",
      "          2.1035e-01,  1.3498e-01,  3.3315e-02,  1.4190e-01,  8.6785e-02,\n",
      "          6.6690e-01, -6.2454e-01, -2.1546e-01,  7.0515e-02,  5.0112e-02,\n",
      "          7.9608e-01, -1.6513e-01,  2.2748e-01,  5.8297e-01, -9.0082e-02,\n",
      "         -3.7423e-01,  4.2774e-01,  2.2795e-01, -5.9078e-01,  1.1271e+00,\n",
      "          3.8206e-02,  1.0636e-01, -4.0786e-01, -5.1568e-03, -3.8653e-01,\n",
      "         -5.0197e-01,  3.6134e+00,  3.5805e-02,  3.5117e-01, -2.7429e-01,\n",
      "          5.2645e-01,  1.4009e-02,  1.0048e-02, -4.9870e-01, -1.4013e-01,\n",
      "          4.3637e-01,  6.5385e-01,  7.5117e-04]], grad_fn=<MeanBackward1>)\n",
      "tensor([[ 4.4503e-01, -3.9394e-02,  3.2673e-02,  5.1893e-01,  6.5775e-01,\n",
      "         -3.8841e-01, -7.2989e-03,  7.0188e-01, -4.4988e-01,  1.5225e-01,\n",
      "         -2.4552e-01, -5.7679e-01, -2.9478e-01,  2.4573e-01,  1.7076e-01,\n",
      "         -3.8229e-01,  4.3036e-01,  7.5661e-02,  1.2467e-01,  3.6124e-01,\n",
      "         -4.3611e-01,  6.8344e-01,  2.4140e-01,  5.4096e-02,  5.0070e-02,\n",
      "         -2.9368e-01, -1.7582e-01, -4.1418e-01,  4.4947e-01, -4.8158e-01,\n",
      "         -7.9039e-01, -3.7592e-01, -4.3887e-01, -1.3904e-01,  1.6268e-01,\n",
      "         -1.8828e-01,  1.8512e-02, -1.8788e-01, -5.0380e-01, -3.2411e-01,\n",
      "          2.9299e-02, -2.0329e-01,  3.2516e-02,  1.3647e-01,  2.5378e-02,\n",
      "          1.6026e-01, -2.2071e-01,  6.1219e-02,  2.8286e-01,  1.4902e-01,\n",
      "          1.0220e-01,  9.7364e+00, -5.1213e-01, -2.7930e-01, -9.6058e-01,\n",
      "          3.9769e-01,  5.9687e-01, -1.7860e-01,  2.7359e-01, -3.9860e-01,\n",
      "         -3.2520e-01,  1.3893e-01, -4.5013e-01,  3.8902e-02, -1.0410e-01,\n",
      "          9.4723e-02,  9.6738e-02,  7.2463e-01,  4.8678e-02, -6.5624e-01,\n",
      "          9.7393e-02, -3.2247e-01, -3.1652e-01, -1.4857e-01,  1.8085e-01,\n",
      "          5.1002e-01, -2.8579e-01,  4.1099e-02, -1.6246e-01, -5.4996e-01,\n",
      "         -6.0553e-01, -6.0606e-02, -4.2777e-01,  3.2107e-01, -3.0163e-01,\n",
      "          4.3055e-01, -3.5896e-01, -3.0020e-01,  5.3531e-01, -2.8189e-01,\n",
      "          1.3695e-02,  3.8775e-01,  3.4805e-01, -1.6004e-01,  5.7040e-01,\n",
      "          3.0390e-01, -3.8105e-01, -1.1714e-01, -1.1672e-01, -1.8631e-01,\n",
      "         -2.9052e-01,  2.3130e-01,  1.3898e-01,  1.6321e-01, -3.6556e-01,\n",
      "          2.5361e-01,  4.3777e-01, -6.4788e-02, -6.3332e-01,  5.4132e-01,\n",
      "          1.3833e-01, -6.0157e-01, -1.4352e-01,  5.8865e-02, -2.9950e-01,\n",
      "          6.4691e-02,  2.1253e-01,  6.1744e-01,  6.8860e-02, -1.9927e-01,\n",
      "          4.3536e-01,  2.1002e-01,  2.8020e-01,  1.8771e-01, -2.7139e-02,\n",
      "         -1.1869e+00, -2.6294e-01, -5.8264e-01, -1.0219e-01,  2.3066e-02,\n",
      "         -2.7625e-01, -5.2972e-01,  7.4889e-01, -2.4413e-02,  3.1305e-01,\n",
      "         -8.0505e-01,  1.0427e-01, -2.1778e-01, -7.2187e-01,  2.7522e-01,\n",
      "         -4.0861e-01,  8.9739e-02, -2.3125e-01,  3.6682e-01,  2.2527e-01,\n",
      "          6.8560e-01, -2.8541e-02, -3.0521e-01,  1.0166e-01,  2.7537e-01,\n",
      "         -1.5993e-01, -7.9574e-01,  1.8761e-02,  8.8995e-02, -3.7921e-01,\n",
      "         -4.2219e-02,  4.4098e-02, -6.1907e-01,  1.6053e-01, -2.2000e-01,\n",
      "         -5.3839e-01,  7.2974e-01, -1.2549e-01,  8.6038e-01,  5.7554e-02,\n",
      "         -3.4707e-01, -2.5309e-01, -7.1088e-02,  1.1735e-01,  1.7627e-01,\n",
      "         -5.1723e-02,  8.0428e-01, -4.3728e-02,  4.6841e-01, -6.3645e-01,\n",
      "          7.4812e-01, -5.8680e-01, -4.4886e-01,  1.8694e-02, -6.5563e-01,\n",
      "         -4.6901e-02, -3.5083e-02,  2.0029e-01, -2.5593e-01,  3.7894e-01,\n",
      "          3.4591e-01,  4.1220e-01, -9.4349e-02,  4.2341e-01, -3.8064e-01,\n",
      "         -8.3019e-02,  3.0858e-01,  1.8703e-02,  1.9023e-01,  3.9691e-01,\n",
      "         -2.5240e-01, -4.4570e-01, -2.4685e-01,  4.8141e-01,  2.9251e-01,\n",
      "         -6.7802e-01,  4.2053e-02, -5.7046e-01,  5.2616e-01,  2.1661e-02,\n",
      "          2.8420e-01,  8.1848e-02, -7.7679e-01, -3.6229e-01, -5.6096e-01,\n",
      "         -6.3916e-01,  2.5362e-02,  2.0315e-01,  8.7336e-02,  2.3719e-01,\n",
      "         -5.7623e-01,  5.8053e-02,  4.7234e-01,  1.2828e-01,  3.6592e-01,\n",
      "          9.2490e-02, -2.3845e-02,  2.9971e-01, -3.0695e-01,  1.0493e-01,\n",
      "         -3.0135e-01,  5.2423e-01,  7.0878e-02,  3.5716e-01, -1.4894e-02,\n",
      "         -1.2634e-01, -3.9952e-01,  3.1995e-01,  4.9634e-01, -9.6707e-02,\n",
      "         -1.3226e-01, -6.1036e-02, -4.1290e-01, -3.4472e-01,  3.4118e-01,\n",
      "         -1.8805e-01,  2.1566e-01, -4.7486e-01, -1.3032e-01, -4.3388e-01,\n",
      "         -1.8932e-01, -3.9361e-01, -3.8786e-01, -6.7739e-01,  1.6222e-01,\n",
      "          3.3465e-01, -1.5041e-01,  1.5762e-01, -3.3208e-01, -3.6827e-01,\n",
      "          1.4162e-01, -3.2746e-01,  1.9100e-01, -1.1088e-01, -2.5103e-02,\n",
      "          8.2996e-02,  6.7674e-01,  4.2258e-01, -3.7320e-01,  1.6051e+00,\n",
      "         -2.5177e-01, -1.6746e-01, -4.9782e-01,  5.5680e-01, -2.4673e-01,\n",
      "         -5.7066e-01,  2.1536e-01,  5.5132e-02, -2.0562e-01, -5.4814e-01,\n",
      "          3.5562e-01,  2.1417e-01,  4.8004e-01,  4.2103e-01, -1.7548e-01,\n",
      "          3.1937e-01,  2.2351e-01, -1.1909e-01, -3.2765e-01, -3.7718e-01,\n",
      "         -1.0321e-01,  3.1332e-02,  2.0111e-01, -4.1696e-01, -6.6398e-01,\n",
      "         -3.2483e-01, -6.8810e-01, -2.1710e-01, -2.4957e-01,  4.7042e-01,\n",
      "          4.8575e-01, -1.5676e-02, -2.2244e-02, -8.1419e-02, -8.9892e-01,\n",
      "         -4.6296e-01, -1.2383e-01, -7.4025e-02,  2.8955e-01,  1.1007e-02,\n",
      "         -4.0191e-01,  1.1976e-01,  1.4720e-01, -4.2985e-01, -2.9004e-01,\n",
      "         -4.7003e-01, -2.6746e-01, -3.3105e-01, -1.7307e-01, -1.9586e+00,\n",
      "         -2.5880e-01,  4.4471e-01, -1.9095e-01, -7.9033e-02, -5.8581e-02,\n",
      "         -1.1523e-01, -2.0332e-01, -4.2334e-02,  3.0949e-01, -3.0647e-01,\n",
      "          3.3878e-01,  6.5912e-01,  7.4400e-02, -2.0956e-01, -4.9200e-01,\n",
      "         -9.5774e-02,  4.1593e-01, -7.9059e-02, -1.0186e-01,  8.8482e-02,\n",
      "          5.5818e-04, -3.9378e-01,  2.5322e-01, -5.1421e-01, -1.1759e-01,\n",
      "          8.9655e-02, -3.0598e-01,  1.0874e+00,  5.2461e-01, -5.6581e-01,\n",
      "          1.2875e-01,  5.0730e-01, -2.4550e-01,  1.8530e-01,  3.3431e-01,\n",
      "          1.9295e-01,  7.7697e-02, -2.2703e-01,  5.9994e-02,  5.4118e-01,\n",
      "          3.7942e-01, -2.9355e-01, -1.1343e-01,  1.7482e-01,  1.5038e-01,\n",
      "         -1.6573e-01,  1.0113e-01,  5.4374e-01, -6.3759e-01,  4.2427e-01,\n",
      "          2.4141e-01,  1.0894e-01, -2.3763e-01,  1.6476e-01, -3.5813e-01,\n",
      "          8.6339e-02, -3.5885e-01,  9.7295e-01, -3.9957e-02, -2.6264e-01,\n",
      "         -3.3961e-01, -3.8119e-01,  3.6745e-01,  3.9774e-01,  7.3978e-02,\n",
      "         -6.7324e-01,  9.0891e-02,  2.8985e-01,  1.0607e-01,  6.4471e-01,\n",
      "         -1.6073e-01,  4.2881e-01, -2.6269e-01,  5.5153e-01, -2.9646e-01,\n",
      "         -3.6656e-01, -1.9331e-01, -6.8534e-02,  2.1628e-01,  6.2954e-01,\n",
      "          5.5651e-01, -1.2526e-01,  2.1225e-02, -1.4977e-02, -2.9338e-01,\n",
      "          3.2220e-01, -3.4610e-01,  2.2607e-01,  3.3248e-01,  8.9736e-02,\n",
      "         -5.9159e-01,  1.2703e-01, -3.5205e-01,  6.1397e-02, -8.5470e-03,\n",
      "         -1.4535e-01, -7.5053e-03, -6.0400e-02,  5.4708e-01,  3.9048e-02,\n",
      "         -1.8690e-02,  8.4039e-01, -7.0430e-02, -2.4938e-02, -8.5588e-02,\n",
      "         -1.2211e-01, -2.9544e-01,  2.8949e-01, -1.9311e-01,  4.4398e-01,\n",
      "         -1.4224e-01,  3.6475e-01,  3.6286e-01,  1.6868e-01, -2.3803e-01,\n",
      "         -7.5285e-01, -4.0192e-01, -9.1944e-02,  5.5140e-01, -3.9163e-01,\n",
      "          1.8772e-01,  2.1719e-02, -3.7656e-01,  6.6714e-01, -6.7193e-02,\n",
      "         -2.9740e-01,  4.4811e-02,  1.5280e-01,  6.1696e-01, -1.0518e-01,\n",
      "         -6.1810e-01,  7.1272e-01, -2.9115e-01,  3.1684e-01, -8.0702e-02,\n",
      "         -2.1481e-01,  5.4309e-02, -5.4913e-01, -6.5478e-01,  3.8710e-01,\n",
      "         -5.5065e-01,  1.6933e-01,  1.0752e+00, -7.8613e-02,  2.5588e-01,\n",
      "         -4.6330e-02,  4.1182e-01, -1.8678e-01,  3.1634e-01,  2.5685e-01,\n",
      "         -4.9481e-01, -1.2756e-01,  1.0771e-01, -8.0495e-02, -4.9192e-01,\n",
      "         -4.7410e-02,  1.7311e-01, -8.6745e-02, -1.7043e-01, -2.7635e-01,\n",
      "          2.4307e-01,  2.4686e-01,  1.3762e-01, -1.3134e+00, -2.0471e-01,\n",
      "         -3.3164e-01, -8.9794e-02,  2.8233e-01, -3.6041e-01, -1.7719e-01,\n",
      "          1.6569e-01,  3.4132e-02, -5.7582e-01,  5.8162e-02,  4.3560e-01,\n",
      "         -9.3783e-01, -2.3596e-01,  2.8372e-01, -2.8481e-01,  8.4541e-02,\n",
      "         -4.8126e-01,  5.0110e-01, -4.4225e-02, -1.5543e-02,  3.2432e-01,\n",
      "          2.4595e-01, -5.8359e-01, -4.2944e-01,  3.3780e-01, -5.4852e-01,\n",
      "          5.1725e-01,  4.7030e-01,  5.7244e-02, -1.1976e-01,  5.0635e-01,\n",
      "         -3.6174e-01, -1.2536e+00,  6.7856e-01, -1.1000e+00, -2.7925e-01,\n",
      "          3.4667e-01, -1.7322e-01, -4.7017e-02, -8.1931e-02, -2.9552e-01,\n",
      "         -9.4376e-02,  3.6435e-01,  1.9981e-02,  2.1618e-01,  2.1454e-01,\n",
      "         -2.7120e-01, -1.1111e-01, -5.0028e-01,  3.8323e-01,  5.3802e-03,\n",
      "          3.2316e-01,  5.4419e-01, -1.2639e-01,  1.4399e-01,  8.2517e-02,\n",
      "         -2.4322e-01,  2.5371e-02, -6.4997e-01, -1.6198e-01, -2.7123e-01,\n",
      "          3.7779e-01, -1.7822e-01,  3.9464e-01, -2.8889e-01, -3.4713e-01,\n",
      "         -5.2601e-01, -6.8494e-01,  4.9975e-01, -2.6613e-01,  1.6539e-02,\n",
      "         -1.3764e-02,  3.8261e-01, -4.3133e-01,  1.9695e-01, -1.6600e-01,\n",
      "         -5.4491e-01,  3.3143e-01,  1.1306e-01,  6.5965e-02, -1.3163e-01,\n",
      "         -4.6552e-01,  6.2857e-01, -3.0475e-01, -4.0430e-01, -1.8927e-01,\n",
      "          1.3104e-01, -2.9861e-01,  6.2646e-02, -5.3908e-01, -5.1569e-02,\n",
      "         -3.0269e-01,  4.0321e-01, -1.5315e-02, -3.1840e-01, -6.2706e-01,\n",
      "          6.8252e-01, -1.7461e-01,  1.9748e-01,  6.3749e-01, -6.2075e-01,\n",
      "          4.1257e-01,  3.3932e-01,  1.7536e-01,  4.9444e-02, -1.5134e-01,\n",
      "         -3.2965e-01,  2.3603e-01,  1.0358e+00, -9.9586e-02, -1.7926e-01,\n",
      "         -2.7783e-01,  3.2185e-01,  4.1179e-01,  7.8111e-02, -3.2427e-01,\n",
      "         -7.2540e-02,  1.9424e-01, -4.3739e-02,  2.3835e-02,  3.4985e-01,\n",
      "          9.8006e-02, -2.8793e-01, -1.3658e-01,  4.3277e-01,  6.6454e-02,\n",
      "         -6.3648e-01,  3.1264e-01,  6.8854e-01,  4.7145e-02, -2.2085e-01,\n",
      "          2.6956e-03, -1.0813e-01, -1.4670e-01,  5.2781e-01, -5.2767e-01,\n",
      "         -2.7020e-01, -6.0455e-02,  1.0102e+00,  4.5252e-01, -2.0769e-01,\n",
      "          8.0291e-03, -4.0466e-01, -6.6143e-01, -6.8180e-02,  3.2490e-01,\n",
      "         -1.8122e-02,  8.1541e-01,  3.4337e-01, -3.1990e-01,  4.0127e-01,\n",
      "         -1.8684e-01, -4.0245e-02,  2.7071e-01,  3.0177e-02, -6.1492e-02,\n",
      "         -1.3461e-01, -4.5620e-02,  4.0520e-01, -3.0534e-01, -2.0777e-01,\n",
      "          2.5514e-01, -6.1243e-01,  1.3819e-01, -3.5828e-01, -1.4476e-01,\n",
      "         -2.4886e-01,  1.8027e-01, -1.3031e-01, -2.3146e-01, -7.0087e-01,\n",
      "         -3.5821e-01,  6.8824e-01, -2.7592e-01, -3.0001e-01,  1.7304e-01,\n",
      "         -5.7713e-01,  7.5743e-01, -2.8268e-01, -2.9544e-01, -4.1847e-01,\n",
      "          3.5769e-01,  6.2564e-02, -1.1633e-01,  3.3574e-01, -2.2869e-01,\n",
      "          4.5180e-01, -2.9618e-01, -2.7375e-01,  5.9838e-02,  3.0771e-01,\n",
      "          5.9119e-01, -6.2793e-01,  5.4836e-01, -2.2912e-01,  1.8745e-01,\n",
      "         -5.7808e-01, -2.0765e-01,  9.2507e-03, -4.1895e-01, -6.6991e-02,\n",
      "          5.7709e-01, -7.1626e-02, -1.4641e-01, -1.2491e-01, -2.6299e-02,\n",
      "          8.1796e-01, -1.9317e-01,  6.5566e-02, -2.3163e-01, -1.9789e-02,\n",
      "          2.0288e-02,  2.2409e-02, -1.9727e-01, -7.4088e-01, -6.5917e-01,\n",
      "          7.6032e+00,  1.8177e-01, -4.3689e-01,  7.7148e-02,  2.0162e-01,\n",
      "         -1.0257e-01,  1.1655e-01, -1.2112e-01, -2.8031e-01,  1.6495e-01,\n",
      "         -2.4357e-01, -6.0403e-02,  3.0890e-03,  1.8939e-01,  5.2821e-01,\n",
      "         -4.7585e-01, -2.3124e-01,  2.4768e-01, -1.6602e-01, -1.0169e-01,\n",
      "          9.9908e-03, -6.9400e-01, -3.7497e-01, -5.4236e-01,  1.0447e+00,\n",
      "         -5.7880e-01, -1.0721e+00, -3.5931e-01, -1.4064e-01, -1.5568e-01,\n",
      "          3.3276e-01, -1.3761e-01, -2.5409e-01, -1.3480e-01,  1.0515e-01,\n",
      "         -2.9298e-01,  4.4440e-02,  7.4073e-02,  3.1937e-01, -3.8497e-01,\n",
      "          7.1253e-01, -1.7707e-03,  9.5287e-02,  5.5151e-01,  3.2253e-02,\n",
      "          1.0730e-02, -1.4733e-01,  4.2646e-01,  1.2140e-01, -1.2114e-01,\n",
      "         -6.5505e-01,  1.6239e-01,  2.0638e-01, -3.3459e-01,  6.9310e-01,\n",
      "         -1.6918e-01, -2.6885e-01, -5.0440e-01, -3.3908e-01, -3.9186e-01,\n",
      "         -2.1487e-01,  3.0352e+00,  1.3345e-01,  3.8632e-01, -3.2434e-01,\n",
      "          4.3457e-01, -1.6121e-01, -2.3445e-01,  1.9966e-01, -4.9266e-01,\n",
      "          1.7306e-01,  5.5170e-01, -1.0517e-01]], grad_fn=<MeanBackward1>)\n",
      "tensor([[ 4.6714e-01,  4.9302e-01,  8.7512e-02,  4.4247e-01,  5.4313e-01,\n",
      "         -1.6208e-01,  1.6063e-02,  4.0917e-01, -2.6003e-01,  4.7895e-01,\n",
      "         -1.7406e-01, -1.5704e-01,  1.7874e-01, -4.3664e-02,  5.7882e-01,\n",
      "          2.0411e-01,  4.9163e-01, -3.7316e-02,  1.8408e-01,  2.4541e-01,\n",
      "          1.1434e-01,  3.4647e-01,  1.2301e-01, -2.0748e-01,  3.4842e-02,\n",
      "         -4.5314e-01, -3.6447e-01, -6.9895e-01,  4.7206e-01, -5.5091e-01,\n",
      "         -4.5991e-01, -2.3810e-01, -4.3129e-01, -2.7088e-01,  3.7612e-03,\n",
      "         -1.8116e-01,  1.7643e-01, -2.3018e-01, -5.6751e-01, -1.8540e-02,\n",
      "          1.6405e-01, -2.6306e-02, -5.6462e-01,  1.6273e-01,  2.1064e-03,\n",
      "          3.3457e-01, -3.7606e-01,  5.3820e-02,  2.7755e-01,  2.9374e-01,\n",
      "          8.6589e-02,  9.5712e+00, -2.3021e-01, -3.2212e-01, -4.9026e-01,\n",
      "          2.3618e-01,  2.5066e-01, -2.0681e-01, -7.5005e-02, -1.8098e-01,\n",
      "         -2.1282e-01,  5.9512e-01, -4.8098e-01, -3.4310e-01, -2.1631e-01,\n",
      "         -8.8570e-02, -1.2951e-01,  2.5260e-01, -8.9384e-01, -3.5558e-01,\n",
      "          1.0456e-02, -8.8754e-01,  4.6609e-01, -3.0851e-02,  2.8861e-01,\n",
      "          6.3405e-01,  3.0515e-01, -2.4399e-01, -3.8096e-01, -1.3397e-01,\n",
      "         -5.5347e-01, -4.5843e-02, -4.0459e-01, -1.0529e-01, -6.6371e-01,\n",
      "          3.5354e-01, -4.1304e-02, -3.0124e-01,  1.0407e+00, -6.3688e-01,\n",
      "         -1.6865e-01,  3.5919e-01, -3.6560e-02,  8.2470e-02,  3.3425e-01,\n",
      "         -1.1971e-01, -3.2604e-01, -3.1864e-01, -5.6650e-01, -2.8498e-01,\n",
      "         -1.8509e-01, -3.1114e-02, -1.1319e-02, -5.7022e-01, -6.8463e-01,\n",
      "          2.5928e-01, -2.3420e-01,  3.2888e-02, -3.2928e-01,  2.1833e-01,\n",
      "         -1.1594e-01, -3.6287e-01, -3.1742e-01,  3.3213e-02, -7.0347e-01,\n",
      "         -2.7655e-01,  2.4524e-01,  6.3956e-01, -2.5146e-02, -3.9045e-01,\n",
      "          1.7425e-01,  4.8608e-01, -1.1015e-01,  1.3092e-01,  2.5249e-01,\n",
      "         -1.1298e+00, -2.7483e-01, -2.8088e-01, -4.1318e-01,  3.8675e-01,\n",
      "          3.5593e-01, -4.9813e-02,  8.1125e-01, -3.8789e-01,  1.6395e-01,\n",
      "         -7.2578e-01,  4.4490e-01, -3.0054e-01, -2.7395e-01,  1.1455e-01,\n",
      "         -2.6919e-01, -2.0063e-01,  5.4981e-02,  1.3401e-01, -3.1560e-02,\n",
      "         -1.0077e-01,  1.3484e-01,  4.4324e-02,  6.4369e-01,  2.4685e-01,\n",
      "         -8.3380e-02, -7.5670e-01, -5.2379e-02, -4.2055e-01, -1.4241e-01,\n",
      "          4.0272e-01, -3.4601e-02, -6.0127e-01,  3.1225e-02, -2.1355e-01,\n",
      "          1.5073e-02,  4.5584e-01, -1.9665e-01,  6.0149e-01,  2.3903e-01,\n",
      "         -5.0369e-01, -1.5946e-02,  2.2663e-01,  2.9975e-01, -1.3968e-01,\n",
      "          2.9652e-02,  5.3637e-01, -1.6667e-01,  6.3336e-01, -4.7931e-01,\n",
      "          2.3062e-01, -4.9308e-01, -3.2568e-01, -3.0603e-01, -2.8043e-02,\n",
      "          1.2371e-01, -2.6033e-01,  1.5621e-01, -2.5187e-01,  5.1011e-01,\n",
      "          1.7320e-02,  4.2913e-02, -3.1490e-01, -1.7280e-01, -9.9340e-03,\n",
      "         -4.4193e-01, -1.2360e-01,  8.9021e-03,  1.3420e-03,  4.8563e-01,\n",
      "          2.0099e-01, -1.4306e-01, -7.1616e-02,  1.3116e-01,  1.4182e-01,\n",
      "         -6.4538e-01,  1.4041e-01, -2.4713e-01,  3.0481e-01,  1.5691e-01,\n",
      "          2.9829e-01,  1.5968e-01, -1.7773e-01,  8.8537e-02, -8.6721e-01,\n",
      "         -7.5454e-01,  5.0038e-02,  6.6398e-02, -2.6861e-01,  2.1392e-01,\n",
      "         -6.2884e-01,  3.3304e-01,  6.4511e-01,  1.8573e-01,  2.0617e-01,\n",
      "          1.2973e-01, -7.7252e-02, -1.1775e-01,  8.6968e-03, -2.4249e-01,\n",
      "         -1.1479e-01, -1.5024e-01, -6.4293e-03,  2.3211e-01, -9.1327e-03,\n",
      "          5.1355e-02, -1.4251e-01, -1.2258e-01,  1.7133e-01,  2.0914e-02,\n",
      "         -5.2929e-01,  6.0870e-02,  2.1995e-01, -4.4953e-01,  3.3415e-01,\n",
      "         -5.6852e-02, -1.7912e-01, -3.9854e-02, -4.2824e-01, -3.8714e-01,\n",
      "         -2.0555e-01,  7.5794e-02,  2.1048e-01, -3.9424e-01,  3.0243e-01,\n",
      "         -7.3008e-02, -1.7175e-01, -4.4153e-01, -2.5057e-01, -4.2919e-01,\n",
      "          5.0850e-01, -3.2681e-01,  3.9260e-01, -3.3353e-01,  1.3534e-01,\n",
      "         -2.2791e-01,  3.3372e-01,  5.1270e-01, -3.8731e-01,  1.8000e+00,\n",
      "         -4.7055e-01,  1.2358e-01,  1.3380e-01,  3.8005e-01, -3.9739e-01,\n",
      "         -8.0993e-01,  3.2255e-01, -3.3056e-01, -2.6413e-01, -3.6703e-01,\n",
      "         -1.2973e-01,  5.2005e-01,  5.9990e-02,  4.6718e-01, -4.0566e-02,\n",
      "          2.5729e-01,  3.7832e-01, -3.2982e-01, -1.5629e-01, -2.2119e-01,\n",
      "         -4.7173e-01, -5.9634e-02,  2.1584e-01, -2.2988e-01, -8.2326e-01,\n",
      "         -2.8226e-01, -7.1994e-01,  6.1907e-02,  2.1843e-01,  9.2971e-01,\n",
      "          7.4180e-01,  8.9862e-02, -2.4267e-01,  4.5873e-02, -3.4507e-01,\n",
      "         -7.5484e-01, -2.7364e-01,  1.2280e-01, -5.8093e-02,  2.5848e-01,\n",
      "          1.8521e-01,  1.3035e-02,  7.2627e-02, -1.7879e-01, -2.9835e-01,\n",
      "         -4.0321e-02, -1.4783e-01,  2.8237e-01,  8.7061e-02, -1.3072e+00,\n",
      "         -3.6420e-01,  4.3921e-01, -7.1956e-01, -2.2211e-02,  6.0330e-02,\n",
      "         -5.9913e-02,  6.7387e-02,  3.5874e-01,  3.2649e-01,  2.0929e-02,\n",
      "          1.4709e-01,  1.3678e-01, -1.5532e-01, -4.8179e-02, -3.9866e-01,\n",
      "          7.1908e-03, -1.8318e-02,  2.3363e-01,  3.3287e-01, -3.2873e-01,\n",
      "          1.8389e-04, -3.1133e-01,  8.6159e-02, -1.1259e-01, -4.2242e-01,\n",
      "          3.5965e-01, -2.5935e-01,  8.8154e-01,  2.6669e-01, -8.8477e-01,\n",
      "         -7.1967e-03,  6.1195e-02, -6.0432e-01, -2.8498e-01,  1.6692e-01,\n",
      "         -5.0707e-02,  3.6645e-01, -2.2168e-01,  5.2016e-02, -2.2447e-01,\n",
      "          6.0556e-01, -2.5332e-01, -6.4852e-02,  1.7318e-01, -1.0558e-02,\n",
      "         -2.3016e-01,  3.3611e-01,  4.2104e-01, -4.3967e-01,  3.1353e-01,\n",
      "          6.1301e-02, -2.0253e-01, -1.1032e-01,  1.4930e-01, -2.5228e-01,\n",
      "          1.6358e-02, -2.9243e-01,  6.5861e-01, -5.3067e-01, -1.2250e-01,\n",
      "         -3.8121e-02, -2.4967e-01,  3.8282e-01,  3.7013e-01, -1.2246e-01,\n",
      "         -4.3231e-01, -3.4591e-01,  7.8741e-02, -5.8890e-01,  4.8331e-01,\n",
      "         -2.7929e-01,  4.8491e-01,  1.0069e-01,  1.0041e-01, -1.8606e-01,\n",
      "         -3.5397e-01,  6.0801e-02,  5.2685e-02, -3.4625e-01,  6.3587e-01,\n",
      "          4.3762e-01,  2.7804e-01, -5.1000e-01, -4.4068e-01, -3.0394e-01,\n",
      "         -1.6303e-01,  7.2937e-02,  2.1888e-01,  3.7042e-01, -1.5385e-02,\n",
      "         -9.4962e-02,  1.8430e-01,  4.4085e-01,  4.4595e-01,  6.5138e-02,\n",
      "         -8.4904e-02,  4.9715e-01, -9.7921e-02, -1.3406e-01, -3.7523e-01,\n",
      "         -4.3906e-02,  7.6728e-01, -6.6923e-03, -1.6762e-01, -2.4615e-01,\n",
      "         -4.3036e-01, -2.5246e-01,  1.8509e-01,  2.5066e-02,  4.9125e-01,\n",
      "          1.8762e-01,  4.6247e-01,  9.5367e-02,  3.4302e-01, -1.4661e-01,\n",
      "         -2.5274e-01, -4.3223e-01, -3.8272e-01,  2.2461e-01, -3.9252e-02,\n",
      "         -2.2185e-01, -1.8306e-01, -5.9939e-01,  2.9784e-01,  5.4306e-01,\n",
      "         -3.3541e-01, -2.9613e-01, -6.9317e-02,  3.3612e-01, -2.0877e-01,\n",
      "          4.1392e-02,  4.2435e-01, -4.0137e-01,  1.9773e-01, -3.1350e-03,\n",
      "          2.5414e-01, -3.0976e-02, -3.3887e-01, -3.7641e-01,  2.8807e-01,\n",
      "         -4.0806e-01,  4.8822e-02,  5.6271e-01,  2.1755e-01,  1.7903e-01,\n",
      "          7.4602e-03,  4.9760e-01, -6.5296e-02,  2.7485e-01,  5.1066e-01,\n",
      "          5.0597e-02, -6.4362e-01,  2.5150e-01, -4.3308e-01, -5.0865e-01,\n",
      "          1.8411e-02, -1.2118e-01,  1.0846e-01, -2.2527e-01, -2.0220e-01,\n",
      "          3.8459e-01,  2.8769e-01, -5.5038e-03, -1.1134e+00, -7.8475e-01,\n",
      "          8.2207e-02, -2.9375e-01,  5.4639e-01, -1.3699e-01,  1.8864e-01,\n",
      "          2.8728e-02,  3.7593e-01, -2.1033e-01, -2.4092e-01,  3.5416e-02,\n",
      "         -5.8670e-01, -6.7257e-01,  3.4108e-01,  1.0249e-01,  5.5955e-01,\n",
      "         -7.5937e-01,  1.2286e-01, -1.4398e-02,  5.8570e-02, -1.0439e-01,\n",
      "          8.5331e-01, -5.6308e-01,  3.3240e-01,  4.3889e-01, -5.8244e-02,\n",
      "         -1.0634e-01,  4.0070e-01, -4.3713e-01, -1.5150e-01,  3.8935e-01,\n",
      "          1.6378e-01, -1.1034e+00,  6.2503e-01, -5.2994e-01,  2.3581e-02,\n",
      "          2.6945e-01, -3.3094e-01,  2.1689e-01, -3.6105e-02, -1.2429e-01,\n",
      "         -1.9301e-01,  1.3058e-01,  4.1661e-01, -3.1452e-01,  2.7032e-01,\n",
      "         -5.8513e-02, -2.4264e-01, -6.0745e-01,  5.0844e-01,  7.8694e-01,\n",
      "          9.1230e-02,  5.4611e-01, -4.0747e-01, -1.8675e-01,  4.1293e-01,\n",
      "          1.1592e-01,  2.6817e-01, -1.1900e+00, -1.9198e-01, -5.0730e-01,\n",
      "          2.5771e-02, -5.6678e-01,  1.3367e-01,  4.2205e-01, -2.9425e-01,\n",
      "         -3.3755e-01, -7.3469e-01,  9.6020e-01, -8.3758e-01, -4.3849e-01,\n",
      "          1.1211e-01,  3.9370e-01,  1.5460e-01,  2.6785e-01, -2.3327e-01,\n",
      "         -5.5519e-01,  1.6735e-01, -1.0325e-01, -2.8766e-02, -1.0163e-01,\n",
      "         -1.3120e-01,  3.1552e-01,  1.8543e-01, -7.0930e-01, -4.4156e-02,\n",
      "          3.7994e-01,  2.3489e-01, -3.7638e-01, -5.9493e-01, -7.0892e-02,\n",
      "         -2.3411e-01,  2.1987e-01, -1.6499e-01, -2.0388e-01, -3.8179e-01,\n",
      "         -8.1720e-02, -8.2428e-02,  1.8214e-02,  4.3888e-01, -1.1276e-01,\n",
      "          3.6915e-01,  3.2648e-01,  2.3427e-01, -3.5633e-01,  2.6490e-01,\n",
      "         -6.7868e-01,  3.3622e-01,  7.0874e-01,  4.7984e-02, -3.6388e-02,\n",
      "         -1.2353e-01,  1.4547e-01,  4.0767e-01,  2.5807e-01, -1.9107e-01,\n",
      "          3.5201e-01,  4.1393e-01,  1.3262e-02, -1.8031e-01,  3.4137e-01,\n",
      "          1.8421e-01,  2.8677e-01, -5.9485e-01,  3.0803e-01,  4.6897e-02,\n",
      "         -6.2791e-01,  4.0246e-01,  9.0076e-01,  2.3388e-01, -1.4427e-01,\n",
      "          1.4160e-01, -2.9825e-01,  1.4208e-01,  1.3134e-01, -1.7664e-01,\n",
      "          3.9714e-01, -1.0584e-01,  4.4137e-01,  6.0710e-01, -8.3775e-02,\n",
      "         -2.8971e-01, -4.5569e-01,  5.8935e-02,  1.1065e-01,  2.6061e-01,\n",
      "         -1.8962e-02,  8.0223e-01, -1.1608e-01,  4.0675e-01,  2.5596e-01,\n",
      "         -3.4300e-01,  8.8866e-02, -1.8050e-02,  1.9935e-02,  2.7102e-01,\n",
      "          1.2694e-01, -2.1683e-02,  2.1454e-01, -3.3723e-01, -1.7876e-01,\n",
      "          5.0444e-03,  1.0368e-01,  3.4847e-01, -1.6954e-01, -5.1897e-01,\n",
      "         -2.8588e-01,  4.5712e-01, -3.4734e-01, -2.1936e-01, -5.6186e-01,\n",
      "         -1.5458e-01,  1.1329e-02,  1.7770e-02, -2.6046e-03,  1.2070e-01,\n",
      "         -3.5890e-01,  6.7412e-01, -1.4069e-01, -4.1435e-01, -6.0374e-01,\n",
      "          5.8903e-02, -1.3532e-02,  2.3016e-01, -2.0259e-01, -4.1832e-01,\n",
      "         -3.6662e-01, -3.7059e-01, -3.9006e-01, -3.9861e-01,  1.8902e-01,\n",
      "          3.2698e-01, -5.1911e-01,  7.2763e-01, -9.4492e-01,  8.4824e-02,\n",
      "         -5.8962e-01, -5.1131e-02,  8.2347e-02, -2.7459e-01, -9.1817e-02,\n",
      "          5.5777e-01, -2.1668e-01, -3.9496e-01, -2.8221e-01,  2.9278e-01,\n",
      "          5.1663e-01, -1.9814e-01,  1.5205e-01, -7.8650e-02,  1.6837e-02,\n",
      "          6.6647e-02,  1.5439e-01,  2.3340e-01, -4.6395e-01, -5.6245e-01,\n",
      "          7.6955e+00,  2.6638e-01, -3.4246e-01,  1.5695e-02,  1.8126e-01,\n",
      "         -4.0084e-01,  1.7625e-01,  3.4879e-01, -6.4329e-02, -5.2683e-01,\n",
      "         -5.8666e-01,  1.2474e-01,  2.4507e-01,  5.6566e-02,  5.0108e-01,\n",
      "         -1.3559e-01,  8.4572e-02, -8.4923e-02, -3.6248e-01,  1.8335e-01,\n",
      "          4.8395e-01, -1.1310e+00, -6.3949e-01, -3.3984e-01,  6.6177e-01,\n",
      "         -5.0560e-01, -7.0087e-01, -3.8705e-01, -9.8171e-02, -5.8507e-01,\n",
      "          2.3993e-01, -6.4960e-01,  1.1946e-01,  4.1357e-01, -1.6475e-01,\n",
      "         -1.3050e-01, -4.0965e-01,  3.9136e-01,  8.2792e-02, -4.9738e-01,\n",
      "          1.0299e+00, -1.7561e-01, -7.7502e-02,  4.6298e-01,  1.3858e-01,\n",
      "          4.6252e-01, -4.1434e-01,  5.8879e-01,  6.5312e-01, -5.7066e-01,\n",
      "         -4.3265e-01,  1.5708e-01,  1.2383e-01, -3.0101e-01,  1.6783e+00,\n",
      "          4.7920e-03, -1.0218e-01, -3.3075e-01, -3.3949e-01, -7.0523e-01,\n",
      "         -2.9111e-01,  2.9200e+00,  3.5748e-01, -8.9715e-02, -3.8456e-01,\n",
      "          4.4976e-01, -1.1971e-01, -5.8245e-02, -6.8561e-03, -1.6700e-01,\n",
      "          2.3903e-01,  1.4458e-01, -1.0363e-01]], grad_fn=<MeanBackward1>)\n",
      "tensor([[ 7.6575e-01, -2.0199e-01,  6.9346e-03,  1.8261e-01,  1.6704e-01,\n",
      "          1.1436e-01, -1.2050e-01,  2.5172e-01, -6.9379e-02,  2.2404e-01,\n",
      "         -1.0887e-01, -3.9231e-01, -1.3981e-01,  6.0878e-02,  6.1140e-02,\n",
      "          3.9591e-02,  4.1496e-01, -5.0670e-02,  5.4807e-01, -9.9523e-02,\n",
      "          3.8131e-01,  6.3714e-01,  7.3835e-02, -2.3930e-01, -1.3664e-01,\n",
      "         -5.2908e-01, -3.9060e-02, -6.2478e-01, -1.0926e-02, -8.8058e-01,\n",
      "         -3.0289e-01, -7.1982e-01, -2.4216e-01, -2.0779e-01, -4.5800e-02,\n",
      "         -3.6832e-01, -3.4393e-01, -3.9740e-01, -4.3535e-01, -5.0143e-02,\n",
      "          2.1612e-01, -4.4779e-01, -3.5662e-01, -4.1607e-02, -1.3664e-01,\n",
      "          3.5968e-01, -2.7548e-01, -3.9043e-01, -1.2081e-01,  6.2398e-01,\n",
      "         -3.7063e-01,  9.3812e+00, -1.3039e-01, -3.5357e-01, -4.5877e-01,\n",
      "         -1.1625e-01,  2.6275e-01, -2.5778e-02, -6.1222e-02, -6.0740e-01,\n",
      "         -5.8996e-01,  8.5031e-02, -3.1941e-01, -7.2179e-02, -9.5825e-02,\n",
      "          8.7608e-02,  1.5214e-01,  1.7468e-01,  3.0312e-01, -2.0651e-01,\n",
      "          1.6724e-01, -4.9729e-01,  3.0679e-01,  1.6495e-01,  2.9821e-01,\n",
      "          8.5999e-01,  5.7084e-01, -1.9589e-01, -2.1767e-01, -2.6779e-01,\n",
      "         -4.4172e-01, -2.4635e-01, -3.3200e-01, -1.3466e-01, -6.6359e-01,\n",
      "         -4.2910e-03, -3.8194e-01, -2.4120e-01,  5.6430e-01, -5.8882e-01,\n",
      "          4.6409e-02, -4.0391e-02,  1.5758e-03, -5.1663e-01,  4.4462e-01,\n",
      "         -1.4237e-01, -6.9861e-01, -1.5797e-01, -1.8324e-01,  6.0006e-02,\n",
      "         -8.9908e-01, -2.2442e-02, -1.6037e-01,  2.0755e-01, -5.0583e-01,\n",
      "         -3.0303e-02, -6.4359e-01,  5.9977e-01, -3.0965e-01,  2.0731e-02,\n",
      "          8.9426e-02, -7.0547e-01, -7.1687e-01,  1.5353e-01, -4.3158e-02,\n",
      "         -2.3443e-01, -1.0191e-01,  7.2712e-01,  6.7417e-01, -1.6404e-01,\n",
      "          4.2909e-01,  1.8856e-01,  1.2628e-01,  6.6300e-02,  9.0034e-02,\n",
      "         -7.0286e-01,  2.8463e-01, -3.4851e-01, -7.4298e-02,  7.5636e-02,\n",
      "          4.4978e-01, -3.2120e-01,  4.4312e-01, -4.2325e-03,  1.4604e-02,\n",
      "         -2.6304e-01,  3.2827e-01, -6.4413e-02, -5.7568e-01, -3.1101e-03,\n",
      "         -1.7588e-01, -2.9311e-01,  7.5753e-02,  2.8968e-01,  3.0333e-01,\n",
      "          7.4252e-01,  1.9947e-01, -4.1506e-01,  3.4865e-01, -2.5545e-02,\n",
      "         -1.0145e-01, -8.7161e-01, -4.2318e-02, -4.1756e-01,  1.5743e-02,\n",
      "         -1.0823e-01,  6.2883e-01, -3.4592e-01,  3.5733e-01, -3.3980e-01,\n",
      "          1.7279e-02,  1.8287e-01, -2.3348e-01,  5.4392e-01,  3.1636e-01,\n",
      "         -3.4752e-01,  6.6921e-02,  8.5947e-02, -2.6426e-01, -8.1285e-02,\n",
      "         -2.1349e-01,  6.4774e-01,  1.6488e-01,  2.8408e-01, -3.8786e-01,\n",
      "         -9.2933e-02, -5.6440e-01, -1.5192e-01, -3.6775e-01, -9.2126e-02,\n",
      "         -2.5784e-01, -1.2313e-03,  4.2071e-02, -5.2875e-04,  6.0015e-01,\n",
      "          5.9080e-01, -3.4322e-02, -9.2472e-03, -1.0975e-01,  1.2207e-01,\n",
      "         -2.8277e-01, -3.6790e-01, -2.3359e-01, -4.5360e-01,  3.7428e-01,\n",
      "          3.0080e-01, -4.6584e-02, -1.6973e-01,  3.7053e-01,  1.9235e-01,\n",
      "         -4.4908e-01, -2.3606e-01, -4.1347e-01,  2.1598e-01,  6.0275e-02,\n",
      "          6.5827e-02,  3.2956e-01,  2.1754e-01, -3.5381e-01, -3.3363e-01,\n",
      "         -3.5803e-01, -6.3397e-02, -2.4754e-01, -5.6602e-02, -1.6295e-01,\n",
      "         -1.2870e-01,  5.0128e-01,  1.7418e-01,  2.6458e-01,  1.3092e-01,\n",
      "          4.4234e-02, -2.0601e-01,  1.6606e-01, -2.9745e-01,  3.6068e-02,\n",
      "         -7.9354e-03, -7.0032e-02,  7.4456e-02,  2.2131e-01, -4.0386e-01,\n",
      "          2.6065e-01, -1.1140e-01,  1.1395e-01,  1.3020e-01,  7.8425e-02,\n",
      "          1.7018e-01, -7.5551e-02, -1.0129e-01, -7.5422e-01, -1.9133e-01,\n",
      "         -3.1459e-01, -1.1355e-01, -2.3779e-01,  2.9456e-02, -6.7720e-03,\n",
      "          9.3192e-02,  6.8682e-02,  1.5544e-01, -2.7404e-01, -2.1183e-02,\n",
      "          3.9112e-01, -4.5061e-01,  2.4941e-01, -3.5506e-01,  1.9801e-01,\n",
      "          3.2441e-01,  1.2071e-01, -2.5933e-01, -1.3803e-01,  5.9295e-01,\n",
      "         -2.7211e-01,  4.4021e-01,  7.0483e-01, -5.0175e-01,  1.4746e+00,\n",
      "          1.4161e-01,  1.3942e-01, -3.9909e-01,  6.9825e-02, -2.2882e-01,\n",
      "         -3.7085e-01,  9.6863e-02,  9.5281e-02, -5.2887e-01, -2.0889e-01,\n",
      "          1.2324e-01,  3.3487e-01,  1.1095e-01,  5.8311e-01, -4.4145e-01,\n",
      "          3.2202e-01,  1.1256e-01, -5.8936e-02, -9.3221e-02, -8.6171e-01,\n",
      "         -2.0087e-01,  3.2546e-01,  2.4392e-01,  3.3365e-02, -5.6668e-01,\n",
      "          1.7032e-02, -3.7688e-01,  8.0370e-02, -2.4932e-02,  4.7648e-01,\n",
      "          5.7677e-01,  3.8213e-01, -4.4292e-01, -3.8565e-01, -4.6972e-01,\n",
      "         -3.0441e-01, -5.1474e-01, -1.8597e-01, -8.6200e-02,  2.3211e-01,\n",
      "          6.7979e-02,  3.4527e-01, -1.7180e-01,  2.1925e-01, -3.1798e-01,\n",
      "         -9.3631e-02, -1.6039e-02, -1.1135e-01, -1.7189e-01, -1.2617e+00,\n",
      "          1.0450e-01,  1.1411e-02,  1.8853e-01,  1.3925e-01, -2.8581e-01,\n",
      "         -1.5604e-01,  4.8125e-01,  2.8202e-01,  2.3267e-01, -2.2168e-01,\n",
      "          8.6101e-02,  5.3741e-01, -4.4237e-02, -9.2872e-02, -1.6415e-01,\n",
      "         -1.9339e-01, -6.7238e-02,  1.3628e-01, -7.3564e-03, -1.0043e-02,\n",
      "          3.3948e-01,  2.6032e-01, -2.6610e-01, -4.6665e-01, -4.2950e-01,\n",
      "         -2.2697e-02, -1.9524e-01,  6.7468e-01,  4.2843e-01, -3.0001e-01,\n",
      "          1.6882e-02, -3.2051e-02, -4.7954e-01,  4.4906e-01,  1.1762e-01,\n",
      "          5.4449e-03,  4.2936e-01, -6.1572e-01, -7.6906e-02,  2.5742e-01,\n",
      "          6.3539e-02, -2.7201e-01, -3.3160e-02,  2.3227e-03,  5.6515e-01,\n",
      "         -2.2247e-01,  1.1069e-01,  8.3812e-02, -6.0032e-01,  2.3152e-01,\n",
      "          1.7646e-01,  6.2731e-02, -3.0199e-02,  1.3078e-01, -3.3358e-01,\n",
      "          3.4554e-01,  2.0602e-01,  1.0188e-01, -1.8408e-01, -2.8520e-02,\n",
      "         -4.1761e-02,  8.8931e-02,  3.3934e-01,  4.3141e-01, -1.6416e-02,\n",
      "         -8.0781e-01,  1.4803e-01,  1.9950e-01, -3.1277e-01,  2.5727e-01,\n",
      "         -2.7899e-02, -5.1434e-01, -9.1630e-02,  2.7182e-01,  7.2700e-03,\n",
      "         -7.5497e-01, -1.6274e-01,  1.6881e-01, -3.6371e-01,  4.2446e-01,\n",
      "          9.8082e-01,  6.1202e-02, -1.5423e-01, -1.9709e-01,  5.0010e-02,\n",
      "         -1.0275e-01, -6.0723e-02,  4.6160e-01,  3.7148e-01, -2.2329e-02,\n",
      "         -3.0612e-01, -9.1445e-02,  2.2906e-01,  9.4235e-02,  2.3473e-01,\n",
      "         -4.5956e-02,  4.7648e-01, -4.2502e-01,  1.2035e-01, -1.9953e-01,\n",
      "          3.8421e-01,  7.5821e-01,  7.1065e-02,  1.9562e-01,  1.5788e-01,\n",
      "         -3.8843e-01, -2.1874e-01,  1.4328e-01,  4.0154e-02,  6.0415e-02,\n",
      "          1.1731e-01,  1.2303e-01,  8.4802e-02,  6.4226e-01, -3.9475e-01,\n",
      "         -1.3728e-01,  3.8411e-02,  1.4067e-02,  9.2278e-01, -2.0506e-01,\n",
      "          1.8342e-01, -1.3470e-01, -1.4862e-01,  2.5385e-01, -2.3690e-02,\n",
      "          9.1659e-02, -6.5151e-02, -3.1460e-02,  3.5988e-01, -1.1978e-01,\n",
      "         -7.5456e-02,  3.2862e-01, -7.6379e-01,  3.8437e-01, -6.3028e-01,\n",
      "         -1.1529e-01, -3.1262e-01, -1.2366e-02, -1.8532e-01,  5.5461e-02,\n",
      "         -5.7698e-01,  4.2851e-01,  6.0317e-01,  1.1361e-01,  4.4668e-02,\n",
      "          2.7755e-01,  2.0692e-01, -1.4990e-01, -1.3475e-01, -5.7126e-02,\n",
      "         -5.3125e-01, -3.6085e-03,  3.3269e-01, -2.7577e-01, -6.3697e-01,\n",
      "         -1.2671e-01,  5.6869e-01, -7.5194e-03,  1.3540e-01, -1.5701e-01,\n",
      "          5.7336e-01, -8.1983e-02,  8.1977e-02, -1.7443e+00, -4.7749e-01,\n",
      "          1.4448e-01, -4.3304e-01,  3.3838e-01, -2.1295e-02, -2.3963e-01,\n",
      "         -1.4543e-01,  1.1353e-01, -7.5058e-01,  5.1484e-01, -2.1466e-01,\n",
      "         -5.3187e-01, -2.6462e-01,  3.8940e-01, -9.6504e-02,  5.5996e-02,\n",
      "          1.0257e-01, -8.2302e-02, -2.8017e-01, -1.0093e-01, -3.4996e-02,\n",
      "          4.7250e-01, -9.9892e-01, -7.8350e-02,  3.0256e-01, -3.4890e-01,\n",
      "         -2.0455e-01,  3.6536e-01, -3.1367e-01, -4.2550e-01,  5.4277e-01,\n",
      "         -2.4438e-01, -1.0155e+00,  6.0782e-01, -3.2056e-01,  4.3231e-01,\n",
      "          2.5879e-01,  8.3425e-02,  5.3263e-02,  2.7818e-01,  2.4646e-01,\n",
      "          2.1255e-01,  1.3460e-01,  3.9132e-01,  1.1204e-01, -2.6111e-01,\n",
      "         -1.4436e-01, -2.5211e-01, -1.9912e-01, -4.6092e-03,  4.5502e-01,\n",
      "          1.8075e-02,  4.8380e-01, -5.0272e-01, -6.3713e-01,  3.1123e-01,\n",
      "         -2.7404e-02,  2.6682e-01, -3.6358e-01, -2.9133e-02,  6.7108e-02,\n",
      "         -2.2409e-01, -7.1691e-01,  4.0246e-01, -5.8556e-01,  1.0890e-01,\n",
      "         -8.8188e-01, -3.1950e-01,  2.9326e-04, -8.4791e-02, -1.7450e-01,\n",
      "          3.8212e-02, -4.5715e-02,  1.7813e-01, -3.1972e-01,  3.5323e-01,\n",
      "         -3.3119e-01,  2.5907e-01,  1.3238e-01, -6.9646e-02,  1.2259e-01,\n",
      "         -1.0976e-01, -2.7849e-01,  1.1948e-02, -2.8536e-01, -9.5447e-02,\n",
      "          3.6469e-01, -5.4906e-02,  1.1721e-01, -5.9460e-01,  4.3324e-01,\n",
      "         -1.2329e-01,  1.6278e-02, -1.7114e-01, -2.1927e-01, -1.9280e-01,\n",
      "          4.8944e-02, -3.9236e-02, -2.5083e-01,  5.5574e-01, -2.0477e-01,\n",
      "          7.7418e-01,  5.1768e-01, -1.6178e-01,  2.8444e-02, -7.8250e-01,\n",
      "         -1.5755e-01,  6.5176e-02,  1.9087e-01, -3.9336e-03, -3.7224e-01,\n",
      "         -1.1345e-01,  2.4023e-01,  1.5200e-01, -1.7859e-01,  3.2476e-02,\n",
      "          1.1128e-01,  5.0229e-01,  2.5338e-01, -3.0635e-02,  8.2335e-02,\n",
      "          1.9362e-01,  1.8112e-01, -3.5282e-01, -2.1762e-01,  2.0488e-01,\n",
      "         -5.9908e-01,  3.4371e-01,  4.7269e-01, -3.0886e-02,  2.3934e-01,\n",
      "          1.2805e-01, -2.6538e-02, -2.8702e-01,  7.1476e-02,  1.2069e-01,\n",
      "         -1.3161e-01, -1.1056e-01,  1.6131e-01,  4.7357e-01, -2.7023e-01,\n",
      "         -4.6590e-01, -7.0751e-02, -1.9242e-01, -2.5128e-01,  2.5259e-01,\n",
      "         -1.7629e-01,  7.9083e-01, -3.7084e-01,  1.8949e-01,  1.3598e-02,\n",
      "         -3.6931e-01,  6.9702e-02, -2.3768e-01,  6.9525e-01, -2.3480e-02,\n",
      "         -2.2809e-02, -3.7439e-02,  1.9892e-01, -1.4468e-01, -7.5663e-02,\n",
      "         -6.1484e-02, -2.0330e-01,  1.4454e-01, -6.5246e-03, -9.0436e-01,\n",
      "         -2.9442e-01,  5.0660e-01,  1.0980e-01,  4.2405e-02, -6.7254e-01,\n",
      "          9.0024e-02, -3.0131e-01, -2.7138e-01, -5.1197e-02,  4.1480e-01,\n",
      "         -7.1854e-02,  5.9322e-01,  3.9600e-01, -4.5629e-01, -4.7856e-01,\n",
      "          2.0892e-01,  5.7641e-03,  2.0552e-01,  1.6076e-02, -4.5069e-01,\n",
      "          2.6694e-02, -2.0474e-01, -4.9211e-01, -2.1434e-01,  6.7039e-01,\n",
      "          2.3343e-01, -1.3702e-01,  5.0607e-01, -8.2113e-01,  9.5123e-04,\n",
      "         -7.5240e-01, -1.5532e-01,  4.3672e-01, -3.0627e-01, -4.0455e-01,\n",
      "          3.0231e-01, -8.3461e-01, -4.6982e-01, -4.7310e-01,  8.5968e-02,\n",
      "          3.1752e-01,  9.7022e-02,  5.3731e-02, -1.0291e-01,  1.8698e-01,\n",
      "          1.4745e-01,  2.6636e-01, -4.1221e-01, -3.9111e-01, -7.8550e-01,\n",
      "          7.7262e+00, -2.8045e-01, -1.9356e-01, -2.6403e-01,  2.0341e-01,\n",
      "         -1.2792e-01,  4.6465e-01, -9.5026e-02,  5.4317e-02,  8.0157e-02,\n",
      "         -5.0310e-01,  3.9277e-01,  2.1583e-02,  1.4490e-01,  4.0722e-01,\n",
      "         -8.4354e-02,  1.8140e-02,  1.9599e-01, -2.5508e-01,  8.9821e-02,\n",
      "          2.2004e-01, -9.6986e-01, -6.5325e-01, -7.0072e-01,  1.0955e+00,\n",
      "          1.3739e-02, -2.1894e-01, -3.0676e-01, -6.0736e-01, -5.7603e-01,\n",
      "         -1.6828e-01, -1.9383e-01, -5.7963e-02,  8.5568e-02, -2.9147e-01,\n",
      "          1.6114e-01,  1.2550e-01,  1.7090e-01,  2.5286e-01, -3.4493e-01,\n",
      "          6.9170e-01, -1.6489e-01, -2.0670e-01,  2.4993e-01,  2.6058e-01,\n",
      "          4.9240e-01, -2.2922e-01,  8.8366e-01,  1.2937e-01,  4.5330e-02,\n",
      "         -6.9947e-01, -6.3118e-02,  7.4924e-02, -3.1801e-01,  1.3863e+00,\n",
      "          1.0775e-01, -2.1954e-01,  1.0518e-01, -4.7280e-01,  2.8346e-01,\n",
      "         -3.2886e-01,  3.0317e+00, -1.9298e-01,  1.5737e-01, -5.6977e-01,\n",
      "          2.0649e-02,  1.6906e-01, -2.1980e-02, -2.6150e-01, -2.7248e-01,\n",
      "          2.6628e-01,  7.0445e-01, -4.7327e-02]], grad_fn=<MeanBackward1>)\n",
      "tensor([[ 4.7181e-01, -1.5922e-02, -5.3690e-01,  3.7595e-01,  4.0826e-01,\n",
      "          9.0959e-02,  1.1696e-01, -2.1997e-02, -2.8848e-01,  6.9889e-01,\n",
      "         -1.7185e-01, -5.0065e-01,  2.1015e-01,  1.5330e-01,  7.6692e-01,\n",
      "         -3.9265e-01,  3.1894e-01, -3.6757e-02,  1.9005e-01,  2.5365e-01,\n",
      "         -9.1114e-01,  2.6776e-01, -3.2341e-01, -1.4994e-01, -1.3920e-01,\n",
      "         -4.6185e-01, -9.0227e-01, -3.5949e-01, -3.5695e-01, -2.7824e-01,\n",
      "         -3.1644e-01, -3.7412e-01, -7.0936e-01, -6.4308e-03,  1.5289e-01,\n",
      "         -2.8925e-01,  3.8391e-01, -4.2262e-01, -7.1216e-01, -1.9652e-01,\n",
      "          1.7114e-02, -3.9829e-01, -5.1188e-03,  5.3372e-01, -5.1764e-01,\n",
      "         -3.5912e-01, -4.4662e-02,  2.3584e-01,  4.2424e-02,  2.9812e-01,\n",
      "          6.8803e-01,  1.0041e+01, -2.8625e-02, -2.2960e-01, -7.3358e-01,\n",
      "          3.5159e-01,  1.5474e-01, -2.2833e-01,  8.8296e-02, -3.2569e-01,\n",
      "          1.0294e-01,  3.8592e-01, -2.7979e-01, -4.6335e-01, -5.2117e-01,\n",
      "         -2.8204e-02,  1.7076e-01,  1.1695e-01, -5.7627e-01, -3.9370e-01,\n",
      "          3.8054e-01, -7.9650e-01, -1.7119e-01, -3.6803e-01,  1.5871e-01,\n",
      "          3.1325e-02,  2.3042e-01, -2.8071e-01, -1.2822e-02, -2.4624e-01,\n",
      "         -2.9378e-01, -1.4882e-01, -6.5833e-01,  1.5094e-01, -6.1653e-01,\n",
      "         -1.9968e-01, -4.5106e-02, -4.7905e-01,  8.4857e-01, -3.2822e-01,\n",
      "         -2.8298e-01,  5.1392e-01,  6.0282e-02, -3.4131e-01, -6.7217e-02,\n",
      "          4.7483e-01, -1.3593e-01, -4.0183e-01, -5.0419e-01, -3.4756e-01,\n",
      "         -6.9536e-01,  4.9128e-01, -1.7610e-02, -6.3874e-01, -4.4946e-01,\n",
      "         -4.0180e-01, -3.0451e-01,  1.6070e-01, -5.7899e-01,  1.8904e-02,\n",
      "         -3.4967e-01, -1.8751e-01, -1.3110e-01,  3.5412e-01, -9.0028e-02,\n",
      "         -2.3211e-01, -2.5642e-01,  7.4765e-01, -5.3935e-01, -4.4846e-01,\n",
      "          7.3638e-02,  8.9019e-01,  1.0877e-01, -3.5524e-02,  4.0289e-01,\n",
      "         -9.3961e-01, -6.0299e-01, -2.4007e-02, -5.5927e-02,  2.0047e-01,\n",
      "          2.7320e-01,  4.3337e-02,  7.3556e-01, -4.6749e-02,  1.2547e-01,\n",
      "         -4.1083e-01,  3.7744e-01, -3.7765e-01, -3.4826e-01, -2.8983e-02,\n",
      "          3.2790e-01,  1.5236e-01, -6.5739e-02,  1.1321e-01, -3.3168e-01,\n",
      "          1.3524e+00,  6.8057e-02, -6.1744e-01,  8.8158e-01,  1.4325e-01,\n",
      "         -2.1342e-01, -6.3200e-01, -1.9719e-01,  1.4464e-01, -2.7552e-01,\n",
      "          5.7572e-02,  1.5383e-01, -4.0056e-01,  3.6985e-01, -1.8596e-01,\n",
      "         -3.9841e-02,  4.3799e-01, -7.4384e-02,  5.0328e-01, -4.2222e-02,\n",
      "         -6.2371e-01,  4.0707e-01, -3.9403e-01, -1.7685e-01,  6.7561e-02,\n",
      "          1.1330e-01,  8.3036e-01, -1.9189e-01, -2.1922e-01, -6.1291e-01,\n",
      "          3.6769e-01, -4.8259e-01,  1.5056e-01,  6.7065e-02,  1.3047e-01,\n",
      "         -2.0803e-01, -2.4980e-01,  4.5986e-01, -1.4771e-01,  5.6937e-01,\n",
      "         -1.0569e-01,  3.4588e-01, -3.6649e-01, -9.7651e-02,  1.4854e-01,\n",
      "         -7.8566e-02,  2.1486e-01, -5.0870e-01,  5.1562e-01,  7.6897e-02,\n",
      "          2.6558e-02, -2.0889e-01,  1.1026e-01,  4.8850e-01,  1.0148e-01,\n",
      "         -1.5576e-01, -1.7787e-01, -3.8117e-01,  3.3173e-01,  2.2331e-01,\n",
      "         -2.9969e-01, -7.7963e-02, -3.0451e-01, -1.1752e-01, -5.8628e-01,\n",
      "         -9.5165e-01,  2.3677e-02,  4.3851e-02, -2.6266e-02,  4.8158e-01,\n",
      "         -2.3402e-01, -2.3148e-01,  3.1962e-01,  7.3521e-01,  8.2058e-02,\n",
      "          1.6876e-01,  2.3535e-01, -4.2025e-02, -7.8137e-02,  3.0734e-01,\n",
      "         -6.3754e-02,  1.1048e-01,  5.5265e-02,  3.6399e-01,  3.3526e-01,\n",
      "          2.9335e-01, -5.0135e-02,  1.2575e-01,  4.0149e-01, -4.8573e-01,\n",
      "          2.0308e-01,  5.5824e-01, -2.5790e-01, -7.3566e-01,  4.2435e-01,\n",
      "         -9.0098e-02, -2.5092e-01, -3.0182e-01, -2.2701e-01, -2.0436e-01,\n",
      "          2.9801e-01,  1.2326e-01, -1.0633e-01,  4.8507e-02, -1.6922e-01,\n",
      "          4.0199e-01, -6.6260e-01, -1.6202e-01, -6.5815e-01,  3.1131e-02,\n",
      "          3.4948e-01, -1.0398e-01,  2.3699e-01,  9.5249e-02,  1.0523e-01,\n",
      "         -1.7964e-01,  2.4878e-01, -3.7978e-02, -1.9707e-01,  1.2466e+00,\n",
      "         -4.8863e-01, -6.6210e-02, -3.5452e-01,  2.9910e-01, -1.4618e-01,\n",
      "         -1.0589e-02,  1.7500e-01, -4.0223e-01, -2.7073e-01, -6.6412e-01,\n",
      "          5.3987e-02,  5.2627e-01, -2.3699e-01,  1.2481e-01, -5.4190e-01,\n",
      "          5.6233e-01,  3.7230e-01, -5.4738e-01, -2.1211e-01, -7.9049e-01,\n",
      "         -5.1477e-02, -4.1873e-02, -1.6992e-01, -2.0792e-01, -5.1566e-01,\n",
      "         -5.2267e-01, -5.2090e-01, -7.3940e-02, -2.0306e-01,  9.1947e-01,\n",
      "          3.2437e-01, -1.3238e-02, -4.6340e-01, -4.8081e-02, -4.5400e-01,\n",
      "         -1.7805e-01, -3.0060e-01,  4.4712e-02,  3.1881e-01,  3.9864e-01,\n",
      "         -2.9548e-01, -1.2164e-01,  3.1139e-01, -1.7616e-01, -1.9052e-01,\n",
      "         -2.7762e-01, -9.5601e-02, -5.4679e-02,  1.9304e-01, -1.5457e+00,\n",
      "         -2.3265e-01,  3.8229e-01, -4.1730e-01,  2.0102e-02, -1.3178e-01,\n",
      "         -6.3451e-02,  3.1902e-02,  4.3154e-01,  5.5077e-01, -9.9141e-02,\n",
      "         -1.8303e-01,  5.1029e-01,  1.4077e-01,  2.9447e-01,  9.4935e-02,\n",
      "          1.7611e-01,  1.0593e-01,  2.9244e-03, -4.4726e-01, -4.5596e-01,\n",
      "         -2.2249e-01, -2.5703e-01,  2.8283e-01, -3.0469e-01,  1.7556e-01,\n",
      "         -3.9382e-01,  1.9384e-01,  8.9413e-01,  4.3361e-01, -1.0260e-01,\n",
      "          2.0906e-01,  1.9283e-01, -3.8284e-01, -2.7708e-02,  2.7327e-01,\n",
      "         -4.7857e-01, -3.2210e-01, -4.2749e-01,  2.7554e-01,  4.7858e-01,\n",
      "          2.2698e-01, -7.4282e-01,  7.7211e-02,  1.9236e-01,  4.2984e-01,\n",
      "         -1.1813e-02,  4.5544e-01,  3.3971e-01, -7.8733e-01,  5.1144e-01,\n",
      "          3.2497e-01,  1.2752e-01, -5.3387e-01,  1.7621e-01, -3.7599e-02,\n",
      "          1.6638e-01,  4.9514e-03,  5.8416e-01, -3.6291e-01, -5.0831e-01,\n",
      "         -8.0066e-02, -3.4420e-01,  1.4830e-01, -3.8699e-02, -2.8714e-01,\n",
      "         -7.6193e-01, -2.8839e-01,  1.4316e-02, -9.7821e-02,  2.6415e-01,\n",
      "         -3.2019e-02,  2.4690e-01,  1.5481e-01,  4.5012e-01, -2.5040e-01,\n",
      "         -2.2882e-01, -1.0309e-01,  2.0111e-01, -1.4440e-03,  1.1636e+00,\n",
      "          5.7593e-01,  3.7852e-01, -4.4422e-01, -8.8653e-02, -4.1735e-01,\n",
      "          5.6391e-02, -2.2542e-01,  4.6913e-01,  6.6820e-02,  6.3150e-01,\n",
      "         -5.1858e-01, -2.3653e-01, -1.2691e-01,  2.8074e-01,  3.1179e-03,\n",
      "         -5.5734e-01,  4.0965e-01, -3.3828e-03,  2.5933e-02, -4.3942e-02,\n",
      "          4.6548e-01,  1.2596e+00, -2.5274e-01, -4.3221e-03, -5.7805e-01,\n",
      "         -7.6081e-01, -3.3775e-01, -1.2677e-01, -1.7180e-01,  3.5885e-01,\n",
      "          2.4636e-01,  1.4195e-01, -3.2032e-01,  4.0069e-01, -3.2645e-01,\n",
      "         -2.6406e-01, -3.5920e-01, -4.5459e-02,  3.6275e-02,  2.5434e-01,\n",
      "          2.8508e-01, -2.8986e-01, -2.6461e-01,  4.6995e-01,  5.3403e-02,\n",
      "         -3.1350e-02, -1.9837e-01,  6.1364e-02,  8.0295e-01,  2.8170e-01,\n",
      "         -1.6409e-01,  2.2200e-01, -4.7324e-01,  3.3036e-01,  3.1141e-01,\n",
      "         -3.4900e-02,  5.1522e-01,  6.8727e-03, -8.8173e-01, -2.5144e-01,\n",
      "         -7.3247e-02,  1.6560e-02,  9.5224e-01, -3.0667e-01,  3.8537e-01,\n",
      "         -2.1799e-01,  1.6869e-01, -4.0485e-02,  8.7906e-02,  4.9902e-01,\n",
      "         -3.6689e-01, -2.8216e-01, -1.4490e-02, -3.8582e-01, -6.3664e-01,\n",
      "         -5.2269e-01,  2.6640e-02,  7.5996e-01,  5.5287e-01, -1.6308e-01,\n",
      "         -3.7109e-02,  2.0123e-01, -4.1562e-01, -1.2318e+00, -7.0270e-01,\n",
      "         -5.1724e-02,  2.1706e-01,  3.2027e-01, -4.8219e-01,  4.9653e-01,\n",
      "         -2.2649e-02,  3.7923e-01,  1.2562e-02,  5.0256e-01,  1.9600e-01,\n",
      "         -9.1717e-01, -2.9677e-01,  1.6920e-01, -4.2574e-01, -2.1831e-01,\n",
      "         -3.1418e-01,  3.5893e-01,  1.9024e-01, -1.0797e-01, -4.1337e-01,\n",
      "          7.3587e-01, -5.1341e-01,  5.5656e-01, -1.9930e-01,  1.0447e-04,\n",
      "          4.1541e-01,  4.9438e-01, -4.6805e-01,  2.3007e-01, -8.4982e-02,\n",
      "         -2.1472e-01, -1.2220e+00,  4.9250e-01, -5.9793e-01,  3.9739e-02,\n",
      "          3.1499e-01, -5.7660e-01,  3.5048e-01, -2.1402e-01, -1.0076e-01,\n",
      "         -1.9707e-01, -4.4909e-02,  3.5409e-01, -1.9341e-01,  3.0959e-01,\n",
      "         -1.0635e-01, -9.2844e-02, -6.4606e-01,  4.1562e-01,  4.4499e-01,\n",
      "          1.6979e-02,  3.6968e-01, -5.7706e-01, -3.3501e-02,  3.6477e-01,\n",
      "          4.6060e-01, -2.4537e-01, -1.4884e-01, -1.7543e-01, -2.8190e-01,\n",
      "          2.8056e-01, -6.0800e-01,  1.3040e-01, -4.8699e-01,  2.7948e-01,\n",
      "         -4.0468e-01, -5.9014e-01,  2.6338e-01, -1.4606e-01, -1.7829e-01,\n",
      "          1.2517e-02,  1.2787e-01, -6.9208e-01,  2.7947e-01, -6.4140e-02,\n",
      "         -3.7984e-01,  2.0830e-01, -7.0148e-03, -7.3073e-04, -4.3866e-01,\n",
      "         -2.5333e-02,  1.9422e-01, -5.5183e-01, -4.0520e-02, -6.2451e-01,\n",
      "          2.3509e-01, -1.4321e-02,  1.4417e-01, -5.3221e-01, -1.4790e-01,\n",
      "         -1.3041e-03,  2.2976e-01,  1.6320e-01, -5.9732e-01, -7.7693e-01,\n",
      "          9.5762e-02, -2.0565e-01,  8.2776e-02,  8.3869e-02,  1.0175e-01,\n",
      "          1.3750e-01,  7.7774e-01, -2.4631e-01, -4.0662e-01, -5.0351e-01,\n",
      "         -3.6597e-01,  2.0997e-01,  8.2144e-01,  4.0533e-01,  1.3236e-02,\n",
      "         -3.7144e-01,  1.0975e-01,  3.7477e-01,  3.8125e-01, -9.0662e-02,\n",
      "         -1.3224e-02,  6.1903e-02, -1.3684e-01,  3.7284e-01,  5.8881e-01,\n",
      "          2.4748e-01, -2.4370e-01, -4.7791e-02,  1.9272e-01, -1.5003e-01,\n",
      "         -4.8081e-01, -2.6185e-01,  5.5054e-01,  1.8582e-01,  4.3327e-02,\n",
      "         -6.4390e-02, -1.7989e-02,  2.8481e-01,  8.4019e-01, -8.8305e-01,\n",
      "         -2.7474e-01, -2.5099e-01,  7.5661e-01,  5.2288e-01,  1.8601e-01,\n",
      "          9.1504e-02,  2.6982e-01, -1.1253e-01, -2.7422e-01,  4.5759e-01,\n",
      "         -3.4370e-01,  8.0341e-01,  1.2020e-02,  3.0773e-01,  3.1507e-01,\n",
      "          1.3923e-01, -3.6422e-01,  2.6409e-01,  1.4718e-01,  1.9350e-02,\n",
      "          1.5219e-01,  9.8811e-02,  6.8967e-01, -3.7756e-01, -5.4829e-01,\n",
      "          1.2540e-01, -4.7711e-01,  4.4668e-01,  5.2736e-01, -1.8504e-01,\n",
      "         -3.0847e-01,  3.7244e-01, -1.8152e-02, -3.8978e-02, -7.1794e-01,\n",
      "         -2.1398e-01,  4.0321e-01, -1.3444e-01, -2.3091e-01,  1.6769e-02,\n",
      "          3.0936e-01,  7.0717e-01, -4.8718e-01, -1.8736e-01, -3.1554e-01,\n",
      "          2.0122e-01, -3.5195e-01,  4.6135e-01, -4.8622e-01, -5.0829e-01,\n",
      "         -1.0329e-01, -6.5229e-01, -6.9098e-01, -5.0158e-01,  4.0045e-01,\n",
      "          6.2701e-01, -2.4479e-01,  2.8421e-01, -9.6520e-01, -3.3255e-01,\n",
      "         -3.2470e-01, -6.8515e-02,  4.9394e-01,  2.1754e-02,  2.5998e-02,\n",
      "          2.8956e-01, -1.0925e-02, -3.0454e-01, -5.1357e-01,  1.8416e-01,\n",
      "          7.7747e-01,  4.9109e-01,  8.9992e-02, -1.0074e-01,  6.6655e-02,\n",
      "          5.6542e-01,  9.3223e-03, -3.6654e-01, -9.6834e-01, -2.2179e-01,\n",
      "          6.4800e+00,  8.0626e-02,  2.9494e-02,  1.1593e-01, -4.6516e-02,\n",
      "         -2.4368e-01,  5.8948e-01,  1.5227e-01, -1.9323e-01, -4.3429e-01,\n",
      "         -5.8223e-01, -1.3361e-01, -2.3570e-02, -5.2785e-01,  2.1816e-01,\n",
      "         -1.1580e-01, -8.3193e-02,  3.7445e-01, -5.5780e-01,  5.4612e-02,\n",
      "          5.2706e-01, -9.5729e-01, -6.2520e-01, -3.4109e-02,  7.2079e-01,\n",
      "         -5.6877e-01, -6.7226e-01, -4.1199e-01, -5.2376e-01, -4.6883e-01,\n",
      "          5.1593e-01, -6.9669e-02, -1.3757e-01,  2.9695e-01, -1.6106e-01,\n",
      "         -3.5153e-01,  3.1618e-01,  3.6291e-01,  2.7359e-01,  9.2066e-02,\n",
      "          1.3017e+00, -4.0704e-01,  1.0672e-01,  5.4593e-01,  5.6863e-01,\n",
      "          5.6206e-01,  2.0333e-01,  5.7317e-01,  1.2269e-01, -1.8843e-01,\n",
      "         -5.7779e-01, -9.7453e-02, -1.4942e-01, -2.4183e-01,  1.4128e+00,\n",
      "         -6.9724e-03, -1.6774e-01, -3.2920e-02, -7.0683e-01, -1.1479e-01,\n",
      "         -6.9339e-02,  2.9376e+00,  2.9770e-01,  1.6153e-01, -3.2735e-01,\n",
      "         -1.7368e-01,  2.6354e-01, -1.8837e-01, -1.5139e-01, -4.1787e-01,\n",
      "          5.7685e-02,  2.0364e-01, -2.9633e-01]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 读取数据\n",
    "data_path = 'merged_data_n.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# 取前100条数据\n",
    "data = data.head(10)\n",
    "\n",
    "# 读取预训练的BERT模型和tokenizer\n",
    "vocab_file = 'pre_model/bert/vocab.txt' \n",
    "tokenizer = BertTokenizer(vocab_file)\n",
    "bert = BertModel.from_pretrained(f\"pre_model/bert/bert-base-chinese/\")\n",
    "\n",
    "# 循环处理每个文本示例\n",
    "for text in data['0']:  # 请将 'your_text_column_name' 替换为包含文本数据的列的实际列名\n",
    "    tokens = tokenizer.encode_plus(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    attention_mask = tokens[\"attention_mask\"]\n",
    "    outputs = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    sentence_embedding = torch.mean(last_hidden_state, dim=1)\n",
    "    print(sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71421776-1962-4bda-afbb-a54429d97f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b2e6c-1082-4ff6-a6d6-ad2adb385ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pre_model/bert/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Generating BERT embeddings:   0%|                                                                                                                            | 0/16726 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Generating BERT embeddings:   6%|██████▋                                                                                                           | 986/16726 [01:45<38:10,  6.87it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = False\n",
    "# 检查 GPU 是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 读取数据\n",
    "data_path = 'merged_data_n.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# # 取前10条数据\n",
    "# data = data.head(10)\n",
    "\n",
    "# 读取预训练的BERT模型和tokenizer\n",
    "vocab_file = 'pre_model/bert/vocab.txt' \n",
    "tokenizer = BertTokenizer(vocab_file)\n",
    "bert = BertModel.from_pretrained(\"pre_model/bert/bert-base-chinese/\")\n",
    "\n",
    "# 循环处理每个文本示例，生成文本嵌入\n",
    "bert_embeddings = []\n",
    "for text in tqdm(data['0'], desc=\"Generating BERT embeddings\"):\n",
    "    tokens = tokenizer.encode_plus(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    attention_mask = tokens[\"attention_mask\"]\n",
    "    outputs = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    sentence_embedding = torch.mean(last_hidden_state, dim=1)\n",
    "    bert_embeddings.append(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c01c2db7-2105-42a1-962f-7a10cc5540c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pre_model/bert/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Generating BERT embeddings:   0%|                                                                                                                               | 0/10 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Generating BERT embeddings: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.32it/s]\n",
      "d:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 131\u001b[0m\n\u001b[0;32m    129\u001b[0m input_ids, attention_mask, labels \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device), attention_mask\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    130\u001b[0m finetune_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 131\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfinetuned_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m    133\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[2], line 92\u001b[0m, in \u001b[0;36mFineTunedModel.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[1;32m---> 92\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[0;32m     94\u001b[0m     lstm_out \u001b[38;5;241m=\u001b[39m lstm_out\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\torch\\nn\\functional.py:2043\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2037\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2038\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2039\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2040\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2041\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2042\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = False\n",
    "# 检查 GPU 是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 读取数据\n",
    "data_path = 'merged_data_n.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# 取前10条数据\n",
    "data = data.head(10)\n",
    "\n",
    "# 读取预训练的BERT模型和tokenizer\n",
    "vocab_file = 'pre_model/bert/vocab.txt' \n",
    "tokenizer = BertTokenizer(vocab_file)\n",
    "bert = BertModel.from_pretrained(\"pre_model/bert/bert-base-chinese/\")\n",
    "\n",
    "# 循环处理每个文本示例，生成文本嵌入\n",
    "bert_embeddings = []\n",
    "for text in tqdm(data['0'], desc=\"Generating BERT embeddings\"):\n",
    "    tokens = tokenizer.encode_plus(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    attention_mask = tokens[\"attention_mask\"]\n",
    "    outputs = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    sentence_embedding = torch.mean(last_hidden_state, dim=1)\n",
    "    bert_embeddings.append(sentence_embedding)\n",
    "\n",
    "# 转换为NumPy数组\n",
    "bert_embeddings = torch.cat(bert_embeddings).detach().numpy()\n",
    "\n",
    "# 准备文本数据\n",
    "max_words = min(5000, bert_embeddings.shape[0] + 1)\n",
    "max_sequence_length = 32\n",
    "\n",
    "# 初始化 word_index\n",
    "word_index = {word: idx + 1 for idx, word in enumerate(tokenizer.get_vocab())}\n",
    "\n",
    "# 初始化正确大小的 embedding_matrix\n",
    "embedding_matrix = np.zeros((bert_embeddings.shape[0] + 1, bert_embeddings.shape[1]))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words and word in tokenizer.get_vocab():\n",
    "        embedding_matrix[i] = bert_embeddings[tokenizer.get_vocab()[word]]\n",
    "\n",
    "# 构建 DataLoader\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# 使用 encode_plus 将文本转换为模型输入\n",
    "train_inputs = [tokenizer.encode_plus(text, truncation=True, padding='max_length', max_length=max_sequence_length, return_tensors='pt') for text in train_data['0']]\n",
    "test_inputs = [tokenizer.encode_plus(text, truncation=True, padding='max_length', max_length=max_sequence_length, return_tensors='pt') for text in test_data['0']]\n",
    "\n",
    "# 提取 input_ids 和 attention_mask\n",
    "train_input_ids = torch.cat([input[\"input_ids\"] for input in train_inputs], dim=0)\n",
    "train_attention_mask = torch.cat([input[\"attention_mask\"] for input in train_inputs], dim=0)\n",
    "test_input_ids = torch.cat([input[\"input_ids\"] for input in test_inputs], dim=0)\n",
    "test_attention_mask = torch.cat([input[\"attention_mask\"] for input in test_inputs], dim=0)\n",
    "\n",
    "# 创建 TensorDataset 和 DataLoader\n",
    "train_labels = train_data['label']\n",
    "test_labels = test_data['label']\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, torch.tensor(train_labels.values, dtype=torch.float32).unsqueeze(1))\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_mask, torch.tensor(test_labels.values, dtype=torch.float32).unsqueeze(1))\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 定义微调模型\n",
    "class FineTunedModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size=768, dropout=0.1):\n",
    "        super(FineTunedModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_matrix.shape[1], hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.embedding(input_ids)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out.mean(dim=1)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        output = torch.sigmoid(self.fc(lstm_out))\n",
    "        return output\n",
    "\n",
    "# 创建微调模型并移动到 GPU\n",
    "finetuned_model = FineTunedModel(embedding_matrix)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "finetuned_model = finetuned_model.to(device)\n",
    "\n",
    "num_finetune_epochs = 50\n",
    "# 定义微调优化器和学习率\n",
    "finetune_optimizer = AdamW([\n",
    "    {'params': finetuned_model.embedding.parameters(), 'lr': 3e-5},\n",
    "    {'params': finetuned_model.lstm.parameters()},\n",
    "    {'params': finetuned_model.fc.parameters()}\n",
    "], lr=3e-5)\n",
    "\n",
    "# 学习率调度器（可选）\n",
    "total_steps = len(train_dataloader) * num_finetune_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    finetune_optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 微调模型\n",
    "num_finetune_epochs = 50\n",
    "for epoch in range(num_finetune_epochs):\n",
    "    finetuned_model.train()\n",
    "    for input_ids, attention_mask, labels in train_dataloader:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        finetune_optimizer.zero_grad()\n",
    "        outputs = finetuned_model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        finetune_optimizer.step()\n",
    "\n",
    "        # 学习率调度器步进（可选）\n",
    "        scheduler.step()\n",
    "\n",
    "    # 在测试集上进行评估\n",
    "    finetuned_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = finetuned_model(test_input_ids.to(device), test_attention_mask.to(device))\n",
    "        test_loss = criterion(test_outputs, test_labels_tensor.to(device))\n",
    "\n",
    "        # 计算准确率\n",
    "        test_outputs = (test_outputs > 0.5).float()\n",
    "        accuracy = (test_outputs == test_labels_tensor.to(device)).float().mean().item()\n",
    "\n",
    "    print(f'Fine-tune Epoch {epoch + 1}/{num_finetune_epochs}, Loss: {loss.item()}, Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# 保存微调后的模型\n",
    "torch.save(finetuned_model.state_dict(), 'finetuned_model.pth')\n",
    "\n",
    "# 评估微调后的模型\n",
    "finetuned_model.eval()\n",
    "with torch.no_grad():\n",
    "    finetuned_outputs = finetuned_model(test_input_ids.to(device), test_attention_mask.to(device))\n",
    "    finetuned_outputs = (finetuned_outputs > 0.5).float()\n",
    "    finetuned_accuracy = (finetuned_outputs == test_labels_tensor.to(device)).float().mean().item()\n",
    "\n",
    "print(f'Fine-tuned Model Test Accuracy: {finetuned_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85d61338-8bb0-4e14-a818-6a2e90ae2cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pre_model/bert/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Generating BERT embeddings:   0%|                                                                                                                              | 0/100 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Generating BERT embeddings: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.87it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# 创建微调模型并移动到 GPU\u001b[39;00m\n\u001b[0;32m     92\u001b[0m finetuned_model \u001b[38;5;241m=\u001b[39m FineTunedModel(embedding_matrix)\n\u001b[1;32m---> 93\u001b[0m finetuned_model \u001b[38;5;241m=\u001b[39m \u001b[43mfinetuned_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# 定义微调优化器和学习率\u001b[39;00m\n\u001b[0;32m     96\u001b[0m finetune_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam([\n\u001b[0;32m     97\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: finetuned_model\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3e-5\u001b[39m},\n\u001b[0;32m     98\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: finetuned_model\u001b[38;5;241m.\u001b[39mlstm\u001b[38;5;241m.\u001b[39mparameters()},\n\u001b[0;32m     99\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: finetuned_model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39mparameters()}\n\u001b[0;32m    100\u001b[0m ], lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m)\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:852\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    848\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    849\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:530\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 530\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    534\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    535\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    541\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:552\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 552\u001b[0m         param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m     should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:850\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m    848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    849\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 850\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取数据\n",
    "data_path = 'merged_data_n.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# 取前100条数据\n",
    "data = data.head(100)\n",
    "\n",
    "# 读取预训练的BERT模型和tokenizer\n",
    "vocab_file = 'pre_model/bert/vocab.txt' \n",
    "tokenizer = BertTokenizer(vocab_file)\n",
    "bert = BertModel.from_pretrained(\"pre_model/bert/bert-base-chinese/\")\n",
    "\n",
    "# 循环处理每个文本示例，生成文本嵌入\n",
    "bert_embeddings = []\n",
    "for text in tqdm(data['0'], desc=\"Generating BERT embeddings\"):\n",
    "    tokens = tokenizer.encode_plus(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    attention_mask = tokens[\"attention_mask\"]\n",
    "    outputs = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    sentence_embedding = torch.mean(last_hidden_state, dim=1)\n",
    "    bert_embeddings.append(sentence_embedding)\n",
    "\n",
    "# 转换为NumPy数组\n",
    "bert_embeddings = torch.cat(bert_embeddings).detach().numpy()\n",
    "\n",
    "\n",
    "# 准备文本数据\n",
    "max_words = 5000\n",
    "max_sequence_length = 500\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(data['0'])\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_words, len(word_index) + 1)\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, bert_embeddings.shape[1]))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words and word in tokenizer.get_config()['word_index']:\n",
    "        embedding_matrix[i] = bert_embeddings[tokenizer.word_index[word]]\n",
    "\n",
    "# 构建 DataLoader\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['0'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['0'])\n",
    "\n",
    "train_inputs = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "test_inputs = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "train_labels = train_data['label']\n",
    "test_labels = test_data['label']\n",
    "\n",
    "train_inputs_tensor = torch.tensor(train_inputs, dtype=torch.long)\n",
    "train_labels_tensor = torch.tensor(train_labels.values, dtype=torch.float32).unsqueeze(1)\n",
    "test_inputs_tensor = torch.tensor(test_inputs, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(train_inputs_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_inputs_tensor, test_labels_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 定义微调模型\n",
    "class FineTunedModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size=384):\n",
    "        super(FineTunedModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_matrix.shape[1], hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out.mean(dim=1)\n",
    "        output = torch.sigmoid(self.fc(lstm_out))\n",
    "        return output\n",
    "\n",
    "# 创建微调模型并移动到 GPU\n",
    "finetuned_model = FineTunedModel(embedding_matrix)\n",
    "finetuned_model = finetuned_model.to(device)\n",
    "\n",
    "# 定义微调优化器和学习率\n",
    "finetune_optimizer = optim.Adam([\n",
    "    {'params': finetuned_model.embedding.parameters(), 'lr': 3e-5},\n",
    "    {'params': finetuned_model.lstm.parameters()},\n",
    "    {'params': finetuned_model.fc.parameters()}\n",
    "], lr=3e-4)\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 微调模型\n",
    "num_finetune_epochs = 50\n",
    "for epoch in range(num_finetune_epochs):\n",
    "    finetuned_model.train()\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        finetune_optimizer.zero_grad()\n",
    "        outputs = finetuned_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        finetune_optimizer.step()\n",
    "\n",
    "    # 在测试集上进行评估\n",
    "    finetuned_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = finetuned_model(test_inputs_tensor.to(device))\n",
    "        test_loss = criterion(test_outputs, test_labels_tensor.to(device))\n",
    "\n",
    "        # 计算准确率\n",
    "        test_outputs = (test_outputs > 0.5).float()\n",
    "        accuracy = (test_outputs == test_labels_tensor.to(device)).float().mean().item()\n",
    "\n",
    "    print(f'Fine-tune Epoch {epoch + 1}/{num_finetune_epochs}, Loss: {loss.item()}, Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# 保存微调后的模型\n",
    "torch.save(finetuned_model.state_dict(), 'finetuned_model.pth')\n",
    "\n",
    "# 评估模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_inputs_tensor.to(device))\n",
    "    test_outputs = (test_outputs > 0.5).float()\n",
    "    accuracy = (test_outputs == test_labels_tensor.to(device)).float().mean().item()\n",
    "\n",
    "print(f'Original Model Test Accuracy: {accuracy}')\n",
    "\n",
    "# 最终评估微调后的模型\n",
    "finetuned_model.eval()\n",
    "with torch.no_grad():\n",
    "    finetuned_outputs = finetuned_model(test_inputs_tensor.to(device))\n",
    "    finetuned_outputs = (finetuned_outputs > 0.5).float()\n",
    "    finetuned_accuracy = (finetuned_outputs == test_labels_tensor.to(device)).float().mean().item()\n",
    "\n",
    "print(f'Fine-tuned Model Test Accuracy: {finetuned_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54529b48-8866-4325-bb53-a4a9df1f6e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pre_model/bert/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Generating BERT embeddings:   0%|                                                                                                                               | 0/10 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Generating BERT embeddings: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 137\u001b[0m\n\u001b[0;32m    133\u001b[0m             accuracy \u001b[38;5;241m=\u001b[39m (test_outputs \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFine-tune Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_finetune_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# 保存微调后的模型\u001b[39;00m\n\u001b[0;32m    140\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(finetuned_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinetuned_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loss' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 检查 GPU 是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 读取数据\n",
    "data_path = 'merged_data_n.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# 取前100条数据\n",
    "data = data.head(10)\n",
    "\n",
    "# 读取预训练的BERT模型和tokenizer\n",
    "vocab_file = 'pre_model/bert/vocab.txt' \n",
    "tokenizer = BertTokenizer(vocab_file)\n",
    "bert = BertModel.from_pretrained(\"pre_model/bert/bert-base-chinese/\")\n",
    "\n",
    "# 循环处理每个文本示例，生成文本嵌入\n",
    "bert_embeddings = []\n",
    "vocab_list = tokenizer.get_vocab()\n",
    "for text in tqdm(data['0'], desc=\"Generating BERT embeddings\"):\n",
    "    tokens = tokenizer.encode_plus(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    attention_mask = tokens[\"attention_mask\"]\n",
    "    outputs = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    sentence_embedding = torch.mean(last_hidden_state, dim=1)\n",
    "    bert_embeddings.append(sentence_embedding)\n",
    "\n",
    "# 转换为NumPy数组\n",
    "bert_embeddings = torch.cat(bert_embeddings).detach().numpy()\n",
    "\n",
    "\n",
    "# 准备文本数据\n",
    "max_words = min(5000, len(vocab_list) - 1)\n",
    "max_sequence_length = 100\n",
    "\n",
    "# 初始化 word_index\n",
    "word_index = {word: idx + 1 for idx, word in enumerate(vocab_list)}\n",
    "\n",
    "# 初始化正确大小的 embedding_matrix\n",
    "embedding_matrix = np.zeros((len(vocab_list) + 1, bert_embeddings.shape[1]))\n",
    "\n",
    "# ...\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if 1 <= i < max_words and word in vocab_list and vocab_list[word] <= len(bert_embeddings):\n",
    "        embedding_matrix[i] = bert_embeddings[vocab_list[word] - 1]\n",
    "\n",
    "\n",
    "\n",
    "# 构建 DataLoader\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_sequences = [tokenizer.encode_plus(text, truncation=True, padding='max_length', max_length=max_sequence_length, return_tensors=\"pt\") for text in train_data['0']]\n",
    "test_sequences = [tokenizer.encode_plus(text, truncation=True, padding='max_length', max_length=max_sequence_length, return_tensors=\"pt\") for text in test_data['0']]\n",
    "\n",
    "train_inputs = torch.cat([sequence[\"input_ids\"] for sequence in train_sequences], dim=0)\n",
    "test_inputs = torch.cat([sequence[\"input_ids\"] for sequence in test_sequences], dim=0)\n",
    "\n",
    "train_labels = train_data['label']\n",
    "test_labels = test_data['label']\n",
    "\n",
    "train_labels_tensor = torch.tensor(train_labels.values, dtype=torch.float32).unsqueeze(1)\n",
    "test_labels_tensor = torch.tensor(test_labels.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(train_inputs, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_inputs, test_labels_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 定义微调模型\n",
    "class FineTunedModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size=384):\n",
    "        super(FineTunedModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_matrix.shape[1], hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out.mean(dim=1)\n",
    "        output = torch.sigmoid(self.fc(lstm_out))\n",
    "        return output\n",
    "\n",
    "# 创建微调模型并移动到 GPU\n",
    "finetuned_model = FineTunedModel(embedding_matrix)\n",
    "finetuned_model = finetuned_model.to(device)\n",
    "\n",
    "# 定义微调优化器和学习率\n",
    "finetune_optimizer = optim.Adam([\n",
    "    {'params': finetuned_model.embedding.parameters(), 'lr': 3e-5},\n",
    "    {'params': finetuned_model.lstm.parameters()},\n",
    "    {'params': finetuned_model.fc.parameters()}\n",
    "], lr=3e-4)\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 微调模型\n",
    "num_finetune_epochs = 50\n",
    "for epoch in range(num_finetune_epochs):\n",
    "    finetuned_model.train()\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        finetune_optimizer.zero_grad()\n",
    "        outputs = finetuned_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        finetune_optimizer.step()\n",
    "\n",
    "    # 在测试集上进行评估\n",
    "    finetuned_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # test_outputs = finetuned_model(test_inputs_tensor.to(device))\n",
    "        # test_loss = criterion(test_outputs, test_labels_tensor.to(device))\n",
    "\n",
    "        # # 计算准确率\n",
    "        # test_outputs = (test_outputs > 0.5).float()\n",
    "        # accuracy = (test_outputs == test_labels_tensor.to(device)).float().mean().item()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_dataloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                test_outputs = finetuned_model(inputs)\n",
    "                test_outputs = (test_outputs > 0.5).float()\n",
    "                accuracy = (test_outputs == labels).float().mean().item()\n",
    "        print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "    # print(f'Fine-tune Epoch {epoch + 1}/{num_finetune_epochs}, Loss: {loss.item()}, Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# 保存微调后的模型\n",
    "torch.save(finetuned_model.state_dict(), 'finetuned_model.pth')\n",
    "\n",
    "# 评估模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_inputs_tensor.to(device))\n",
    "    test_outputs = (test_outputs > 0.5).float()\n",
    "    accuracy = (test_outputs == test_labels_tensor.to(device)).float().mean().item()\n",
    "\n",
    "print(f'Original Model Test Accuracy: {accuracy}')\n",
    "\n",
    "# 最终评估微调后的模型\n",
    "finetuned_model.eval()\n",
    "with torch.no_grad():\n",
    "    finetuned_outputs = finetuned_model(test_inputs_tensor.to(device))\n",
    "    finetuned_outputs = (finetuned_outputs > 0.5).float()\n",
    "    finetuned_accuracy = (finetuned_outputs == test_labels_tensor.to(device)).float().mean().item()\n",
    "\n",
    "print(f'Fine-tuned Model Test Accuracy: {finetuned_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6adcaa78-358a-49d4-8be7-d1776f26f48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 2769, 4263, 1266, 776, 1921, 2128, 7305, 102], [101, 1920, 2157, 1962, 102], [101, 791, 1921, 1921, 3698, 4696, 1962, 102]]\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# texts = ['我爱北京天安门', '大家好', '今天天气真好']\n",
    "\n",
    "# sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# print(sequences)\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "texts = ['我爱北京天安门', '大家好', '今天天气真好']\n",
    "\n",
    "# 使用tokenizer对文本进行编码\n",
    "encoded_texts = [tokenizer.encode(text, add_special_tokens=True) for text in texts]\n",
    "\n",
    "print(encoded_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c90df34a-744c-45b1-a13e-70f791ec643e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] 找不到指定的程序。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field, TabularDataset\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequence\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 读取数据\u001b[39;00m\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\torchtext\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_torch_home\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m      8\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\torchtext\\_extension.py:64\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\torchtext\\_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\torchtext\\_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\pycharmprojects\\tianchi\\venv\\lib\\site-packages\\torch\\_ops.py:104\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     99\u001b[0m path \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[1;32mD:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\lib\\ctypes\\__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] 找不到指定的程序。"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# from tqdm import tqdm\n",
    "# from torchtext.data import Field, TabularDataset\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# # 读取数据\n",
    "# data_path = 'merged_data_n.csv'\n",
    "# data = pd.read_csv(data_path)\n",
    "\n",
    "# # 取前100条数据\n",
    "# data = data.head(100)\n",
    "\n",
    "# # 读取预训练的BERT模型和tokenizer\n",
    "# vocab_file = 'pre_model/bert/vocab.txt' \n",
    "# tokenizer = BertTokenizer(vocab_file)\n",
    "# bert = BertModel.from_pretrained(\"pre_model/bert/bert-base-chinese/\")\n",
    "\n",
    "# # 循环处理每个文本示例，生成文本嵌入\n",
    "# bert_embeddings = []\n",
    "# for text in tqdm(data['0'], desc=\"Generating BERT embeddings\"):\n",
    "#     tokens = tokenizer.encode_plus(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "#     input_ids = tokens[\"input_ids\"]\n",
    "#     attention_mask = tokens[\"attention_mask\"]\n",
    "#     outputs = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#     last_hidden_state = outputs.last_hidden_state\n",
    "#     sentence_embedding = torch.mean(last_hidden_state, dim=1)\n",
    "#     bert_embeddings.append(sentence_embedding)\n",
    "\n",
    "# # 转换为NumPy数组\n",
    "# bert_embeddings = torch.cat(bert_embeddings).detach().numpy()\n",
    "\n",
    "# # 准备文本数据\n",
    "# max_words = 5000\n",
    "# max_sequence_length = 100\n",
    "\n",
    "# # 定义Field\n",
    "# TEXT = Field(sequential=True, use_vocab=True, tokenize=tokenizer.encode, lower=True, include_lengths=True, batch_first=True, fix_length=max_sequence_length, pad_token=tokenizer.pad_token_id)\n",
    "\n",
    "# # 使用TabularDataset处理数据\n",
    "# fields = [(\"text\", TEXT), (\"label\", None)]  # label字段设为None，因为我们不处理它\n",
    "# examples = []\n",
    "\n",
    "# for text, label in zip(data['0'], data['label']):\n",
    "#     examples.append((text, label))\n",
    "\n",
    "# train_data, test_data = TabularDataset.splits(\n",
    "#     path='',\n",
    "#     train='train_data.csv',\n",
    "#     test='test_data.csv',\n",
    "#     format='csv',\n",
    "#     fields=fields,\n",
    "#     skip_header=True\n",
    "# )\n",
    "\n",
    "# # 构建词汇表\n",
    "# TEXT.build_vocab(train_data, max_size=max_words)\n",
    "\n",
    "# # 获取词汇表和embedding matrix\n",
    "# vocab = TEXT.vocab\n",
    "# embedding_matrix = np.zeros((num_words, bert_embeddings.shape[1]))\n",
    "# for word, i in vocab.stoi.items():\n",
    "#     if i < max_words and word in tokenizer.get_config()['word_index']:\n",
    "#         embedding_matrix[i] = bert_embeddings[tokenizer.word_index[word]]\n",
    "\n",
    "# # 获取训练和测试数据\n",
    "# train_inputs_tensor = pad_sequence([getattr(train_data.examples[i], 'text') for i in range(len(train_data.examples))], batch_first=True)\n",
    "# train_labels_tensor = torch.tensor([train_data.examples[i].label for i in range(len(train_data.examples))], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# test_inputs_tensor = pad_sequence([getattr(test_data.examples[i], 'text') for i in range(len(test_data.examples))], batch_first=True)\n",
    "# test_labels_tensor = torch.tensor([test_data.examples[i].label for i in range(len(test_data.examples))], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # 构建 DataLoader\n",
    "# batch_size = 64\n",
    "# train_dataset = TensorDataset(train_inputs_tensor, train_labels_tensor)\n",
    "# test_dataset = TensorDataset(test_inputs_tensor, test_labels_tensor)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # 定义微调模型\n",
    "# class FineTunedModel(nn.Module):\n",
    "#     def __init__(self, embedding_matrix, hidden_size=384):\n",
    "#         super(FineTunedModel, self).__init__()\n",
    "#         self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=tokenizer.pad_token_id)\n",
    "#         self.lstm = nn.LSTM(embedding_matrix.shape[1], hidden_size, bidirectional=True, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)\n",
    "#         lstm_out, _ = self.lstm(x)\n",
    "#         lstm_out = lstm_out.mean(dim=1)\n",
    "#         output = torch.sigmoid(self.fc(lstm_out))\n",
    "#         return output\n",
    "\n",
    "# # 创建微调模型并移动到 GPU\n",
    "# finetuned_model = FineTunedModel(embedding_matrix)\n",
    "# finetuned_model = finetuned_model.to(device)\n",
    "\n",
    "# # 定义微调优化器和学习率\n",
    "# finetune_optimizer = optim.Adam([\n",
    "#     {'params': finetuned_model.embedding.parameters(), 'lr': 3e-5},\n",
    "#     {'params': finetuned_model.lstm.parameters()},\n",
    "#     {'params': finetuned_model.fc.parameters()}\n",
    "# ], lr=3e-4)\n",
    "\n",
    "# # 定义损失函数\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "# # 微调模型\n",
    "# num_finetune_epochs = 50\n",
    "# for epoch in range(num_finetune_epochs):\n",
    "#     finetuned_model.train()\n",
    "#     for inputs, labels in train_dataloader:\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "#         finetune_optimizer.zero_grad()\n",
    "#         outputs = finetuned_model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         finetune_optimizer.step()\n",
    "\n",
    "#     # 在测试集上进行评估\n",
    "#     finetuned_model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         test_outputs = finetuned_model(test_inputs_tensor.to(device))\n",
    "#         test_loss = criterion(test_outputs, test_labels_tensor.to(device))\n",
    "\n",
    "#         # 计算准确率\n",
    "#         test_outputs = (test_outputs > 0.5).float()\n",
    "#         accuracy = (test_outputs == test_labels_tensor.to(device)).float().mean().item()\n",
    "\n",
    "#     print(f'Fine-tune Epoch {epoch + 1}/{num_finetune_epochs}, Loss: {loss.item()}, Test Loss: {test_loss.item()}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# # 保存微调后的模型\n",
    "# torch.save(finetuned_model.state_dict(), 'finetuned_model.pth')\n",
    "\n",
    "# # 评估模型\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     test_outputs = model(test_inputs_tensor.to(device))\n",
    "#     test_outputs = (test_outputs > 0.5).float()\n",
    "#     accuracy = (test_outputs == test_labels_tensor.to(device)).float().mean().item()\n",
    "\n",
    "# print(f'Original Model Test Accuracy: {accuracy}')\n",
    "\n",
    "# # 最终评估微调后的模型\n",
    "# finetuned_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     finetuned_outputs = finetuned_model(test_inputs_tensor.to(device))\n",
    "#     finetuned_outputs = (finetuned_outputs > 0.5).float()\n",
    "#     finetuned_accuracy = (finetuned_outputs == test_labels_tensor.to(device)).float().mean().item()\n",
    "\n",
    "# print(f'Fine-tuned Model Test Accuracy: {finetuned_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7193cb2-e83b-4d75-9d00-616c7323c6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e9dbce-4482-4928-968d-2008c44c9034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "first point: 0 1\n",
      "second point: 7 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def draw_line(x1,y1,x2,y2):\n",
    "    dx=x2-x1\n",
    "    dy=y2-y1\n",
    "    d=dx-2*dy\n",
    "    y=y1\n",
    "    x=x1\n",
    "\n",
    "    points=[(x,y)]\n",
    "    while x<x2:\n",
    "        if d<0:\n",
    "            y+=1\n",
    "            x+=1\n",
    "            d=d+2*dx-2*dy\n",
    "        else:\n",
    "            x+=1\n",
    "            d-=2*dy \n",
    "        # print(d,x,y)\n",
    "        points.append((x,y))\n",
    "    return points\n",
    "\n",
    "x1,y1 = map(int,input(\"first point:\").split())\n",
    "x2,y2 = map(int,input(\"second point:\").split())\n",
    "\n",
    "line_points = draw_line(x1,y1,x2,y2)\n",
    "\n",
    "x_values,y_values = zip(*line_points)\n",
    "plt.plot(x_values,y_values,marker='o')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ede9ba2-b036-4b0e-b6b1-b74a94004b84",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'gfile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 这里是下载下来的bert配置文件\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m bert_config \u001b[38;5;241m=\u001b[39m \u001b[43mmodeling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBertConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchinese_L-12_H-768_A-12/bert_config.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#  创建bert的输入\u001b[39;00m\n\u001b[0;32m      8\u001b[0m input_ids\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mplaceholder (shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m128\u001b[39m],dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32,name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\PycharmProjects\\shuju\\bert\\modeling.py:92\u001b[0m, in \u001b[0;36mBertConfig.from_json_file\u001b[1;34m(cls, json_file)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_json_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, json_file):\n\u001b[0;32m     91\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241m.\u001b[39mGFile(json_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[0;32m     93\u001b[0m     text \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_dict(json\u001b[38;5;241m.\u001b[39mloads(text))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'gfile'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from bert import modeling\n",
    "import os\n",
    "\n",
    "# 这里是下载下来的bert配置文件\n",
    "bert_config = modeling.BertConfig.from_json_file(\"chinese_L-12_H-768_A-12/bert_config.json\")\n",
    "#  创建bert的输入\n",
    "input_ids=tf.placeholder (shape=[64,128],dtype=tf.int32,name=\"input_ids\")\n",
    "input_mask=tf.placeholder (shape=[64,128],dtype=tf.int32,name=\"input_mask\")\n",
    "segment_ids=tf.placeholder (shape=[64,128],dtype=tf.int32,name=\"segment_ids\")\n",
    "\n",
    "# 创建bert模型\n",
    "model = modeling.BertModel(\n",
    "    config=bert_config,\n",
    "    is_training=True,\n",
    "    input_ids=input_ids,\n",
    "    input_mask=input_mask,\n",
    "    token_type_ids=segment_ids,\n",
    "    use_one_hot_embeddings=False # 这里如果使用TPU 设置为True，速度会快些。使用CPU 或GPU 设置为False ，速度会快些。\n",
    ")\n",
    "\n",
    "#bert模型参数初始化的地方\n",
    "init_checkpoint = \"chinese_L-12_H-768_A-12/bert_model.ckpt\"\n",
    "use_tpu = False\n",
    "# 获取模型中所有的训练参数。\n",
    "tvars = tf.trainable_variables()\n",
    "# 加载BERT模型\n",
    "\n",
    "(assignment_map, initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(tvars,\n",
    "                                                                                       init_checkpoint)\n",
    "\n",
    "tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "tf.logging.info(\"**** Trainable Variables ****\")\n",
    "# 打印加载模型的参数\n",
    "for var in tvars:\n",
    "    init_string = \"\"\n",
    "    if var.name in initialized_variable_names:\n",
    "        init_string = \", *INIT_FROM_CKPT*\"\n",
    "    tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "                    init_string)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb076d2f-7a15-4364-bfd2-ae00646743b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
